---
title: "Explorer"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Explorer R {#chapitre-explorer}

L'apprentissage de R peut être étourdissant. Cette section est une petite pause fourre-tout qui vous introduira aux nombreuses possibilités de R.

***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- serez en mesure d'identifier les sources d'information principales sur le développement de R et de ses modules
- comprendrez l'importance du prétraitement des données, en particulier dans le cadre de l'analyse de données compositionnelles, et saurez effectuer un prétraitement adéquat
- saurez comment acquérir des données météo d'Environnement Canada avec le module [weathercan](http://ropensci.github.io/weathercan/)
- saurez identifier les modules d'analyse de sols ([soiltexture](https://github.com/julienmoeys/soiltexture) et [aqp](http://ncss-tech.github.io/aqp/docs/))
- saurez créer des cartes
- etc.

***

Pour certains, le langage R est un labyrinthe. Pour d'autres, c'est une myriade de portes ouvertes. Si vous lisez ce manuel, vous vous êtes peut-être engagé dans un labyrinthe dans l'objectif d'y trouver la clé qui dévérouillera une porte bien précise qui mène à un trésor, un objet magique... ou un diplôme. Peut-être aussi prendrez-vous le goût d'errer dans ce labyrinthe, explorant ses débouchés, pour y dénicher au hasard des petits outils et des débouchés.

![](images/06_zelda.gif)

Séquence du jeu vidéo *The legend of Zelda*.

Cette section est un amalgame de plusieurs outils de R pertinents en analyse écologique.

## R sur le web

Dans un environnement de travail en évolution rapide et constante, il est difficile de considérer ses compétences comme étant abouties. Rester informé sur le développement de R vous permettra de dénicher de résoudre des problèmes persistants de manière plus efficace ou par de nouvelles avenues, et vous offrira même l'occasion de dénicher des problèmes dont vous ne soupçonniez pas l'existance.  Plusieurs sources d'information vous permettront de vous tenir à jour sur le développement de R, de ses environnement de travail (RStudio, Jupyter, Atom, etc.) et des nouveaux modules qui s'y greffent. Plus largement, vous gagnerez à vous informer sur les dernières tendances en calcul scientifique sur d'autres plate-forme que R (Python, Javascript, Julia, etc.). Évidemment, nos tâches quotidiennes ne nous permettent pas de tout suivre. Même si vous pouviez n'attrapper qu'1% du défilement, ce sera déjà 1% de plus que rien du tout.

Je vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez à l'aventure!

![](images/06_hobbit.gif)

The Hobbit: An Unexpected Journey, Peter Jackson (2012)

### GitHub

Nous verrons au chapitre \@ref(chapitre-git) l'importane d'utilser des outils d'archivage et de suivi de version, comme *git*, dans le déploiement de la *science ouverte*. Pour l'instant, retenons que [GitHub](https://www.github.com) est une plate-forme *git* en ligne acquise par Microsoft qui est devenue un réseau social de développement informatique. [De nombreux modules de R y sont développés](https://github.com/topics/r). Au chapitre \@ref(chapitre-git), vous serez invités à y ouvrir un compte et à y archiver du contenu. Vous pourrez alors suivre le développement de projets et suivre les travaux des personnes qui vous semblent d'intérêt.

### Nouvelles
Le site d'aggrégation [R-bloggers](https://www.r-bloggers.com/), mis à jour quotidiennement, republie des articles en anglais tirés d'un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux développement. Deux fois par mois, l'organisation [rOpenSci](https://news.ropensci.org/) offre un portrait de l'univ-R, ce que [R Weekely](https://rweekly.org/) offre de manière hebdomadaire (l'information sera probablement redondante). Le tidyverse a quant à lui son propre [blogue](https://www.tidyverse.org/articles/).

### Twitter
Le *hashtag* `#rstats` rassemble sur [Twitter](https://twitter.com/hashtag/rstats?src=hash) ce qui se tweete sur le sujet. On y retrouve les comptes de [R-bloggers](https://twitter.com/Rbloggers), [RStudio](https://twitter.com/rstudio) et [rOpenSci](https://twitter.com/rOpenSci). Certaines communauté y sont aussi actives, comme [R4DS online learning community](https://twitter.com/R4DScommunity), qui partage des nouvelles sur R, et [R-Ladies Global](https://twitter.com/RLadiesGlobal), qui vise à amener davantage de diversité à la communauté de R. Des comptes thématiques comme [Daily R Cheatsheets](https://twitter.com/daily_r_sheets) et [One R Package a Day](https://twitter.com/RLangPackage) permettent de découvrir quotidiennement de nouvelles possibilités. Enfin, plusieurs personnes contribuent positivement à la communauté R. [Hadley Wickham](https://twitter.com/hadleywickham) brille parmi les étoiles de R. Les comptes de [Mara Averick](https://twitter.com/dataandme), [Claus Wilke](https://twitter.com/ClausWilke) et [David Robinson](https://twitter.com/drob) sont aussi intéressants.

### Des questions?

Bien que davantage voués à la résolution de problème qu' à l'exploration de nouvelles opportunités, [Stackoverflow](https://stackoverflow.com/questions/tagged/r) et [Cross Validated](https://stats.stackexchange.com/questions/tagged/r) sont des plate-forme prisées. De plus, la liste de courriels [r-sig-ecology](https://www.mail-archive.com/r-sig-ecology@r-project.org/info.html) permet des échanges entre professionnels et novices en analyse de données écologiques avec R.

### Mise en garde

Les modules de R sont développés par quiconque le veut bien: leur qualité n'est pas nécessairement auditée. Souvent, ils ne sont vérifiés que par une vigilance communautaire: dans ce cas, vous êtes les cobailles. Ce qui n'est pas nécessairement une mauvaise chose, mais cela nécessite de prendre ses précautions. Dans sa conférence [How to be a resilient R user](https://maelle.github.io/fluctuat_nec_mergitur), [Maëlle Salmon](https://twitter.com/ma_salmon) propose quelques guides pour juger de la qualité d'un module.

**1. Le module est-il activement développé?**

Bien!

![](images/06_2019-01-14-facebook-prophet.png)

Attention!

![](images/06_2019-01-14_mlammens_meteR.png)

**2. Le module est-il bien testé?**

Vérifiez si le module a fait l'objet d'une publication scientifique, s'il a été utilisé avec succès dans la litérature ou dans des documents crédibles.

**3. Le module est-il bien documenté?**

Un site internet dédié est-il utilisé pour documenter l'utilisation du module? Les fichiers d'aide sont-ils complets, et sont-ils de bonne qualité?

**4. Le module est-il largement utilisé?**

Un module peu populaire n'est pas nécessaissairement de mauvaise qualité: peut-être est-il seulement destiné à des applications de niche. S'il n'est pas un indicateur à lui seul de la solidité ou la validité d'un module, une masse critique indique que le module a passé sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peu être évalué par le nombre d'étoiles attribué au module (équivalent à un J'aime).

![](images/06_peu-etoiles.png) ![](images/06_bcp-etoiles.png)

**5. Le module est-il développé par une personne ou une organisation crédible?**

On peut affirmer sans trop se compromettre que l'équipe de RStudio développe des modules de confiance. Tout comme il faudrait se méfier d'un module développé par une personne anonyme.

Le module [packagemetrics](https://github.com/ropenscilabs/packagemetrics) permet d'évaluer ces critères.

```{r}
library("packagemetrics")
pm <- package_list_metrics(c("dplyr", "ggplot2", "vegan", "greta"))
metrics_table(pm)
```

### Prendre tout ça en note

Un logiciel de prise de notes (comme [Evernote](http://evernote.com/), [OneNote](http://onenote.com/), [Notion](http://notion.so), [Simplenote](https://simplenote.com), [Turtl](https://turtlapp.com/), etc.) pourrait vous être utile pour retrouver l'information soutirée de vos flux d'information. Cerains utilisent simplement le traitement de texte.

## Quelques outils en écologie mathématique avec R

### Prétraitement des données

Il arrive souvent ques les données brutes ne soient pas exprimées de manière appropriée ou optimale pour l'analyse statistique ou la modélisation. Vous devrez alors effectuer un prétraitement sur ces données. En particulier, si vos données forment une partie d'un tout (exprimées en pourcentages ou fractions), vous devriez probablement utiliser les outils de l'**analyse compositionnelle**. Avant de les aborder, nous allons traiter des transformations de base.

#### Standardisation

La standardisation consiste à centrer vos données à une moyenne de 0 et à les échelonner à une variance de 1, c'est-à-dire

$$x_{standard} = \frac{x - \bar{x}}{\sigma}$$

où $\bar{x}$ est la moyenne du vecteur $x$ et où $\sigma$ est son écart-type.

Ce prétraitement des données peut s'avérér utile lorsque la modélisation tient compte de l'échelle de vos mesures (par exemple, les paramètres de régression vus au chapitre \@ref(chapitre-biostats) ou les distances que nous verrons au chapitre \@ref(chapitre-ordination)). En effet, les pentes d'une régression linéaire multiple ne pourront être comparées entre elles que si elles sont une même échelle. Par exemple, on veut modéliser la consommation en miles au gallon (`mpg`) de voitures en fonction de leur puissance (`hp`), le temps en secondes pour parcourir un quart de mile (`qsec`) et le nombre de cylindre.

```{r}
data(mtcars)
modl <- lm(mpg ~ hp + qsec + cyl, mtcars)
summary(modl)
```

Les pentes signifie que la distance parcourue par gallon d'essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L'interprétation est conviviale à cette échelle. Mais lequel de ces effets est le plus important? L `t value` indique que ce seraient les cylindres. Mais pour juger l'importance en terme de pente, il vaudrait mieux standardiser.

```{r}
library("tidyverse")
mtcars_sc <- mtcars %>%
  apply(., 2, function(x) (x-mean(x))/sd(x)) %>% 
  as_tibble() # ou bien scale(mtcars, center = TRUE, scale = TRUE)
modl_sc <- lm(mpg ~ hp + qsec + cyl, mtcars_sc)
summary(modl_sc)
```

Les valeurs des pentes ne peuvent plus être interprétées directement, mais peuvent maintenant être comparées entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile.

Les algorithmes basés sur des distances auront, de même, avantage à être standardisés.

#### Normalisation 

#### Analyse compositionnelle en R

Les variables exprimées sans égard à leur somme totale mais en terme de proportions d'un tout sont *compositionnelles*. En 1898, le statisticien Karl Pearson nota que des corrélations négatives étaient nécessairement induites dans une matrice de compositions. Par exemple, dans une composition simple de deux types d'utilisation du territoire, si une proportion augmente, l'autre doit nécessairement diminuer. Comment alors effectuer des statistiques multivariées valides, alors que les compositions portent les contraintes suivantes.

1. **Redondance d'information**. Un système de deux proportions ne contient qu'une seule variable du fait que l'on puisse déduire l'une en soutrayant l'autre de la somme totale. Un vecteur compositionnel contient de l'information redondante. Pourtant, effectuer des statistiques sur l'une plutôt que sur l'autre donnera des résultats différents.
2. **Dépendance d'échelle**. Les statistiques devraient être indépendantes de la somme totale utilisée. Pourtant, elles différeront sur l'on utilise par exemple, une proportion des mâles d'une part et des femelles d'autre part, ou la proportion de la somme des deux, de même que les résultats d'un test sanguin différera si l'on utilise une base sèche ou une base humide.
3. **Distribution théorique des données**. Étant donnée que les proportions sont confinées entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui s'étend de -∞ à +∞) n'est souvent pas appropriée. On pourra utiliser la distribution de Dirichlet, mais d'autres approches sont souvent plus pratiques.

En outre, Pawlowksy-Glahn et Egozcue (2006), s'appuyant en outre sur Rock (1988), ...

Pour contourner cet arbitraire, on pourra effectuer un ratio de l'une sur l'autre. Le problème persiste, pusique l'attribution au rôle du numérateur ou du dénominateur aboutiera à des rendu statistiques différents.

En R, on pourra aisément rapporter une composition en somme unitaire grâce à la fonction `apply`. Mais auparavant, chargeons le module `compositions` (n'oubliez pas de l'installer au préalable) pour accéder à des données fictives de proportions de sable, limon et argile dans des sédiments.

```{r}
library("compositions")
data("ArcticLake")
ArcticLake <- ArcticLake %>% as_tibble()
head(ArcticLake)
```

```{r}
comp <- ArcticLake %>%
  select(-depth) %>%
  apply(., 1, function(x) x/sum(x)) %>% 
  t()
comp[1:5, ]
```


On pourra aussi utiliser la fonction `acomp` (pour Aitchison-composition) pour fermer la composition à une somme de 1.

```{r}
comp <- ArcticLake %>%
  select(-depth) %>%
  acomp(.)
comp[1:5, ]
```

Cette stratégie a pour avantage d'attribuer à la variable `comp` la classe `acomp`, qui automatise les opérations dans l'espace compositionnel (que l'on nomme aussi le *simplex*). La représentation ternaire est souvent utilisée pour présenter des compositions. Toutefois, il est difficile d'interpréter les compositions de plus de trois parties. La classe `acomp` automatise aussi la représentation teranaire.

```{r}
plot(comp)
```

La solution proposée par Aitchison (1986) est d'effectuer un log-ratio. Ainsi, permuter le ratio ne résultera qu'en un changement de signe du log-ratio. Cette solution s'applique non seulement sur les compositions à deux composantes, mais sur toute composition. Il s'agit alors d'utiliser une composition de référence pour effecteur les ratios. Pour une composition de $A$, $B$, $C$, $D$ et $E$:

$$alr_A = log \left( \frac{A}{E} \right), alr_B = log \left( \frac{B}{E} \right), alr_C = log \left( \frac{C}{E} \right), alr_D = log \left( \frac{D}{E} \right)$$

Dans R, la colonne de référence est par défaut la ??? colonne de la matrice des compositions.

```{r}
add_lr <- alr(comp)
```

Cette dernière stratégie se nomme les log-ratios aditifs ($alr$ pour *additive log-ratio*). Bien que valide pour effectuer des tests statistiques, cette stratégie a le désavantage de dépendre de la décision arbitraire de la composante à utiliser au numérateur. Deuxième restriction des *alr*: les axes de l'espace des *alr* n'étant pas orthogonaux, ils ne peuvent pas être utilisés pour effectuer des statistiques basées sur les distances (que nous couvrirons au chapitre ???).

L'autre stratégie proposée par Aitchison était d'effectuer un log-ratio entre chaque composante et la moyenne géométrique de toutes les composantes.



Enfin, une autre méthode de transformation développée par Egoscue et alé (2003) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonormées. Ces dimensions doivent être conçues au préalable dans un diagramme de balances compositionnelles. **The ilr transformation is suitable for conducting multivariate analyses of ionomes, because it can illustrate the nutrient relationships as a multidimensionnal map of sound, hierarchically arranged binary balances betweeen groups of nutrients (Parent et al., 2013, Modesto et al., 2014). Nutrient balances can also be included in non-linear models to achieve greater accuracy (Silverman et al., 2017).**



J'ai reformulé la fonction CoDaDendrogram pour que l'on puisse ajouter des informations intéressantes sur les balants horizontaux. Cette fonction est disponible sur github.



Si vos données font partie d'un tout, je vous recommande chaudement d'utiliser des méthodes compositionnelles autant pour l'analyse que la modélisation. Pour en savoir davantage, le livre *Compositional data analysis with R*, de ??? et ???, est disponible en format électronique à la bibliothèque de l'Université Laval.

### Acquérir des données météo


### Pédométrie avec R

*Cette section a été écrite par [Michael Leblanc](https://www.researchgate.net/profile/Michael_Leblanc7).*

Plusieurs fonctionnalités ont été développées sur R afin d'aider les *pédométriciens* à visualiser, explorer et traiter les données numériques en science des sols. Voici quelques exemples.

#### Texture du sol

La texture du sol est définie par sa composition granulométrique, habituellement représentée par trois fractions (sable, limon, argile), laquelle peut être généralisée en classe texturale. La définition des classes texturales diffère d'un système ou d'un pays à l'autre comme en témoigne l'article [Perdus dans le triangle des textures (Richer de Forges et al. 2008)](http://www.afes.fr/wp-content/uploads/2017/10/EGS_15_2_richerdeforges.pdf). La définition des fractions granulométriques peut également différer selon le domaine d'étude (ingénierie, pédologie) ou le pays. Par exemple, le diamètre du limon est de 0,002 mm à 0,05 mm dans le système canadien, américain et français alors qu'il est de 0,002 mm à 0,02 mm dans le système australien et de 0,002 mm à 0,063 mm dans le système allemand. Il est donc important de vérifier la méthodologie et le système de classification utilisés pour interpréter les données de texture du sol. Le module `soilTexture` propose des fonctions permettant d'aborder ces multiples définitions.

```{r}
library("soiltexture")
```

***Les triangles texturaux***

Avec la fonction `TT.plot`, vous pouvez présenter vos données granulométriques dans un triangle textural tel que défini par les différents systèmes nationaux. Auparavant, créons un objet comprenant des textures aléatoires.

```{r}
set.seed(848341) # random.org
rand_text <- TT.dataset(n=100, seed.val=29)
head(rand_text)
```

Avec le module soiltexture, les tableaux de texture doivent inclure les intitullés exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures générées peuvent être portés dans des diagrammes ternaires texturaux de différents systèmes de classification, par exemple le système canadioen et le système USDA.

```{r, fig.width = 16, fig.height = 8}
par(mfrow=c(1, 2))

TT.plot(class.sys = "CA.FR.TT", 
        tri.data = rand_text,
        col = "blue")
TT.plot(class.sys = "USDA.TT", 
        tri.data = rand_text,
        col = "blue")

```

Les paramètres de la figure (titres, polices, style de la grille, etc.) peuvent être personnalisés avec les [arguments TT.plot](https://www.rdocumentation.org/packages/soiltexture/versions/1.5.1/topics/TT.plot).

***Les classes texturales***

La fonction `TT.points.in.classes` est utile pour désigner la classe texturale à partir des données granulométriques, en spécifiant bien le système de classification désiré.

```{r}
TT.points.in.classes(
  tri.data = rand_text[1:10, ], # 
  class.sys = "CA.FR.TT",
  PiC.type = "t"
)
```

Plusieurs autres fonctions sont proposées par `soiltexture` afin de visualiser, classifier et transformer les données de texture du sol : [Functions in soiltexture](https://www.rdocumentation.org/packages/soiltexture/versions/1.5.1). Julien Moeys (2018) propose également le tutoriel [*The soil texture wizard: a tutorial*](https://cran.r-project.org/web/packages/soiltexture/vignettes/soiltexture_vignette.pdf).

#### Profils de sols

Le profil de sols est une entité décrite par une séquence de couches ou d'horizons avec différentes caractéristiques morphologiques. Le module AQP, pour [*Algorithms for Quantitative Pedology*](http://ncss-tech.github.io/AQP/), propose des fonctions de visualisation, d'agrégation et de classification permettant d'aborder la complexité inhérente aux informations pédologiques.

##### La visualisation de profils

Vous devez d'abord structurer vos données dans un tableau (`data.frame`) incluant minimalement ces trois colonnes :

1. Identifiant unique du profil (groupes d'horizons) (`id`)
2. Limites supérieures de l'horizon (`top`)
3. Limites inférieures de l'horizon (`down`)

Vos données morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier pédologique à titre d'exemple.

```{r}
profils <- read_csv("data/06_pedometric-profile.csv")
head(profils)
```

La fonction `munsell2rgb` permet de convertir le code de couleur *Munsell* en format *RGB*.

```{r}
library("aqp")
profils$soil_color <- with(profils, munsell2rgb(hue, value, chroma))
```

Préalablement à la visualisation, le tableau est transformé en objet `SoilProfileCollection` par la fonction `depths`. Pour ce faire, le tableau doit être un pur `data.frame`, non pas un `tibble`.

```{r}
profils <- profils %>% as.data.frame()
depths(profils) <- id ~ top + bottom
```

La fonction `plot` détectera le type d'objet et appellera la fonction de visualisation en conséquence.

```{r, fig.width = 12, fig.height = 4}
par(mfrow = c(1, 3))
plot(profils, name="horizon")
title('Couleur des horizons', cex.main=1)
plot(profils, name="horizon", color='C.CNS.pc', col.label='C total (%)')
plot(profils, name="horizon", color='pH.CaCl2', col.label='pH CaCl2')
```

De multiples figures thématiques peuvent être générées afin de représenter les particuliarités des profils. Pour aller plus loin, consultez les guides [*Introduction to SoilProfileCollection Objects*](http://ncss-tech.github.io/AQP/aqp/aqp-intro.html) et [*Generating Sketches from SPC Objects*](http://ncss-tech.github.io/AQP/aqp/SPC-plotting-ideas.html).

##### Les plans verticaux (depth functions)

Les plans verticaux sont des diagrammes qui permettent d'interpréter les données en fonction de la profondeur. La fonction `slab` permet le calcul de statisques descriptives par interval de profondeur régulier lesquelles permettent de figurer la variabilité verticale des propriétés de sols.

```{r}
agg <- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2)
```

La visualisation est générée par le module graphique ggplot2

```{r}
agg %>%
  ggplot(mapping = aes(x = -top, y = p.q50)) +
  facet_grid(. ~ variable, scale = "free") +
  geom_ribbon(aes(ymin =  p.q25, ymax = p.q75), fill = "grey75", alpha = 0.5) +
  geom_path() +
  labs(x = "Profondeur (cm)",
       y = "Médiane bordée des 25e and 75e percentiles") +
  coord_flip()
```

##### Le regroupement de profils

Le calcul des distances de dissimilarité entre les profils avec `profile_compare` permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitre \@ref(chapitre-ordination) les concepts de dissimilarité et de partitionnement.

```{r}
library("cluster")
library("sharpshootR")
d <- profile_compare(profils, vars=c('C.CNS.pc', 'pH.CaCl2'), k=0, max_d=40)
d_diana <- diana(d)
plotProfileDendrogram(profils, name="horizon", d_diana, scaling.factor = 0.3, y.offset = 5, color='pH.CaCl2',  col.label='pH CaCl2')
```

##### Diagramme de relations entre les horizons

Il est possible de visualiser les transitions d'horizon les plus probables dans un groupe de profils de sols.

```{r, fig.width = 10, fig.height = 5}
tp <- hzTransitionProbabilities(profils, name="horizon")
par(mar = c(0, 0, 0, 0), mfcol = c(1, 2))
plot(profils, name="horizon")
plotSoilRelationGraph(tp, graph.mode = "directed", edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, 
                      vertex.label.family = "sans")
```

Consultez [AQP project](http://ncss-tech.github.io/AQP/) pour des présentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilités du package `AQP`.

### Cartographier