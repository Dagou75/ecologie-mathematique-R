[
["index.html", "Analyse et modélisation d’agroécosystèmes 1 Introduction 1.1 Définitions 1.2 À qui s’adresse ce manuel? 1.3 Les logiciels libres 1.4 Langage de programmation 1.5 Contenu du manuel 1.6 Lectures complémentaires 1.7 Besoin d’aide? 1.8 À propos de l’auteur 1.9 Un cours complémentaire à d’autres cours 1.10 Contribuer au manuel", " Analyse et modélisation d’agroécosystèmes Serge-Étienne Parent 2018-12-20 1 Introduction En développant son jeu de la vie (game of life) en 1970, John Horton Connway a présenté un exemple percutant que des règles simples peuvent mener à des résultats inattendus. Le jeu consiste à placer des jetons sur les cases d’un plateau de jeu consistant en une simple grille orthogonale. Le jeu évolue en fonction du nombre de jetons présents parmi les huit cases du voisinage des jetons ou des cases vides. Les jetons ayant 0 ou 1 voisin sont retirés. Les jetons ayant 2 ou 3 voisins restent intacts Les jetons ayant plus de 3 voisins sont retirés Un jeton est posé sur les cases ayant exactement 3 voisins C’est tout. Selon la manière dont les jetons sont placés au départ, il se peut que la grille se vide de ses jetons, ou que les jetons y prennent beaucoup de place. Il arrive aussi que des cycles réguliers se dégagent ou que l’on se retrouve avec des formes régulières. Vous aurez peut-être compris à ce stade pourquoi le jeu est appelé “jeu de la vie”. La première règle est une situation localisée de sous-population, condition dans laquelle la reproduction est difficile. La deuxième règle est une situation localisée stable. La troisième est une situation de surpopulation, où des individus meurent par un environnement rendu inadéquat par insuffisance de ressource ou surplus de toxicité. Enfin, la quatrième indique une situation favorable à la reproduction. Une grille vidée correspond à une extinction et une grille remplie correspond à une explosion de population. Une oscillation est un “climax”, un état stable en écologie. Un léger changement initial dans la disposition initiale des jetons peu mener à des solutions différentes. Le jeu, qui en fait est une application de la technique des automates cellulaires, se complexifie à mesure que le nombre de jetons grandit. Un humain passera des heures à calculer une itération à 50 jetons, commettra probablement quelques erreurs et demandera quelques cafés. Un processeur pourra gérer des centaines d’itérations sur des grilles de centaines de jetons en quelques secondes. En établissant des règles correspondant aux mécanismes de l’objet étudié, il devient possible de modéliser l’évolution des systèmes vivants, comme l’émergence d’espèces invasives. Simulation avec automates cellulaires. Source: Anonyme, publié sur Giphy. 1.1 Définitions Les mathématiques confèrent aux humains une capacité d’abstraction suffisamment complexe pour leur permettre de toucher les étoiles et les atomes, d’assembler la pensée pour mieux apprécier l’histoire et de prédire le futur, de toucher l’infini et de goûter à l’éternité. À partir des maths, on a pu créer des outils de calcul permettent de projeter des images de l’univers, bien au-delà de la Voie lactée. Mais appréhender le vivant demeure néanmoins une tâche complexe. Carte des domaines de l’écologie mathématique L’écologie mathématique couvre un large spectre de domaines, mais peut être divisée en deux branches: l’écologie théorique et l’écologie quantitative (Legendre et Legendre, 2012). Alors que l’écologie théorique s’intéresse à l’expression mathématique des mécanismes écologiques, l’écologie quantitative, plus empirique, en étudie principalement les phénomènes. La modélisation écologique vise à prévoir une situation selon des conditions données. Faisant partie à la fois de l’écologie théorique et de l’écologie quantitative, elle superpose souvent des mécanismes de l’écologie théorique et des phénomènes empiriques de l’écologie quantitative. L’écologie numérique comprend la branche descriptive de l’écologie quantitative, c’est-à-dire qu’elle s’intéresse à évaluer des effets à partir de données empiriques. L’exploration des données dans le but d’y découvrir des structures passe souvent par des techniques multivariées comme la classification hiérarchique ou la réduction d’axe (par exemple, l’analyse en composantes principales), qui sont davantage heuristiques (dans notre cas, bioheuristique) que statistiques. Les tests d’hypothèses et l’analyse des probabilités, quant à eux, relèvent de la biostatistique. Le génie écologique, une discipline intimement liée à l’écologie mathématique, est vouée à l’analyse, la modélisation, la conception et la construction de systèmes vivants dans le but de résoudre de manière efficace des problèmes liés à l’écologie et à une panoplie de domaines qui lui sont raccordés. L’agriculture est l’un de ces domaines. C’est d’emblée la discipline qui sera prisée dans ce manuel. Néanmoins, les principes qui seront discutés sont transférable à l’écologie générale. 1.2 À qui s’adresse ce manuel? Le cours vise à introduire des étudiants gradués en agronomie, biologie, écologie, sols, génie agroenvironnemental, génie civil et génie écologique à l’analyse et la modélisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse d’outil émancipatrice pour leur cheminement professionnel. Plus spécifiquement, vous serez accompagné à découvrir différents outils numériques qui vous permettront d’appréhender vos données, d’en faire émerger l’information et de construire des modèles. En ce sens, c’est un cours de pilotage, pas un cours de mécanique. Bien que des connaissances en programmation et en statistiques aideront grandement les étudiant.e.s à appréhender ce document, une littératie informatique n’est pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve d’autonomie. Vous serez guidés vers des ressources et des références, mais je vous suggère vivement de développer votre propre bibliothèque adaptée à vos besoins et à votre manière de comprendre. 1.3 Les logiciels libres Tous les outils numériques qui sont proposés dans ce cours sont des logiciels libres: « Logiciel libre » [free software] désigne des logiciels qui respectent la liberté des utilisateurs. En gros, cela veut dire que les utilisateurs ont la liberté d’exécuter, copier, distribuer, étudier, modifier et améliorer ces logiciels. Ainsi, « logiciel libre » fait référence à la liberté, pas au prix1 (pour comprendre ce concept, vous devez penser à « liberté d’expression », pas à « entrée libre »). - Projet GNU Donc: codes sources ouverts, développement souvent communautaire, gratuité. Plusieurs raisons éthiques, principalement liées au contrôle de l’environnement virtuel par les utilisateurs et les communautés, peuvent justifier l’utilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, d’une entreprise à l’autre, au bureau, ou à la maison, et ce, sans vous soucier d’acheter de coûteuses licences. On soulève avec justesse les risques liés aux possibles erreurs dans les codes des logiciels communautaires. Pour les scientifiques, une erreur peu mener à une étude retirée de la littérature et même, potentiellement, des politiques publiques mal avisées. Pour les ingénieurs, les conséquences pourraient être dramatiques. Mais retenez qu’en toute circonstance, comme professionnel.le, vous êtes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualité d’un logiciel, qu’il soit propriétaire ou communautaire. Alors que la qualité des logiciels propriétaires est généralement suivie par audits, celle des logiciels libres est plutôt soumise à la vigilance communautaire. Chaque approche a ses avantages et inconvénients, mais elles ne sont pas exclusives. Ainsi les logiciels libres peuvent être audités à l’externe par quiconque décide de le faire. Différentes entreprises, souvent concurrentes, participent tant à cette vigilance qu’au développement des logiciels libres: elles en sont même souvent les instigatrices (comme RStudio, Anaconda et Enthought). Par ailleurs, ce manuel est distribué librement (licence MIT). 1.4 Langage de programmation 1.4.1 R Ce cours est basé sur le langage R. En plus d’être libre, R est un langage de programmation dynamique largement utilisé dans le monde universitaire, et dont l’utilisation s’étend de manière soutenue hors des tours d’ivoire. Son développement est supporté par la R Foundation for Statistical Computing, basée à l’Université de Vienne. Également, l’équipe de RStudio contribue largement au développement de modules génériques. R est principalement utilisé pour le calcul statistique, mais les récents développements le rendent un outil de choix pour tout ce qui entoure la science des données, de l’interaction avec les bases de données au déploiement d’outil d’intelligence artificielle en passant par la visualisation. Une fois implémenté avec des modules de calcul scientifique spécialisés en biologie, en écologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable pour le calcul écologique. 1.4.2 Pourquoi pas Python? La première mouture de ce cous se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique prisé pour le calcul scientifique. Python est un langage générique apprécié pour sa polyvalence et sa simplicité. Python est utilisé autant pour créer des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut être utilisé en interopérabilité avec une panoplie de logiciels libres, comme QGIS pour la cartographie et FreeCAD pour le dessin technique. Il est particulièrement apprécié en ingénierie pour ses modules de calcul par éléments finis (FeNICS, SfePy) et en bioinformatique pour ses outils liés au séquençage (scikit-bio), mais ses lacunes en analyse statistique, en particulier en statistiques multivariées m’ont amené à favoriser R. Bien que leurs possibilités se superposent largement, ce serait une erreur d’aborder R et Python comme des langages rivaux. Les deux langages s’expriment de manière similaire et s’inspirent mutuellement: apprendre à travailler avec l’un revient à apprendre l’autre. Les spécialistes en calcul scientifique tendent à apprendre à travailler avec plus d’un langage de programmation. Par ailleurs, l’entreprise Ursa labs travaille en ce moment à l’élaboration d’une infrastructure de données permettant de partager des objets R et Python, en vue d’intégrer différents langages de programmation dans un même flux de travail. 1.4.3 Pourquoi pas Matlab? Parce qu’on est en 2019. 1.4.4 Et… SAS? Parce qu’on est à l’université. 1.4.5 Mais pourquoi pas ______ ? D’autres langages, comme Julia, Scala, Javascript et même Ruby sont utilisés en calcul scientifique. Ils sont néanmoins moins garnis et moins documentés que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus à utiliser au jour le jour, mais leur rapidité de calcul est imbattable. 1.5 Contenu du manuel Le pire angle avec lequel je pourrais aborder le sujet, c’est avec du code et des formules mathématiques. À travers chacun des chapitres, je tenterai de vous amener à résoudre des problèmes de la manière la plus intuitive possible. Nous aborderons l’analyse et la modélisation inférentielle, prédictive et déterministe appliquée aux agroécosystèmes. Chapitre 2 - Introduction au langage de programmation R. Qu’est-ce que R? Comment l’aborder? Quelles sont les fonctionnalités de base et comment tirer profit de tout l’écosystème de programmation? Chapitre 3 - Organisation des données et opérations sur des tableaux. Les tableaux permettent d’enchâsser l’information dans un format prêt-à-porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires? Chapitre 4 - Visualisation. Comment présenter l’information contenue dans un long tableau en un seul coup d’oeil? Chapitre 5 - Biostatistiques. Il est audacieux de ne consacrer qu’un seul chapiter sur ce vaste sujet. Nous irons à l’essentiel… pour vous donner les outils qui permettront d’approfondir le sujet. Chapitre 6 - Biostatistiques bayésiennes. Une très brève introduction pour qui est intéressé à l’analyse bayésienne. Chapitre 7 - Prétraitement des données. Les données que vous recueillez ne sont pas néessairement prêtes à être analysées et modélisées: elles nécessite souvent un traitement préalable. Chapitre 8 - Association, partitionnement et ordination. Les écosystèmes diffèrent, mais en quoi sont-ils semblables, et en quoi dffèrent-ils? Ces questions importantes peuvent être abordés par l’écologie numérique, domaine d’étude au sein duquel l’association, le partitionnement et l’ordination sont des outils prédominants. Chapitre 9 - Détection de valeurs aberrantes et imputation. Une donnée aberrantes sortira du lot, pour une raison ou pour une autre. Comment les détecter de manière systématique? D’autre part, que faire lorsqu’une donnée est manquante? Peut-on l’imputer? Comment? Chapitre 10 - Les séries temporelles. Les capteurs modernes permettent de générer des données en fonction du temps. Que ce soit des données météorologiques enregistrées quotidiennement ou des données de teneur en eau enregistrées au 5 secondes, les données en fonction du temps forment un signal. Comment analyser ces signaux? Chapitre 11 - Le travail collaboratif, le suivi de version et la science ouverte. Ce chapitre offre une introduction à l’utilisation des outils de calcul collaboratif, ainsi qu’un aperçu du système de suivi de version git et de son utilisation sur GitHub. Chapitre 12 - L’autoapprentissage. Les applications de l’intelligence artificielle ne sont limitées que par votre imagination. Encore faut-il l’utiliser intéligemment. Chapitre 13 - Les données spatiales. Non, nous n’aborderons pas les géostatistiques. Ce chapitre porte plutôt sur l’utilisation de R comme système d’information géographique de base. Nous utiliserons aussi l’autoapprentissage comme outil d’interpolation spatial. Chapitre 14 - La modélisation déterministe. Les modèles sont des maquettes simplifiées. Comment utiliser les équations différentielles ordinaires pour créer ces maquettes? Si les chapitres 3 à 5 peuvent être considérés comme fondamentaux pour bien maîtriser R, les autres peuvent être feuilletés à la pièce, bien qu’ils forment une suite logique. Chaque chapitre de ce manuel est rédigé en format R notebook, dans un environnement RStudio. Pour exécuter les commandes, les vous pourrez soit copier-coller les commande dans R (ou RStudio), soit télécharger les fichiers-source et exécuter les blocs de code. 1.6 Lectures complémentaires 1.6.1 Écologie mathématique How to be a quantitative ecologist. Jason Mathipoulos vous prend par la main pour découvrir les notions de mathématiques fondamentales en écologie, appliquées avec le langage R. Numerical ecology. L’ouvrage hautement détaillé des frères Legendre est non seulement fondamental, mais aussi fondateur d’une science qui évolue encore aujourd’hui: l’analyse des données écologiques. A practical guide to ecological modelling. Soetaert et Herman portent une attention particulière à la présentation des principes de modélisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la modélisation. Les modèles présentés concernent principalement les bilans de masse, en terme de systèmes de réactions chimiques et de relations biologiques. Modélisation mathématique en écologie. Rare livre en modélisation écologique publié en français, la première partie s’attarde aux concepts mathématiques, alors que la deuxième planche à les appliquer. Si le haut niveau d’abstraction de la première partie vous rebute, n’hésitez pas débuter par la seconde partie et de vous référer à la première au besoin. A new ecology: systems perspective. Principalement grâce au soleil, la Terre forme un ensemble de gradients d’énergie qui se déclinent en des systèmes d’une étonnante complexité. C’est ainsi que le regretté Sven Erik Jørgensen (1934-2016) et ses collaborateurs décrivent les écosystèmes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum. Sven Erik Jørgensen Ecological engineering. Principle and Practice. Ecological processes handbook. Modeling complex ecological dynamics 1.6.2 Programmation R for data science. L’analyse de données est une branche importante de l’écologie mathématique. Ce manuel traite des matrices et la manipulation de données chapitre 3), de la visualisation (chapitre 4) ainsi que de l’apprentissage automatique (chapitre 11). R for data science repasse ces sujets plus en profondeur. En particulier, l’ouvrage de Garrett Grolemund et Hadley Wickham offre une introduction au module graphique ggplot2. Numerical ecology with R. Daniel Borcard enseigne l’écologie numérique à l’Université de Montréal. Son cours est condensé dans ce livre recettes voué à l’application des principes lourdement décrits dans Numerical ecology. 1.6.3 Divers The truthful art. Dans cet ouvrage, Alberto Cairo s’intéresse à l’utilisation des données et de leurs présentation pour fournir une information adéquate à différents publics. 1.7 Besoin d’aide? Les ouvrages de référence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-delà des principes, au jour le jour, vous vous butterez immanquablement à toutes sortes de petits problèmes. Quel module utiliser pour cette tâche précise? Que veut dire ce message d’erreur? Comment interpréter ce résultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont très hétérogènes en qualité. Vous apprendrez à reconnaître les ressources fiables à celles qui sont douteuses. Les plateformes basées sur Stack Exchange, comme Stack Overflow et Cross Validated, m’ont souvent été d’une aide précieuse. Vous aurez avantage à vous construire une petite banque d’information (Turtl, Notion, Evernote, Google Keep, One Note, etc.) en collectant des liens, en prenant en notes certaines recettes et en suivant des sites d’intérêt avec des flux RSS. 1.8 À propos de l’auteur Je m’appelle Serge-Étienne Parent. Je suis ingénieur écologue et professeur adjoint au Département des sols et de génie agroalimentaire de l’Université Laval, Québec, Canada. Je crois que la science est le meilleur moyen d’appréhender le monde pour prendre des décisions avisées. 1.9 Un cours complémentaire à d’autres cours Ce cours a été développé pour ouvrir des perspectives mathématiques en écologie et en agronomie à la FSAA de l’Université Laval. Il est complémentaire à certains cours offerts dans d’autres institutions académiques au Québec, dont ceux-ci. BIO2041. Biostatistiques 1, Université de Montréal BIO2042. Biostatistiques 2, Université de Montréal BIO109. Introduction à la programmation scientifique, Université de Sherbrooke BIO500. Méthodes en écologie computationnelle, Université de Sherbrooke. 1.10 Contribuer au manuel Je suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le dépôt du manuel sur GitHub, puis ouvrez une Issue pour en discuter. Créez une nouvelle branche (fork), effectuez les modifications, puis lancer une requête de fusion (pull resquest). "],
["chapitre-intro-a-R.html", "2 La science des données avec R 2.1 Statistiques ou Science des données? 2.2 Organiser son environnement de travail en R 2.3 Préparer son flux de travail 2.4 Premiers pas avec R 2.5 Enfin…", " 2 La science des données avec R Un projet en science des données comprends trois grandes étapes. D’abord, vous devez collecter des données et vous les compilez adéquatement. Cela peut consister à télécharger des données existantes, exécuter un dispositif expérimental ou effectuer une recensement (étude observationnelle). Compiler les données dans un format qui puisse être importé est une tâche souvent longue et fastidieuse. Puis, vous investiguez les données collectées, c’est-à-dire vous les visualisez, vous appliquez des modèles et testez des hypothèses. Enfin, la communication des résultats consiste à présenter les connaissances qui émerge de votre analyse sous forme visuelle et narrative, avec un langage adapté à la personne qui vous écoute, qu’elle soit experte ou novice, réviseure de revue savante ou administratrice. Grolemund et Wickham (2018) propose la structure d’analyse suivante, avec de légères modifications de ma part. Le grand cadre spécifie Programmer. Oui, vous aurez besoin d’écrire du code. Mais comme je l’ai indiquer dans le premier chapitre, ceci n’est pas un cours de programmation et je préférerai les approches intuitives. À la fin de ce chapitre, vous devrez: contextualiser la science des données par rapport aux statistiques, être en mesure de vous lancer dans un environnement de programmation R, être en mesure d’effectuer des opérations d’une seule ligne en R, et installer et charger des modules. 2.1 Statistiques ou Science des données? Selon Whitlock et Schluter (2015), la statistique est l’étude des méthodes pour décrire et mesurer des aspects de la nature à partir d’échantillon. Pour Grolemund et Wickham (2018), la science des données est une discipline excitante permettant de transformer des données brutes en compréhension, perspectives et connaissances. Oui, excitante! La différence entre les deux champs d’expertise est subtile, et certaines personnes n’y voient qu’une différence de ton. Data Science is statistics on a Mac. — Big Data Borat (@BigDataBorat) 27 août 2013 Confinées à ses applications traditionnelles, les statistiques sont davantage vouées à la définition de dispositifs expérimentaux et à l’exécution de tests d’hypothèses, alors que la science des données est moins linéaire, en particulier dans sa phase d’analyse, où de nouvelles questions (donc de nouvelles hypothèses) peuvent être posées au fur et à mesure de l’analyse. Cela arrive généralement davantage lorsque l’on fait face à de nombreuses observations sur lesquelles ne nombreux paramètres sont mesurés. La quantité de données et de mesures auxquelles nous avons aujourd’hui accès grâce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des données une discipline particulièrement attrayante, pour ne pas dire sexy. 2.2 Organiser son environnement de travail en R R est un langage de programmation dérivé du langage S, qui fut initialement lancé en 1976. R figure parmi les langages de programmation les plus utilisés au monde. Bien qu’il soit basé sur les langages statiques C et Fortran, R est un langage dynamique, c’est à dire que le code peut être exécuté ligne par ligne ou bloc par bloc: un avantage majeur pour des activités qui nécessitent des interactions fréquentes. Bien que R soit surtout utilisé pour le calcul statistique, il s’impose de plus en plus comme outil privilégié en sciences des données en raison des récents développements de modules d’analyse, de modélisation et de visualisation, dont plusieurs seront utilisés dans ce manuel. 2.3 Préparer son flux de travail Il existe de nombreuses manières d’utiliser R. Parmi celles-ci, j’en couvrirai 2: Installation classique Installation avec Anaconda Pour l’instant, j’écarte l’option infonuagique, qui n’est pas tout à fait au point. Les services de Azure Notebooks et de CoCalc peuvent néamoins s’avérer utiles… lorsqu’ils fonctionnent covenablement. 2.3.1 Installation classique Sur Windows ou Mac, dirigez-vous ici, téléchargez et installez. Sur Linux, ouvrez votre gestionnaire d’application, chercher r-base (Ubuntu, Debian), R-base (openSuse) ou R-core (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi installées: gcc, gcc-fortran, gcc-c++ et make), vous aurez peut-être besoin d’installer des librairies supplémentaires pour faire fonctionner certains modules. Note. Les modules présentés dans ce cours devraient être disponibles sur Linux, Windows et Mac. Ce n’est pas le cas pour tous les modules R. La plupart fonctionnent néanmoins sur Linux, dont les systèmes d’opération (je recommande Ubuntu ou l’une de ses dérivées) sont de bonnes options pour le calcul scientifique. À cette étape, R devrait fonctionner dans un interpréteur de commande . Si vous lancez R dans un terminal (chercher cmd dans le menu si vous êtes sur Windows), vous obtiendrez quelque chose comme ceci. Le symbole &gt; indique que R attend que vos instructions. Vous voilà dans un état méditatif devant l’indéchiffrable vide du terminal. Afin de travailler dans un environnement de travail plus convivial, je recommande l’installation de l’interface RStudio, gratuite et open source: téléchargez l’installateur et suivez les instructions. RStudio ressemble à ceci. En haut à droite se trouve un menu Project (None). Il s’agit d’un menu de vos projets. Je recommande d’utiliser ces projets avec RStudio, qui vous permettront de mieux gérer vos environnements de travail, en particulier en lien avec les chemins vers de vos données, graphiques, etc., que vous pouvez gérer relativement à l’emplacement de votre dossier de projet plutôt qu’à l’emplacement des fichiers sur votre machine. En haut à gauche, vous avez vos feuilles de calcul, qui apparaîtront en tant qu’onglets. Je recommande de prendre en main les R notebooks, dans lesquels vous pouvez écrire du texte en format Markdown (dont il sera question plus loin) entre des blocs de code. Ceci vous permet de détailler votre flux de travail. En bas à gauche apparaît la Console, où vous voyez les commandes envoyées à R ainsi que ses sorties. Si vous travaillez en format notebook, vous n’en aurez probablement pas besoin. En haut à droite, les différents onglets indiquent où vous en êtes dans vos calculs. En particulier, la liste sous Environment indique les objets qui ont été générés jusqu’alors. En bas à droite, on retrouve des onglets de nature variés. Files contient les sous-dossiers et fichiers du dossier de projet. Plots est l’endroit où apparaîtront vos graphiques. Packages contient la liste des modules déjà installés, ainsi qu’un outil de gestion des modules pour leur installation, leur désinstallation et leur mise à jour. Help affiche les fiches d’aide des fonctions (pour obtenir de l’aide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur F1). Enfin, l’onglet Viewer affichera les sorties HTML, en particulier les graphiques interactifs que vous générerez par exemple avec le module plotly. 2.3.2 R notebooks Les R notebooks offrent une approche de programmation littéraire, c’est-à-dire que vous écrivez votre code comme vous écrivez une article, une thèse ou une histoire. Cette approche permet de partager plus facilement vos codes, que ce soit avec une équipe de travail ou à la communauté scientifique pour accompagner un article scientifique en tant que matériel supplémentaire. Lorsque vous créez un notebook (File &gt; New file &gt; R notebook), les instructions de base apparaissent. Ajoutons que pour lancer du code ligne par ligne, vous pouvez surligner le code en question ou placez le curseur sur la ligne à exécuter, puis taper Ctrl + Enter. La sortie de R apparaîtra sous le bloc de code. Dans votre texte, vous pouvez ajouter des équations mathématiques en format Mathjax inspiré du format Latex, par exemple $a = \\sum_{i=1}^n x_i^2$ sera affiché comme \\(a = \\sum_{i=1}^n x_i^2\\) (pour aider dans l’édition d’équation, vous pouvez utiliser un éditeur dans les nuages). Pour les titres, les caractères gras, l’insertion d’image, les hyper-liens, les tableaux, etc., référez-vous à la documentation de Markdown. Si votre environnement de travail était un avion, R serait le moteur et RStudio serait le cockpit! 2.3.3 Installation avec Anaconda Si vous cherchez une trousse complète d’analyse de données, comprenant R et Python, vous pourrez préférer Anaconda. Une fois installée, vous pourrez isoler une environnement de travail sur R, ou même isoler des environnements de travail particuliers pour vos projets. Une manière conviviale de créer des environnements de travail est de passer par l’interface Anaconda navigator, que vous lancerez soit dans le menu Windows, soit en ligne de commande anaconda-navigator sous Mac et Linux, puis d’installer r-essentials, rstudio et jupyterlab dans l’onglet Environment. Vous pourrez aussi installer RStudio et Jupyter lab via l’onglet Home de Anaconda navigator. Dans l’environnement de base, installez le package nb_conda_kernels pour vous assurer que tous les noyaux (R, Python, etc.) installés dans les environnements de travail soient automatiquement accessibles dans Jupyter. Jupyter lab est une interface notebook semblable à R notebook. À vrai dire, l’utilisation de R en Anaconda n’est pas tout à fait au point, et pourrait poser problème pour l’installation de certains modules. Si vous optez pour cette option, préparez-vous à avoir à bidouiller un peu. 2.4 Premiers pas avec R R ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par cœur au fur et à mesure, ou que vous retrouverez en lançant des recherches sur internet. Par expérience personnelle, lorsque je travaille avec R, j’ai toujours un navigateur ouvert prêt à recevoir une question. Les étapes qui suivent sont des premiers pas. Elles ne feront pas de vous des ceintures noires delà programmation. La plupart des utilisateurs de R ont appris R en se pratiquant sur leurs données, en frappant des murs, en apprenant comment les escalader ou les contourner… Pour l’instant, ouvrez seulement un interpréteur de commande, et lancez R. Voyons si R est aussi libre qu’on le prétend. “La liberté, c’est la liberté de dire que deux et deux font quatre. Si cela est accordé, tout le reste suit.” - George Orwell, 1984 2 + 2 ## [1] 4 Et voilà. Les opérations mathématiques sont effectuées telles que l’on devrait s’attendre. 67.1 - 43.3 ## [1] 23.8 2 * 4 ## [1] 8 1 / 2 ## [1] 0.5 L’exposant peut être noté ^, comme c’est le cas dans Excel, ou ** comme c’est le cas en Python. 2^4 ## [1] 16 2**4 ## [1] 16 1 / 2 # utilisez des espaces de part et d&#39;autre des opérateurs (sauf pour l&#39;exposant) pour éclaircir le code ## [1] 0.5 R ne lit pas ce qui suit le caractère #. Cela vous laisse l’opportunité de commenter un code comprenant une séquence de plusieurs lignes. Remarquez également que la dernière opération comporte des espaces entre les nombres et l’opérateur /. Dans ce cas (ce n’est pas toujours le cas), les espaces ne signifient rien: ils aident seulement à éclaircir le code. Il existe des guides pour l’écriture de code en R. Je recommande le guide de style de Hadley Wickahm. Assigner des objets à des variables est fondamental en programmation. En R, on assigne traditionnellement avec la flèche &lt;-, mais vous verrez parfois le =, qui est davantage utilisé comme standard dans d’autres langages de programmation. Par exemple. a &lt;- 3 Techniquement, a pointe vers le nombre entier 3. Conséquemment, on peut effectuer des opérations sur a. a * 6 ## [1] 18 #A + 2 Le message d’erreur nous dit que A n’est pas défini. Sa version minuscule, a, l’est pourtant. La raison est que R considère la case dans la définition des objets. Utiliser la mauvaise case mène donc à des erreurs. Note. Les messages d’erreur ne sont pas toujours clairs, mais vous apprendrez à les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement! En général, le nom d’une variable doit toujours commencer par une lettre, et ne doit pas contenir de caractères réservés (espaces, +, *). Dans la définition des variables, plusieurs utilisent des symboles . pour délimiter les mots, mais la barre de soulignement _ est à préférer. En effet, dans d’autres langages de programmation comme Python, le . a une autre signification: son utilisation est à éviter autant que possible. Note. À ce stade, vous serez probablement plus à l’aise de copier-coller ces commandes dans votre terminal. rendement_arbre &lt;- 50 # pomme/arbre nombre_arbre &lt;- 300 # arbre nombre_pomme &lt;- rendement_arbre * nombre_arbre nombre_pomme ## [1] 15000 Comme chez la plupart des langages de programmation, R respecte les conventions des priorités des opérations mathéatiques. 10 - 9^0.5 * 2 ## [1] 4 2.4.1 Types de données Jusqu’à maintenant, nous n’avons utilisé que des nombres entiers (integer ou int) et des nombres réels (numeric ou float64). R inclut d’autres types. La chaîne de caractère (string ou character) contient un ou plusieurs symboles. Elle est définie entre des double-guillemets \" \" ou des apostrophes ' '. Il n’existe pas de standard sur l’utilisation de l’un ou de l’autre, mais en règle générale, on utilise les apostrophe pour les expressions courtes, contenant un simple mot ou séquence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour insérer des apostrophes dans une chaîne de caractère. a &lt;- &quot;L&#39;ours&quot; b &lt;- &quot;polaire&quot; paste(a, b) ## [1] &quot;L&#39;ours polaire&quot; On colle a et b avec la fonction paste. Notez que l’objet a a été défini précédemment. Il est possible en R de réassigner une variable, mais cela peut porter à confusion, jusqu’à générer des erreurs de calcul si une variable n’est pas assigné à l’objet auquel on voulait référer. Combien de caractères contient la chaîne \"L'ours polaire\"? R sait compter. Demandons-lui. c &lt;- paste(a, b) nchar(c) ## [1] 14 Quatorze, c’est bien cela (comptez “L’ours polaire”, en incluant l’espace). Comme paste, nchar est une fonction incluse par défaut dans l’environnement de travail de R: plus précisément, ces fonctions sont incluses dans le module base, inclut par défaut lorsque R est lancé. La fonction est appelée en écrivant nchar(). Mais une fonction de quoi? Des arguments, qui se trouvent entre les parenthèses. Dans ce cas, il y a un seul argument: c. En calcul scientifique, il est courant de lancer des requêtes sur si une résultat est vrai ou faux. a &lt;- 17 a &lt; 10 ## [1] FALSE a &gt; 10 ## [1] TRUE a == 10 ## [1] FALSE a != 10 ## [1] TRUE a == 17 ## [1] TRUE !(a == 17) ## [1] FALSE Je viens d’introduire un nouveau type de donnée: les données booléennes (boolean, ou logical), qui ne peuvent prendre que deux états - TRUE ou FALSE. En même temps, j’ai utilisé la fonction print parce que dans mon carnet, seule la dernière opération permet d’afficher le résultat. Si l’on veut forcer une sortie, on utilise print. Puis, on a vu plus haut que le symbole = est réservé pour assigner des objets: pour les tests d’égalité, on utilise le double égal, ==, ou != pour la non égalité. Enfin, pour inverser une donnée de type booléenne, on utilise le point d’exclamation !. 2.4.2 Les collections de données Les exercices précédents ont permis de présenter les types de données offerts par défaut sur R qui sont les plus importants pour le calcul scientifique: int (integer, ou nombre entier), numeric (nombre réel), character (string, ou chaîne de caractère) et logical (booléen). D’autres s’ajouteront tout au long du cours, comme les catégories (factor) et les unités de temps (date-heure). Lorsque l’on procède à des opérations de calcul en science, nous utilisons rarement des valeurs uniques. Nous préférons les organiser et les traiter en collections. Par défaut, R offre quatre types importants de collections: les vecteurs, les matrices, les listes et les tableaux. 2.4.2.1 Vecteurs D’abord, les vecteurs sont une série de variables de même type. Un vecteur est délimité par la fonction c( ) (c pour concaténation). Les éléments de la liste sont séparés par des virgules. espece &lt;- c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;) espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; Pour accéder aux éléments d’une liste, appelle la liste suivie de la position de l’objet désiré entre crochets. espece[1] ## [1] &quot;Petromyzon marinus&quot; espece[2] ## [1] &quot;Lepisosteus osseus&quot; espece[1:3] ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; espece[c(1, 3)] ## [1] &quot;Petromyzon marinus&quot; &quot;Amia calva&quot; On peut noter que le premier élément de la liste est noté 1, et non 0 comme c’est le cas de la plupart de langages. Le raccourcis 1:3 crée une liste de nombres entiers de 1 à 3 inclusivement, c’est-à-dire l’équivalent de c(1, 2, 3). En effet, on crée une liste d’indices pour soutirer des éléments d’une liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs éléments d’un vecteur. espece[-c(1, 3)] ## [1] &quot;Lepisosteus osseus&quot; &quot;Hiodon tergisus&quot; Pour ajouter un élément à notre liste, on peut utiliser la fonction c( ). espece &lt;- c(espece, &quot;Cyprinus carpio&quot;) espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; &quot;Cyprinus carpio&quot; Notez que l’on efface l’objet espece par une concaténation de l’objet espece, précédemment défini, et d’un autre élément. En lançant espece[3] &lt;- \"Lepomis gibbosus\", il est possible de changer une élément de la liste. espece[3] &lt;- &quot;Lepomis gibbosus&quot; espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Lepomis gibbosus&quot; ## [4] &quot;Hiodon tergisus&quot; &quot;Cyprinus carpio&quot; 2.4.2.2 Matrices Une matrice est un vecteur de dimension plus élevée que 1. En écologie, on dépasse rarement la deuxième dimension, quoi que les matrices en N dimensions soient courantes en modélisation mathématique. Je ne considérerai pour le moment que des matrices 2D. Comme c’est la cas des vecteurs, les matrices contiennent des valeurs de même type. En R, on peut attribuer aux matrices 2D des noms de ligne et de colonne. mat &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), ncol=3) mat ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 colnames(mat) &lt;- c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) rownames(mat) &lt;- c(&#39;site_1&#39;, &#39;site_2&#39;, &#39;site_3&#39;, &#39;site_4&#39;) mat ## A B C ## site_1 1 5 9 ## site_2 2 6 10 ## site_3 3 7 11 ## site_4 4 8 12 On peut soutirer les noms de colonne et les noms de ligne. Le résultat est un vecteur. colnames(mat) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; rownames(mat) ## [1] &quot;site_1&quot; &quot;site_2&quot; &quot;site_3&quot; &quot;site_4&quot; 2.4.2.3 Listes Les listes sont des collections hétérogènes dans lesquelles on peut placer les objets désirés, sans distinction: elles peuvent même inclure d’autres listes. Chacun des éléments de la liste peut être identifié par une clé. ma_liste &lt;- list(especes = c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;), site = &#39;A101&#39;, stations_meteos = c(&#39;746583&#39;, &#39;783786&#39;, &#39;856363&#39;)) ma_liste ## $especes ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; ## ## $site ## [1] &quot;A101&quot; ## ## $stations_meteos ## [1] &quot;746583&quot; &quot;783786&quot; &quot;856363&quot; Les éléments de la liste peuvent être soutirés par le nom de la clé ou par l’indice, de cette manière. ma_liste$especes ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; ma_liste[[1]] ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; Exercice. Accéder au deuxième élément du vecteur d’espèces dans la liste ma_liste. 2.4.2.4 Tableaux Enfin, le type de collection de données le plus important est le tableau, ou data.frame. Techniquement, il s’agit d’une liste composée de vecteurs de même longueur. Chaque colonne peut ainsi prendre un type de donnée indépendamment des autres colonnes. tableau &lt;- data.frame(espece = c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;), poids = c(10, 13, 21, 4), longueur = c(35, 44, 50, 8)) tableau ## espece poids longueur ## 1 Petromyzon marinus 10 35 ## 2 Lepisosteus osseus 13 44 ## 3 Amia calva 21 50 ## 4 Hiodon tergisus 4 8 En programmation classique en R (nous verrons plus loin la méthode tidyverse), les éléments d’un tableau se manipulent comme ceux d’une matrice et les colonnes peuvent être appelés comme les éléments d’une liste. tableau[, 2:3] ## poids longueur ## 1 10 35 ## 2 13 44 ## 3 21 50 ## 4 4 8 tableau$poids ## [1] 10 13 21 4 Vous verrez aussi, quoi que rarement, ce format, qui à la différence du format $ génère un tableau. tableau[&#39;poids&#39;] ## poids ## 1 10 ## 2 13 ## 3 21 ## 4 4 Le tableau est le format de collection à privilégier pour manipuler des données. Récemment, le format de tableau tibble a été créé par l’équipe de RStudio pour offrir un format plus moderne. 2.4.3 Les fonctions Lorsque vous écrivez une commande suivit de parenthèses, comme data.frame(especes = ...), vous demandez à R de passer à l’action en appelant une fonction. De manière très générale, une fonction transforme quelque chose en quelque chose d’autre. Par exemple, la fonction mean() prend une collection de nombre comme entrée, puis en sort vous devinez quoi. mean(tableau$poids) ## [1] 12 Les entrées sont appelés les arguments de la fonction. Leur définition est toujours disponible dans la documentation. Exercice. Familiarisez-vous avec la documentation de R en lançant ?mean. Truc: si vous avez pris de l’avance et que vous travaillez déjà en RStudio, mettez le terme en surbrillance, puis appuyez sur F1. Vous verrez dans la documentation que la fonction mean() demande trois arguments, x, trim et na.rm. Or nous avons seulement placé un vecteur, sans spécifier d’argument! En effet. En l’absence d’une définition des arguments, R supposera que les arguments dans la parenthèse, séparés par une virgule, sont présentés dans le même ordre que celui spécifié dans la définition de la fonction (celle qui est présentée dans le fichier d’aide). Dans le cas qui nous intéresse, mean(tableau$poids) est équivalent à mean(x = tableau$poids). Maintenant, selon la fiche d’aire l’argument na.rm est un valeur logique spécifiant si oui (TRUE) ou non (FALSE) les valeurs manquantes doivent être considérées (une moyenne d’un vecteur comprenant au moins un NA sera de NA). En ne spécifiant rien, R prend la valeur par défaut, telle que spécifiée dans la documentation. Il en va de même que l’argument trim, qui permet d’élaguer des valeurs extrêmes. Dans la fiche d’aide, mean(x, trim = 0, na.rm = FALSE, ...) signifie que par défaut, l’argument x est vide (il doit donc être spécifié), l’argument trim est de 0 et l’argument na.rm est FALSE. mean(c(6, 1, 7, 4, 9, NA, 1)) ## [1] NA mean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE) ## [1] 4.666667 Vous n’êtes pas emprisonné par les fonctions offertes par R. Vous pouvez installer des modules qui complètent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour l’instant, voyons comment vous pouvez créer vos propres fonctions. Disons que vous voulez créer une fonction qui calcule la sortie de \\(x^3-2y+a\\). Pour obtenir la réponse on a besoin des arguments x, y et a. La sortie de la fonction est ici triviale: la réponse de l’équation. L’opération function permet de prendre ça en charge. operation_f &lt;- function(x, y, a = 10) { return(x^3-2*y+a) } Notez que a a une valeur par défaut. La sortie de la fonction est ce qui se trouve entre les parenthèses de return. Vous pouvez maintenant utiliser la fonction operation_nl au besoin. operation_f(x = 2, y = 3, a = 1) ## [1] 3 Une telle fonction est peu utile. Mais l’utilisation de fonctions personnalisées vous permettra d’éviter de répéter la même opération plusieurs fois dans un flux de travail, en évitant de générer trop de code, donc aussi de potentielles erreurs. Personnellement, j’utilise les fonctions surtout pour générer des graphiques personnalisés. Exercice. Afin d’acquérir de l’autonomie, vous devrez être en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la tâche que vous désirer effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus à l’aise avec R jour après jour. L’exercice ici est de trouver par vous-même la commande qui vous permettra mesurer la longueur d’un vecteur. 2.4.4 Les boucles Les boucles permettent d’effectuer une même suite d’opérations sur plusieurs objets. Pour faire suite à notre exemple, nous désirons obtenir le résultat de l’opération f pour des paramètres que nous enregistrons dans ce tableau. params &lt;- data.frame(x = c(2, 4, 1, 5, 6), y = c(3, 4, 8, 1, 0), a = c(6, 1, 8, 2, 5)) params ## x y a ## 1 2 3 6 ## 2 4 4 1 ## 3 1 8 8 ## 4 5 1 2 ## 5 6 0 5 Nous créons un vecteur vide, puis nous itérons ligne par ligne en remplissant le vecteur. operation_res &lt;- c() for (i in 1:nrow(params)) { operation_res[i] &lt;- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3]) } operation_res ## [1] 8 57 -7 125 221 En faisant varier i sur des valeurs du vecteur donné par la séquence de nombre entiers de 1 au nombre de ligne du tableau de paramètres, nous demandons à R d’effectuer la suite d’opération entre les accolades {}. À chaque boucle, i prend une valeur de la séquence. i est utilisé ici comme indice de la ligne à soutirer du tableau params, qui correspond à l’indice dans le vecteur operation_res. Ainsi, chaque résultat est calculé dans l’ordre des lignes du tableau de paramètres et l’on pourra très bien y coller nos résultats: params$resultats &lt;- operation_res params ## x y a resultats ## 1 2 3 6 8 ## 2 4 4 1 57 ## 3 1 8 8 -7 ## 4 5 1 2 125 ## 5 6 0 5 221 Notez que puisque la colonne resultat n’existe pas dans le tableau params, R crée automatiquement une nouvelle colonne. Les boucles for vous permettront par exemple de générer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues à partir de conditions initiales différentes, et de les enregistrer dans un répertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut être effectué en R en quelques secondes. Un second outil est disponible pour les itérations: les boucles while. Elles effectue une opération tant qu’un critère n’est pas atteint. Elles sont utiles pour les opérations où l’on cherche une convergence. Je les couvre rapidement puisqu’elles sont rarement utilisées dans les flux de travail courants. En voici un petit exemple. x &lt;- 100 while (x &gt; 1.1) { x &lt;- sqrt(x) print(x) } ## [1] 10 ## [1] 3.162278 ## [1] 1.778279 ## [1] 1.333521 ## [1] 1.154782 ## [1] 1.074608 Nous avons initié x à une valeur de 100. Puis, tant que (while) le test x &gt; 1.1 est vrai, attribuer à x la nouvelle valeur calculée en extrayant la racine de la valeur précédente de x. Enfin, indiquer la valeur avec print. 2.4.5 Conditions: if, else if, else Si la condition 1 est remplie, effectuer une suite d’instruction 1. Si la condition 1 n’est pas remplie, et si la condition 2 est remplie, effectuer la suite d’instruction 2. Sinon, effectuer la suite d’instruction 3. Voilà comment on exprime une suite de conditions. Prenons l’exemple simple d’une discrétisation d’une valeur continue. Si \\(x&lt;10\\), il est classé comme faible. Si \\(10 \\leq x &lt;20\\), il est classé comme moyen. Si \\(x \\geq 20\\), il est classé comme élevé. Plaçons cette classification dans une fonction. classification &lt;- function(x, lim1=10, lim2=20) { if (x &lt; lim1) { categorie &lt;- &quot;faible&quot; } else if (x &lt; lim2) { categorie &lt;- &quot;moyen&quot; } else { categorie &lt;- &quot;élevé&quot; } return(categorie) } classification(-10) ## [1] &quot;faible&quot; classification(15.4) ## [1] &quot;moyen&quot; classification(1000) ## [1] &quot;élevé&quot; Une condition est définie avec le if, suivi du test à vrai ou faux entre parenthèse. Si le test retourne un vrai (TRUE), l’instruction entre accolades est exécutée. Si elle est fausse, on passe au suivant. Exercice. Explorer les commandes ifelse et cut et réfléchissez à la manière qu’ils pourraient être utilisées pour effectuer une discrétisation plus efficacement qu’avec les if et les else. 2.4.6 Installer et charger un module La plupart des opérations d’ordre général (comme les racines carrées, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles grâce aux modules de base de R, qui sont installés et chargés par défaut lors du démarrage de R. Des équipes de travail ont néanmoins développé plusieurs modules pour répondre à leurs besoins spécialisés, et les ont laissées disponibles au grand public dans des modules que vous pouvez installer d’un dépôt CRAN (le AppStore de R), d’un dépôt Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), d’un dépôt Github (dépôts décentralisés), etc. RStudio possède un pratique bouton Install qui vous permet d’y inscrire une liste de modules. Le navigateur anaconda offre aussi une interface d’installation. La commande R pour installer un module est install.packages(\"ggplot2\"), si par exemple vous désirez installer ggplot2, le module graphique par excellence en R. C’est la commande que RStudio lancera tout seul si vous lui demandez d’installer ggplot2. Les modules sont l’équivalent des applications spécialisées que vous installez sur un téléphone mobile. Pour les utiliser, il faut les ouvrir. Généralement, j’ouvre toutes les applications nécessaires à mon flux de travail au tout début de ma feuille de calcul (la prochaine cellule retournera un message d’erreur si les packages ne sont pas installés). library(&quot;tidyverse&quot;) # méta-package qui charge entre autres dplyr et ggplot2 ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.8 ## ✔ tidyr 0.8.2 ✔ stringr 1.3.1 ## ✔ readr 1.3.0 ✔ forcats 0.3.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(&quot;vegan&quot;) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-3 library(&quot;nlme&quot;) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse Les modules sont installés sur votre ordinateur à un endroit que vous pourrez retrouver avec la commande .libPaths() Exercice. À partir d’ici jusqu’à la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec l’interface! Quelques petits trucs: pour lancer une ligne, placez-votre curseur sur la ligne, puis appuyez sur Ctrl+Enter pour lancer une partie de code précise, mettez le en surbrillance, puis Ctrl+Enter utilisez toujours le gestionnaire de projets, en haut à droite! installez le module tidyverse lancez data(iris) pour obtenir un tableau d’exercice, puis cliquez sur l’objet dans la fenêtre environnement essayer R notebook 2.5 Enfin… Comme une langue, on n’apprend à s’exprimer en un langage informatique qu’en se mettant à l’épreuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre à coder en R. R n’aime pas l’ambiguïté. Une simple virgule mal placée et il ne sait plus quoi faire. Cela peut être frustrant au début, mais cette rigidité est nécessaire pour effectuer du calcul scientifique. Le copier-coller est votre ami. En gardant à l’esprit que vous être responsable de votre code et que vous respectez les droits d’auteur, n’ayez pas peur de copier-coller des lignes de code et de personnaliser par la suite. L’erreur que vous obtenez: d’autres l’on obtenue avant vous. Le site de question-réponse stackoverflow est une ressource inestimable où des gens ayant posté des questions ont reçu des réponses d’experts (les meilleures réponses et les meilleures questions apparaissent en premier). Apprenez à chercher intelligemment des réponses en formulant précisément vos questions! Étudiez et pratiquez. Les messages d’erreur en R sont courants, même chez les personnes expérimentées. La meilleure manière d’apprendre une langue est de la parler, d’étudier ses susceptibilités, de les tester dans une conversation, etc. "],
["chapitre-tableaux.html", "3 Organisation des données et opérations sur des tableaux 3.1 Les collections de données 3.2 Organiser un tableau de données 3.3 Formats de tableau 3.4 Entreposer ses données 3.5 Manipuler des données en mode tidyverse 3.6 Références", " 3 Organisation des données et opérations sur des tableaux Les données sont utilisées à chaque étape dans les flux de travail en sciences. Elles alimentent l’analyse et la modélisation. Les résultats qui en découlent sont aussi des données qui peuvent alimenter les travaux subséquents. Une bonne organisation des données facilitera le flux de travail. Dicton. Proportions de temps voué aux calcul scientifique: 80% de nettoyage de données mal organisées, 20% de calcul. Qu’est-ce qu’une donnée? De manière abstraite, il s’agit d’une valeur associée à une variable. Une variable peut être une dimension, une date, une couleur, le résultat d’un test statistique, à laquelle on attribue la valeur quantitative ou qualitative d’un chiffre, d’une chaîne de caractère, d’un symbole conventionné, etc. Par exemple, lorsque vous commandez un café latte végane, au latte est la valeur que vous attribuez à la variable type de café, et végane est la valeur de la variable type de lait. L’exemple est peut être horrible. J’ai besoin d’un café… Ce chapitre traite de l’importation, l’utilisation et l’exportation de données structurées, en R, sous forme de vecteurs, matrices, tableaux et ensemble de tableaux (bases de données). Bien qu’il soit toujours préférable d’organiser les structures qui accueilleront les données d’une expérience avant-même de procéder à la collecte de données, l’analyste doit s’attendre à réorganiser ses données en cours de route. Or, des données bien organisées au départ faciliteront aussi leur réorganisation. Ce chapitre débute avec quelques définitions: les données, les matrices, les tableaux et les bases de données, ainsi que leur signification en R. Puis nous verrons comment organiser un tableau selon quelques règles simples, mais importantes pour éviter les erreurs et les opérations fastidieuses pour reconstruire un tableau mal conçu. Ensuite, nous traiterons des formats de tableau courant, pour enfin passer à l’utilisation de dplyr, le module tidyverse pour effectuer des opérations sur les tableaux. 3.1 Les collections de données Dans le chapitre 2, nous avons survolé différents types d’objets: réels, entiers, chaînes de caractères et booléens. Les données peuvent appartenir à d’autres types: dates, catégories ordinales (ordonnées: faible, moyen, élevé) et nominales (non ordonnées: espèces, cultivars, couleurs, unité pédologique, etc.). Comme mentionné en début de chapitre, une donnée est une valeur associée à une variable. Les données peuvent être organisées en collections. Nous avons aussi vu au chapitre 2 que la manière privilégiée d’organiser des données était sous forme de tableau. De manière générale, un tableau de données est une organisation de données en deux dimensions, comportant des lignes et des colonnes. Il est préférable de respecter la convention selon laquelle les lignes sont des observations et les colonnes sont des variables. Ainsi, un tableau est une collection de vecteurs de même longueur, chaque vecteur représentant une variable. Chaque variable est libre de prendre le type de données approprié. La position d’une donnée dans le vecteur correspond à une observation. Imaginez que vous consignez des données de différents sites (A, B et C), et que chaque site possède ses propres caractéristiques. Il est redondant de décrire le site pour chaque observation. Vous préférerez créer deux tableaux: un pour décrire vos observations, et un autre pour décrire les sites. De cette manière, vous créez une collection de tableaux intereliés: une base de données. R peut soutirer des données des bases de données grâce au module DBI, qui n’est pas couvert à ce stade de développement du cours. Dans R, les données structurées en tableaux, ainsi que les opérations sur les tableaux, peuvent être gérés grâce aux modules readr, dplyr et tidyr, tous des modules faisant partie du méta-module tidyverse, devenu incontoutnable. Mais avant de se lancer dans l’utilisation de ces modules, voyons quelques règles à suivre pour bien structurer ses données en format tidy, un jargon du tidyverse qui signifie proprement organisé. 3.2 Organiser un tableau de données Afin de repérer chaque cellule d’un tableau, on attribue à chaque lignes et à chaque colonne colonnes un identifiant unique, que l’on nomme indice pour les lignes et entête pour les colonnes. Règle no 1. Une variable par colonne, une observation par ligne, une valeur par cellule. Les unités expérimentales sont décrits par une ou plusieurs variables par des chiffres ou des lettres. Chaque variable devrait être présente en une seule colonne, et chaque ligne devrait correspondre à une unité expérimentale où ces variables ont été mesurées. La règle parait simple, mais elle est rarement respectée. Prenez par exemple le tableau suivant. Site Traitement A Traitement B Traitement C Sainte-Zéphirine 4.1 8.2 6.8 Sainte-Aurélie 5.8 5.9 NA Saint-Serge-Étienne 2.9 3.4 4.6 Tableau 1. Rendements obtenus sur les sites expérimentaux selon les traitements. Qu’est-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations d’une même variable, le rendement, qui devient étalé sur plusieurs colonnes. À bien y penser, le type de traitement est une variable et le rendement en est une autre: Site Traitement Rendement Sainte-Zéphirine A 4.1 Sainte-Zéphirine B 8.2 Sainte-Zéphirine C 6.8 Sainte-Aurélie A 5.8 Sainte-Aurélie B 5.9 Sainte-Aurélie C NA Saint-Serge-Étienne A 2.9 Saint-Serge-Étienne B 3.4 Saint-Serge-Étienne C 4.6 Tableau 2. Rendements obtenus sur les sites expérimentaux selon les traitements. Plus précisément, l’expression à bien y penser suggère une réflexion sur la signification des données. Certaines variables peuvent parfois être intégrées dans une même colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contaminé peuvent être placés dans la même colonne “Concentration” ou déclinées en plusieurs colonnes Cu, Zn et Pb. La première version trouvera son utilité pour des créer des graphiques (chapitre 3), alors que la deuxième favorise le traitement statistique (chapitre 5). Il est possible de passer d’un format à l’autre grâce à la fonction gather() et spread() du module tidyr. Règle no 2. Un tableau par unité observationnelle: ne pas répéter les informations. Reprenons la même expérience. Supposons que vous mesurez la précipitation à l’échelle du site. Site Traitement Rendement Précipitations Sainte-Zéphirine A 4.1 813 Sainte-Zéphirine B 8.2 813 Sainte-Zéphirine C 6.8 813 Sainte-Aurélie A 5.8 642 Sainte-Aurélie B 5.9 642 Sainte-Aurélie C NA 642 Saint-Serge-Étienne A 2.9 1028 Saint-Serge-Étienne B 3.4 1028 Saint-Serge-Étienne C 4.6 1028 Tableau 3. Rendements obtenus sur les sites expérimentaux selon les traitements. Segmenter l’information en deux tableaux serait préférable. Site Précipitations Sainte-Zéphirine 813 Sainte-Aurélie 642 Saint-Serge-Étienne 1028 Tableau 4. Précipitations sur les sites expérimentaux. Les tableaux 2 et 4, ensemble, forment une base de données (collection organisée de tableaux). Les opérations de fusion entre les tableaux peuvent être effectuées grâce aux fonctions de jointure (left_join(), par exemple) du module tidyr. Règle no 3. Ne pas bousiller les données. Par exemple. Ajouter des commentaires dans des cellules. Si une cellule mérite d’être commentée, il est préférable de placer les commentaires soit dans un fichier décrivant le tableau de données, soit dans une colonne de commentaire juxtaposée à la colonne de la variable à commenter. Par exemple, si vous n’avez pas mesure le pH pour une observation, n’écrivez pas “échantillon contaminé” dans la cellule, mais annoter dans un fichier d’explication que l’échantillon no X a été contaminé. Si les commentaires sont systématique, il peut être pratique de les inscrire dans une colonne commentaire_pH. Inscription non systématiques. Il arrive souvent que des catégories d’une variable ou que des valeurs manquantes soient annotées différemment. Il arrive même que le séparateur décimal soit non systématique, parfois noté par un point, parfois par une virgule. Par exemple, une fois importés dans votre session, les catégories St-Ours et Saint-Ours seront traitées comme deux catégories distinctes. De même, les cellules correspondant à des valeurs manquantes ne devraient pas être inscrite parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention NA. Le plus simple est de laisser systématiquement ces cellules vides. Inclure des notes dans un tableau. La règle “une colonne, une variable” n’est pas respectée si on ajoute des notes un peu n’importe où sous ou à côté du tableau. Ajouter des sommaires. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, qu’est-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera considérée comme une observation supplémentaire. Inclure une hiérarchie dans le entêtes. Afin de consigner des données de texture du sol, comprenant la proportion de sable, de limon et d’argile, vous organisez votre entête en plusieurs lignes. Une ligne pour la catégorie de donnée, Texture, fusionnée sur trois colonnes, puis trois colonnes intitulées Sable, Limon et Argile. Votre tableau est joli, mais il ne pourra pas être importé conformément dans un votre session de calcul: on recherche une entête unique par colonne. Votre tableau de données devrait plutôt porter les entêtes Texture sable, Texture limon et Texture argile. Un conseil: réserver le travail esthétique à la toute fin d’un flux de travail. 3.3 Formats de tableau Plusieurs outils sont à votre disposition pour créer des tableaux. Je vous présente ici les plus communs. 3.3.1 xls ou xlsx Microsoft Excel est un logiciel de type tableur, ou chiffrier électronique. L’ancien format xls a été remplacé par le format xlsx avec l’arrivée de Microsoft Office 2010. Il s’agit d’un format propriétaire, dont l’alternative libre la plus connue est le format ods, popularisé par la suite bureautique LibreOffice. Les formats xls, xlsx ou ods sont davantage utilisés comme outils de calcul que d’entreposage de données. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. Je ne les recommande pas pour stocker des données. 3.3.2 csv Le format csv, pour comma separated values, est un fichier texte, que vous pouvez ouvrir avec n’importe quel éditeur de texte brut (Bloc note, Atom, Notepad++, etc.). Chaque colonne doit être délimitée par un caractère cohérent (conventionnellement une virgule, mais en français un point-virgule ou une tabulation pour éviter la confusion avec le séparateur décimal) et chaque ligne du tableau est un retour de ligne. Il est possible d’ouvrir et d’éditer les fichiers csv dans un éditeur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.). Encodage des fichiers texte. Puisque le format csv est un fichier texte, un souci particulier doit être porté sur la manière dont le texte est encodé. Les caractères accentués pourrait être importer incorrectement si vous importez votre tableau en spécifiant le mauvais encodage. Pour les fichiers en langues occidentales, l’encodage UTF-8 devrait être utilisé. Toutefois, par défaut, Excel utilise un encodage de Microsoft. Si le csv a été généré par Excel, il est préférable de l’ouvrir avec votre éditeur texte et de l’enregistrer dans l’encodage UTF-8. 3.3.3 json Comme le format csv, le format json indique un fichier en texte clair. Il est utilisé davantage pour le partage de données des applications web. En analyse et modélisation, ce format est surtout utilisé pour les données géoréférencées. L’encodage est géré de la même manière qu’un fichier csv. 3.3.4 SQLite SQLite est une application pour les bases de données relationnelles de type SQL qui n’a pas besoin de serveur pour fonctionner. Les bases de données SQLite sont encodés dans des fichiers portant l’extension db, qui peuvent être facilement partagés. 3.3.5 Suggestion En csv pour les petits tableaux, en sqlite pour les bases de données plus complexes. Ce cours se concentre toutefois sur les données de type csv. 3.4 Entreposer ses données La manière la plus sécurisée pour entreposer ses données est de les confiner dans une base de données sécurisée sur un serveur sécurisé dans un environnement sécurisé et d’encrypter les communications. C’est aussi la manière la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou d’autres options similaires, peuvent être pratiques pour les backups et le partage des données avec une équipe de travail (qui risque en retour de bousiller vos données). Le suivi de version est possible chez certains fournisseurs d’espace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de développement (comme GitHub et GitLab) sont plus appropriés (couverts au chapitre 11). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas d’erreurs et (2) un petit fichier décrivant les changements effectués sur les données. 3.5 Manipuler des données en mode tidyverse Le méta-module tidyverse regroupe une collection de précieux modules pour l’analyse de données en R. Il permet d’importer des données dans votre session de travail avec readr, de les explorer avec le module de visualisation ggplot2, de les transformer avec tidyr et dplyr et de les exporter avec readr. Les tableaux de classe data.frame, comme ceux de la plus moderne classe tibble, peuvent être manipulés à travers le flux de travail pour l’analyse et la modélisation (chapitres suivants). Comme c’était le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalités qui sont offertes dans le tidyverse. 3.5.1 Importer vos données dans voter session de travail Supposons que vous avec bien organisé vos données en mode tidy. Pour les importer dans votre session et commencer à les inspecter, vous lancerez une des commandes du module readr, décrites dans la documentation dédiée. read_csv() si le séparateur de colonne est une virgule read_csv2() si le séparateur de colonne est un point-virgule et que le séparateur décimal est une virgule read_tsv() si le séparateur de colonne est une tabulation read_table() si le séparateur de colonne est un espace blanc read_delim() si le séparateur de colonne est un autre caractère (comme le point-virgule) que vous spécifierez dans l’argument delim = \";\" Les principaux arguments sont les suivants. file: le chemin vers le fichier. Ce chemin peut aussi bien être une adresse locale (data/…) qu’une adresse internet (https://…). delim: le symbole délimitant les colonnes dans le cas de read_delim. col_names: si TRUE, la première ligne est l’entête du tableau, sinon FALSE. Si vous spécifiez un vecteur numérique, ce sont les numéros des lignes utilisées pour le nom de l’entête. Si vous utilisez un vecteur de caractères, ce sont les noms des colonnes que vous désirez donner à votre tableau. na: le symbole spécifiant une valeur manquante. L’argument na='' signifie que les cellules vides sont des données manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple na = c(\"\", \"NA\", \"NaN\", \".\", \"-\"). local: cet argument prend une fonction local() qui peut inclure des arguments de format de temps, mais aussi d’encodage (voir documentation) D’autres arguments peuvent être spécifiés au besoin, et les répéter ici dupliquerait l’information de la documentation de la fonction read_csv de readr. Je déconseille d’importer des données en format xls ou xlsx. Si toutefois cela vous convient, je vous réfère au module readxl. L’aide-mémoire de readr est à afficher près de soi. Aide-mémoire de readr, source: https://www.rstudio.com/resources/cheatsheets/ Nous allons charger des données de culture de la chicouté (Rubus chamaemorus), un petit fruit nordique, tiré de Parent et al. (2013). Ouvrons d’abord le fichier pour vérifier les séparateurs de colonne et de décimale. Le séparateur de colonne est un point-virgule et le décimal est une virgule. Avec Atom, mon éditeur texte préféré, je vais dans Edit &gt; Select Encoding et j’obtiens bien le UTF-8. Nous allons donc utiliser read_csv2() avec ses arguments par défaut. library(&quot;tidyverse&quot;) chicoute &lt;- read_csv2(&#39;data/chicoute.csv&#39;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## .default = col_double(), ## CodeTourbiere = col_character(), ## Ordre = col_character(), ## Traitement = col_character(), ## DemiParcelle = col_character(), ## SousTraitement = col_character() ## ) ## See spec(...) for full column specifications. Quelques commandes utiles inspecter le tableau: head() présente l’entête du tableau, soit ses 6 premières lignes str() et glimpse() présentent les variables du tableau et leur type - glimpse()est la fonction tidyverse et str() est la fonction classique (je préfère str()) summary() présente des statistiques de base du tableau names() ou colnames() sort les noms des colonnes sous forme d’un vecteur dim() donne les dimensions du tableau, ncol() son nombre de colonnes et nrow() son nombre de lignes skim est une fonction du module skimr montrant un portrait graphique et numérique du tableau Extra 1. Plusieurs modules ne se trouvent pas dans les dépôt CRAN, mais sont disponibles sur GitHub. Pour les installer, installez d’abord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr. Extra 2. Lorsque je désire utiliser une fonction, mais sans charger le module dans la session, j’utilise la notation module::fonction. Comme dans ce cas, pour skimr. # devtools::install_github(&quot;ropenscilabs/skimr&quot;) skimr::skim(chicoute) ## Skim summary statistics ## n obs: 90 ## n variables: 31 ## ## ── Variable type:character ──────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## CodeTourbiere 0 90 90 1 4 0 12 ## DemiParcelle 50 40 90 4 5 0 2 ## Ordre 0 90 90 1 2 0 20 ## SousTraitement 50 40 90 1 7 0 3 ## Traitement 50 40 90 6 11 0 2 ## ## ── Variable type:numeric ────────────────────────────────────────────────── ## variable missing complete n mean sd ## Al_pourc 0 90 90 0.0027 0.0013 ## B_pourc 0 90 90 0.0031 0.00067 ## C_pourc 0 90 90 50.28 1.61 ## Ca_pourc 0 90 90 0.39 0.1 ## Cu_pourc 0 90 90 0.00041 0.00064 ## Fe_pourc 0 90 90 0.015 0.0059 ## FemelleAvorte_nombre_m2 4 86 90 8.49 14.52 ## FemelleFruit_nombre_m2 18 72 90 19.97 23.79 ## ID 0 90 90 45.5 26.12 ## K_pourc 0 90 90 0.89 0.27 ## Latitude_m 0 90 90 5701839.86 1915.5 ## Longitude_m 0 90 90 485295.54 6452.33 ## Mg_pourc 0 90 90 0.5 0.085 ## Mn_pourc 0 90 90 0.033 0.025 ## N_pourc 0 90 90 2.2 0.4 ## P_pourc 0 90 90 0.14 0.037 ## Rendement_g_5m2 50 40 90 13.33 21.56 ## S_pourc 0 90 90 0.13 0.039 ## Site 0 90 90 6.33 5.49 ## SterileFleur_nombre_m2 4 86 90 0.26 0.71 ## TotalFemelle_nombre_m2 4 86 90 27.53 29.83 ## TotalFloral_nombre_m2 4 86 90 52.08 40.41 ## TotalMale_nombre_m2 4 86 90 24.4 26.87 ## TotalRamet_nombre_m2 0 90 90 251.26 156.06 ## TotalVegetatif_nombre_m2 4 86 90 199.02 139.13 ## Zn_pourc 0 90 90 0.0067 0.0021 ## p0 p25 p50 p75 p100 ## 9e-04 0.0019 0.0024 0.0033 0.0093 ## 0.0018 0.0026 0.0032 0.0035 0.0042 ## 46.72 49.14 50.45 51.58 53.83 ## 0.19 0.32 0.37 0.44 0.88 ## 3.7e-05 3.7e-05 0.00021 0.00046 0.0042 ## 0.0091 0.011 0.014 0.017 0.052 ## 0 1.27 3.07 10.14 76.8 ## 0.4 7.64 11.46 22.83 157.88 ## 1 23.25 45.5 67.75 90 ## 0.35 0.69 0.86 1.13 1.54 ## 5695688 5701868.5 5702129 5702537 5706394 ## 459873 485927 486500 486544.75 491955 ## 0.36 0.45 0.48 0.52 0.86 ## 0.0023 0.012 0.028 0.05 0.1 ## 1.53 1.89 2.12 2.58 3.1 ## 0.071 0.12 0.14 0.16 0.23 ## 0 0 0.95 15.63 72.44 ## 0.07 0.11 0.13 0.14 0.28 ## 1 2 4 9 20 ## 0 0 0 0 3.82 ## 2.55 10.34 17.19 31.96 187.17 ## 4.8 22.92 43 69.52 198.62 ## 0 3.3 15.28 36.51 104.41 ## 40.74 122.7 212.92 347.8 651.9 ## 22.92 86.26 161.25 263.78 580.6 ## 0.0033 0.0055 0.0063 0.0072 0.016 ## hist ## ▆▇▅▂▁▁▁▁ ## ▃▂▅▃▃▇▁▅ ## ▂▃▆▃▅▇▂▁ ## ▂▇▇▃▂▁▁▁ ## ▇▁▁▁▁▁▁▁ ## ▇▅▂▁▁▁▁▁ ## ▇▂▁▁▁▁▁▁ ## ▇▂▁▁▁▁▁▁ ## ▇▇▇▇▇▇▇▇ ## ▃▂▇▆▃▆▂▁ ## ▁▁▁▂▇▆▁▁ ## ▁▁▁▁▁▁▇▁ ## ▃▇▆▂▁▁▁▁ ## ▇▃▃▃▂▁▁▁ ## ▃▆▇▆▂▅▃▂ ## ▆▂▇▇▇▃▂▁ ## ▇▁▁▁▁▁▁▁ ## ▂▆▇▂▁▁▁▁ ## ▇▅▁▁▁▁▁▁ ## ▇▁▁▁▁▁▁▁ ## ▇▂▁▁▁▁▁▁ ## ▇▇▃▂▁▁▁▁ ## ▇▃▂▂▁▁▁▁ ## ▇▃▇▂▂▂▂▁ ## ▇▇▆▃▂▂▂▁ ## ▂▇▅▁▁▁▁▁ Exercice. Inspectez le tableau. 3.5.2 Comment sélectionner et filtrer des données? On utiliser le terme sélectionner lorsque l’on désire choisir une ou plusieurs lignes et colonnes d’un tableau (la plupart du temps des colonnes). L’action de filtrer signifie de sélectionner des lignes selon certains critères. 3.5.2.1 Sélectionner Voici trois manières de sélectionner une colonne en R. Une méthode rapide mais peu expressive consiste à indiquer les valeurs numériques de l’indice de la colonne entre des crochets. Il s’agit d’appeler le tableau suivit de crochets. L’intérieur des crochets comprend deux éléments séparés par une virgule. Le premier élément sert à filtrer selon l’indice, le deuxième sert à sélectionner selon l’indice. Ainsi: chicoute[, 1]: sélectionner la première colonne chicoute[, 1:10]: sélectionner les 10 premières colonnes chicoute[, c(2, 4, 5)]: sélectionner les colonnes 2, 4 et 5 chicoute[c(10, 13, 20), c(2, 4, 5)]: sélectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20. Une autre méthode rapide, mais plus expressive, consiste à appeler le tableau, suivi du symbole $, puis le nom de la colonne. Truc. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. Après avoir entrer le $, taper sur la touche de tabulation: vous pourrez sélectionner la colonne dans une liste défilante. Une autre option est d’inscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, c’est-à-dire chicoute[c(\"Site\", \"Latitude_m\", \"Longitude_m\")]. Enfin, dans une séquence d’opérations en mode pipeline (chaque opération est mise à la suite de la précédente en plaçant le pipe %&gt;% entre chacune), il peut être préférable de sélectionner des colonnes avec la fonction select(), i.e. chicoute %&gt;% select(Site, Latitude_m, Longitude_m) La fonction select() permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un - (signe de soustraction) devant le nom de la colonne. D’autre arguments de select() permettent une sélection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages: chicoute %&gt;% select(ends_with(&quot;pourc&quot;)) %&gt;% head(3) ## # A tibble: 3 x 13 ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.5 1.72 0.108 1.21 0.435 0.470 0.0976 0.00258 ## 2 51.3 2.18 0.0985 1.22 0.337 0.439 0.0996 0.00258 ## 3 50.6 2.12 0.0708 1.05 0.373 0.420 0.104 0.00258 ## # ... with 5 more variables: Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, ## # Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt; 3.5.2.2 Filtrer Comme c’est le cas de la sélection, on pourra filtrer un tableau de plusieurs manières. J’ai déjà présenté comment filtrer selon les indices des lignes. Les autres manières reposent néanmoins sur une opération logique ==, &lt;, &gt; ou %in% (le %in% signifie se trouve parmi et peut être suivi d’un vecteur de valeur que l’on désire accepter). Les conditions booléennes peuvent être combinées avec les opérateurs et, &amp;, et ou, |. Pour rappel, Opération Résultat Vrai et Vrai Vrai Vrai et Faux Faux Faux et Faux Faux Vrai ou Vrai Vrai Vrai ou Faux Vrai Faux ou Faux Faux La méthode classique consiste à appliquer une opération logique entre les crochets, par exemple chicoute[chicoute$CodeTourbiere == \"BEAU\", ] La méthode tidyverse, plus pratique en mode pipeline, passe par la fonction filter(), i.e. chicoute %&gt;% filter(CodeTourbiere == &quot;BEAU&quot;) Combiner le tout. chicoute %&gt;% filter(Ca_pourc &lt; 0.4 &amp; CodeTourbiere %in% c(&quot;BEAU&quot;, &quot;MB&quot;, &quot;WTP&quot;)) %&gt;% select(contains(&quot;pourc&quot;)) ## # A tibble: 4 x 13 ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.3 2.18 0.0985 1.22 0.337 0.439 0.0996 0.00258 ## 2 50.6 2.12 0.0708 1.05 0.373 0.420 0.104 0.00258 ## 3 53.8 2.04 0.115 0.947 0.333 0.472 0.106 0.00258 ## 4 52.6 2.11 0.0847 0.913 0.328 0.376 0.111 0.00296 ## # ... with 5 more variables: Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, ## # Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt; 3.5.3 Le format long et le format large Dans le tableau chicoute, chaque élément possède sa propre colonne. Si l’on voulait mettre en graphique les boxplot des facettes de concentrations d’azote, de phosphore et de potassium dans les différentes tourbières, il faudrait obtenir une seule colonne de concentrations. Pour ce faire, nous utiliserons la fonction gather(). Le premier argument est le nom de la colonne des variables, le deuxième est le nom de la nouvelle colonne des valeurs. La suite consiste à décrire les colonnes à inclure ou à exclure. Dans le cas qui suit, j’exclue CodeTourbiere de la refonte j’utilise sample_n() pour présenter un échantillon du résultat. chicoute_long &lt;- chicoute %&gt;% select(CodeTourbiere, N_pourc, P_pourc, K_pourc) %&gt;% gather(key = element, value = concentration, -CodeTourbiere) chicoute_long %&gt;% sample_n(10) ## # A tibble: 10 x 3 ## CodeTourbiere element concentration ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BEAU K_pourc 1.19 ## 2 2 P_pourc 0.224 ## 3 MR P_pourc 0.113 ## 4 2 N_pourc 2.61 ## 5 2 N_pourc 2.59 ## 6 1 P_pourc 0.150 ## 7 2 N_pourc 2.70 ## 8 NBM P_pourc 0.105 ## 9 BEAU N_pourc 2.12 ## 10 MB P_pourc 0.103 L’opération inverse est spread(). Pour que cette opération fonctionne, spread() a besoin d’une colonne ayant un identifiant unique. chicoute_long$ID &lt;- 1:nrow(chicoute_long) Nous pouvons enlever cet identifiant une fois l’opération effectuée. chicoute_large &lt;- chicoute_long %&gt;% spread(key=element, value=concentration, fill=0) %&gt;% select(-ID) chicoute_large %&gt;% sample_n(10) ## # A tibble: 10 x 4 ## CodeTourbiere K_pourc N_pourc P_pourc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0 2.78 0 ## 2 BEAU 1.21 0 0 ## 3 2 1.01 0 0 ## 4 2 0 0 0.181 ## 5 WTP 0 1.98 0 ## 6 SSP 0 1.66 0 ## 7 BEAU 0 0 0.0909 ## 8 1 0.928 0 0 ## 9 WTP 0 1.63 0 ## 10 SSP 0 0 0.131 3.5.4 Combiner des tableaux Nous avons introduit plus haut la notion de base de données. Nous voudrions peut-être utiliser le code des tourbières pour inclure leur nom, le type d’essai mené à ces tourbières, etc. Importons d’abord le tableau des noms liés aux codes. tourbieres &lt;- read_csv2(&quot;data/chicoute_tourbieres.csv&quot;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## Tourbiere = col_character(), ## CodeTourbiere = col_character(), ## Type = col_character(), ## TypeCulture = col_character() ## ) tourbieres ## # A tibble: 11 x 4 ## Tourbiere CodeTourbiere Type TypeCulture ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Beaulieu BEAU calibration naturel ## 2 Brador Path BP calibration naturel ## 3 Lichen (BS2E) 2 validation cultive sec ## 4 Mannys Brook MB calibration naturel ## 5 Middle Bay Road MR calibration naturel ## 6 North Est of Smelt Pond NESP calibration naturel ## 7 North of Blue Moon NBM calibration naturel ## 8 South of Smelt Pond SSP calibration naturel ## 9 Sphaigne (BS2F) BS2 validation cultive sec ## 10 Sphaigne (BS2F) 1 calibration naturel ## 11 West of Trout Pond WTP calibration naturel Notre information est organisée en deux tableaux, liés par la colonne CodeTourbiere. Comment fusionner l’information pour qu’elle puisse être utilisée dans son ensemble? La fonction left_join effectue cette opération typique avec les bases de données. chicoute_merge &lt;- left_join(x = chicoute, y = tourbieres, by = &quot;CodeTourbiere&quot;) # ou bien chicoute %&gt;% left_join(y = tourbieres, by = &quot;CodeTourbiere&quot;) chicoute_merge %&gt;% sample_n(4) ## # A tibble: 4 x 34 ## ID CodeTourbiere Ordre Site Traitement DemiParcelle SousTraitement ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 34 MB C 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 31 MB C 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 79 1 4 7 fertilisa… right Control ## 4 59 SSP F 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # ... with 27 more variables: Latitude_m &lt;dbl&gt;, Longitude_m &lt;dbl&gt;, ## # Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;, ## # TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;, ## # TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;, ## # FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;, ## # SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, ## # P_pourc &lt;dbl&gt;, K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, ## # S_pourc &lt;dbl&gt;, B_pourc &lt;dbl&gt;, Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, ## # Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt;, Tourbiere &lt;chr&gt;, ## # Type &lt;chr&gt;, TypeCulture &lt;chr&gt; D’autres types de jointures sont possibles, et décrites en détails dans la documentation. Garrick Aden-Buie a préparé de jolies animations pour décrire les différents types de jointures. left_join(x, y) colle y à x seulement ce qui dans y correspond à ce que l’on trouve dans x. right_join(x, y) colle y à x seulement ce qui dans x correspond à ce que l’on trouve dans y. inner_join(x, y) colle x et y en excluant les lignes où au moins une variable de joint est absente dans x et y. full_join(x, y)garde toutes les lignes et les colonnes de x et y. 3.5.5 Opérations sur les tableaux Les tableaux peuvent être segmentés en éléments sur lesquels on calculera ce qui nous chante. On pourrait vouloir obtenir: la somme avec la function sum() la moyenne avec la function mean() ou la médiane avec la fonction median() l’écart-type avec la function sd() les maximum et minimum avec les fonctions min() et max() un décompte d’occurrence avec la fonction n() ou count() Par exemple, mean(chicoute$Rendement_g_5m2, na.rm = TRUE) ## [1] 13.32851 En mode classique, pour effectuer des opérations sur des tableaux, on utilisera la fonction apply(). Cette fonction prend, comme arguments, le tableau, l’axe (opération par ligne = 1, opération par colonne = 2), puis la fonction à appliquer. apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 2, mean) ## C_pourc N_pourc P_pourc K_pourc Ca_pourc ## 5.027911e+01 2.199411e+00 1.388959e-01 8.887000e-01 3.884391e-01 ## Mg_pourc S_pourc B_pourc Cu_pourc Zn_pourc ## 4.980142e-01 1.347177e-01 3.090922e-03 4.089891e-04 6.662155e-03 ## Mn_pourc Fe_pourc Al_pourc ## 3.345239e-02 1.514885e-02 2.694979e-03 Les opération peuvent aussi être effectuées par ligne, par exemple une somme (je garde seulement les 10 premiers résultats). apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 1, sum)[1:10] ## [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 ## [8] 55.10991 55.06295 55.16774 La fonction à appliquer peut être personnalisée, par exemple: apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 2, function(x) (prod(x))^(1/length(x))) ## C_pourc N_pourc P_pourc K_pourc Ca_pourc ## 50.253429104 2.165246915 0.133754530 0.846193827 0.376192724 ## Mg_pourc S_pourc B_pourc Cu_pourc Zn_pourc ## 0.491763884 0.129900753 0.003014675 0.000000000 0.006408775 ## Mn_pourc Fe_pourc Al_pourc ## 0.024140327 0.014351745 0.002450982 Vous reconnaissez cette fonction? C’était la moyenne géométrique (la fonction prod() étant le produit d’un vecteur). En mode tidyverse, on aura besoin principalement des fonction suivantes: group_by() pour effectuer des opérations par groupe, l’opération group_by() sépare le tableau en plusieurs petits tableaux, en attendant de les recombiner. C’est un peu l’équivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitre 4. summarise() pour réduire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou s’il y a lieu sur chaque petit tableau segmenté. Il en existe quelques variantes. summarise_all() applique la fonction à toutes les colonnes summarise_at() applique la fonction aux colonnes spécifiées summarise_if() applique la fonction aux colonnes qui ressortent comme TRUE selon une opération booléenne mutate() pour ajouter une nouvelle colonne Si l’on désire ajouter une colonne à un tableau, par exemple le sommaire calculé avec summarise(). À l’inverse, la fonction transmute() retournera seulement le résultat, sans le tableau à partir duquel il a été calculé. De même que summarise(), mutate() et transmute() possèdent leurs équivalents _all(), _at() et _if(). arrange() pour réordonner le tableau On a déjà couvert arrange() dans le chapitre 3. Rappelons que cette fonction n’est pas une opération sur un tableau, mais plutôt un changement d’affichage en changeant l’ordre d’apparition des données. Ces opérations sont décrites dans l’aide-mémoire Data Transformation Cheat Sheet. Aide-mémoire de dplyr, source: https://www.rstudio.com/resources/cheatsheets/ Pour effectuer des statistiques par colonne, on utilisera summarise_all() étant donnée que l’on désire un sommaire sur toutes les variables sélectionnées. Pour spécifier que l’on désire la moyenne et l’écart-type on inscrit les noms des fonctions dans funs(). chicoute %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% summarise_all(funs(mean, sd)) ## # A tibble: 1 x 26 ## C_pourc_mean N_pourc_mean P_pourc_mean K_pourc_mean Ca_pourc_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 50.3 2.20 0.139 0.889 0.388 ## # ... with 21 more variables: Mg_pourc_mean &lt;dbl&gt;, S_pourc_mean &lt;dbl&gt;, ## # B_pourc_mean &lt;dbl&gt;, Cu_pourc_mean &lt;dbl&gt;, Zn_pourc_mean &lt;dbl&gt;, ## # Mn_pourc_mean &lt;dbl&gt;, Fe_pourc_mean &lt;dbl&gt;, Al_pourc_mean &lt;dbl&gt;, ## # C_pourc_sd &lt;dbl&gt;, N_pourc_sd &lt;dbl&gt;, P_pourc_sd &lt;dbl&gt;, ## # K_pourc_sd &lt;dbl&gt;, Ca_pourc_sd &lt;dbl&gt;, Mg_pourc_sd &lt;dbl&gt;, ## # S_pourc_sd &lt;dbl&gt;, B_pourc_sd &lt;dbl&gt;, Cu_pourc_sd &lt;dbl&gt;, ## # Zn_pourc_sd &lt;dbl&gt;, Mn_pourc_sd &lt;dbl&gt;, Fe_pourc_sd &lt;dbl&gt;, ## # Al_pourc_sd &lt;dbl&gt; On utilisera group_by() pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe. chicoute %&gt;% group_by(CodeTourbiere) %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% summarise_all(funs(mean, sd)) ## Adding missing grouping variables: `CodeTourbiere` ## # A tibble: 12 x 27 ## CodeTourbiere C_pourc_mean N_pourc_mean P_pourc_mean K_pourc_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 50.7 2.26 0.156 0.880 ## 2 2 49.2 2.76 0.181 1.12 ## 3 BEAU 51.7 2.00 0.0967 1.12 ## 4 BP 51.2 2.05 0.158 0.747 ## 5 BS2 52.5 2.08 0.103 1.12 ## 6 MB 51.5 2.15 0.109 0.675 ## 7 MR 50.6 1.99 0.127 0.830 ## 8 NBM 51.6 2.01 0.127 0.854 ## 9 NESP 48.3 1.76 0.135 0.945 ## 10 NTP 51.3 1.83 0.0873 0.402 ## 11 SSP 48.7 1.83 0.130 0.700 ## 12 WTP 47.8 1.79 0.0811 0.578 ## # ... with 22 more variables: Ca_pourc_mean &lt;dbl&gt;, Mg_pourc_mean &lt;dbl&gt;, ## # S_pourc_mean &lt;dbl&gt;, B_pourc_mean &lt;dbl&gt;, Cu_pourc_mean &lt;dbl&gt;, ## # Zn_pourc_mean &lt;dbl&gt;, Mn_pourc_mean &lt;dbl&gt;, Fe_pourc_mean &lt;dbl&gt;, ## # Al_pourc_mean &lt;dbl&gt;, C_pourc_sd &lt;dbl&gt;, N_pourc_sd &lt;dbl&gt;, ## # P_pourc_sd &lt;dbl&gt;, K_pourc_sd &lt;dbl&gt;, Ca_pourc_sd &lt;dbl&gt;, ## # Mg_pourc_sd &lt;dbl&gt;, S_pourc_sd &lt;dbl&gt;, B_pourc_sd &lt;dbl&gt;, ## # Cu_pourc_sd &lt;dbl&gt;, Zn_pourc_sd &lt;dbl&gt;, Mn_pourc_sd &lt;dbl&gt;, ## # Fe_pourc_sd &lt;dbl&gt;, Al_pourc_sd &lt;dbl&gt; Pour obtenir des statistiques à chaque ligne, mieux vaut utiliser apply(), tel que vu précédemment. Le point, ., représente le tableau dans la fonction. chicoute %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% apply(., 1, sum) ## [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 ## [8] 55.10991 55.06295 55.16774 56.41123 55.47917 55.43537 55.79175 ## [15] 55.44561 54.85448 54.34262 55.03075 54.40533 51.89319 54.70172 ## [22] 54.62176 54.30250 53.86976 53.44731 53.86244 52.43280 54.34978 ## [29] 53.96756 51.46672 55.44267 54.70350 55.30711 56.16200 56.64710 ## [36] 55.95499 54.76370 54.32775 54.95419 53.37094 53.07855 53.04541 ## [43] 52.09520 52.40456 51.92376 53.33248 56.56405 56.35004 56.27185 ## [50] 55.56986 53.81654 55.39638 55.51961 54.88098 54.74774 51.08921 ## [57] 51.31462 53.46819 53.15640 52.82020 57.78038 57.94636 56.65558 ## [64] 56.28845 55.54463 56.51751 55.36497 56.00594 55.64247 56.56967 ## [71] 56.81674 55.87070 55.72308 56.14116 56.42611 55.35650 54.90469 ## [78] 54.03674 53.42991 53.99334 53.09085 53.23222 53.28212 53.63192 ## [85] 53.48102 52.31131 51.72026 51.10534 51.49055 51.59297 Revenons à notre tableau des espèces menacées. especes_menacees &lt;- read_csv(&#39;data/WILD_LIFE_16082018051732754.csv&#39;) ## Parsed with column specification: ## cols( ## IUCN = col_character(), ## `IUCN Category` = col_character(), ## SPEC = col_character(), ## Species = col_character(), ## COU = col_character(), ## Country = col_character(), ## `Unit Code` = col_character(), ## Unit = col_character(), ## `PowerCode Code` = col_double(), ## PowerCode = col_character(), ## `Reference Period Code` = col_logical(), ## `Reference Period` = col_logical(), ## Value = col_double(), ## `Flag Codes` = col_logical(), ## Flags = col_logical() ## ) Nous avions exécuté le pipeline suivant. especes_menacees %&gt;% filter(IUCN == &#39;CRITICAL&#39;) %&gt;% select(Country, Value) %&gt;% group_by(Country) %&gt;% summarise(n_critical_species = sum(Value)) %&gt;% arrange(desc(n_critical_species)) %&gt;% top_n(10) ## Selecting by n_critical_species ## # A tibble: 10 x 2 ## Country n_critical_species ## &lt;chr&gt; &lt;dbl&gt; ## 1 Czech Republic 2159 ## 2 United States 1409 ## 3 Germany 915 ## 4 Japan 628 ## 5 Austria 618 ## 6 Slovak Republic 602 ## 7 Canada 522 ## 8 Poland 485 ## 9 Switzerland 483 ## 10 Brazil 453 Ce pipeline consistait à: prendre le tableau especes_menacees, puis filtrer pour n&#39;obtenir que les espèces critiques, puis sélectionner les colonnes des pays et des valeurs (nombre d&#39;espèces), puis segmenter le tableaux en plusieurs tableaux selon le pays, puis appliquer la fonction sum pour chacun de ces petits tableaux (puis de recombiner ces sommaires), puis trier les pays en nombre décroissant de décompte d&#39;espèces, puis afficher le top 10 3.5.6 Exemple (difficile) Pour revenir à notre tableau chicoute, imaginez que vous aviez une station météo (station_A) située aux coordonnées (490640, 5702453) et que vous désiriez calculer la distance entre l’observation et la station. Prenez du temps pour réfléchir à la manière dont vous procéderez… On pourra créer une fonction qui mesure la distance entre un point x, y et les coordonnées de la station A… dist_station_A &lt;- function (x, y) { return(sqrt((x - 490640)^2 + (y - 5702453)^2)) } … puis ajouter une colonne avec mutate grâce à une fonction prenant les arguments x et y spécifiés. chicoute %&gt;% mutate(dist = dist_station_A(x = Longitude_m, y= Latitude_m)) %&gt;% select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) %&gt;% top_n(10) ## Selecting by dist ## # A tibble: 10 x 5 ## ID CodeTourbiere Longitude_m Latitude_m dist ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 BP 484054 5706307 7631. ## 2 36 MR 459875 5701988 30769. ## 3 37 MR 459873 5701987 30771. ## 4 38 MR 459880 5701971 30764. ## 5 39 MR 459894 5701966 30750. ## 6 40 MR 459915 5701994 30728. ## 7 46 NBM 485975 5695688 8218. ## 8 48 NBM 485912 5696607 7519. ## 9 49 NBM 485903 5696611 7521. ## 10 50 NBM 485884 5696612 7532. Nous pourrions procéder de la même manière pour fusionner des données climatiques. Le tableau chicoute ne possède pas d’indicateurs climatiques, mais il est possible de les soutirer de stations météo placées près des site. Ces données ne sont pas disponibles pour le tableau de la chicouté, alors j’utiliserai des données fictives pour l’exemple. Voici ce qui pourrait être fait. Créer un tableau des stations météo ainsi que des indices météo associés à ces stations. Lier chaque site à une station (à la main où selon la plus petite distance entre le site et la station). Fusionner les indices climatiques aux sites, puis les sites aux mesures de rendement. Ces opérations demandent habituellement du tâtonnement. Il serait surprenant que même une personne expérimentée soit en mesure de compiler ces opérations sans obtenir de message d’erreur, et retravailler jusqu’à obtenir le résultat souhaité. L’objectif de cette section est de vous présenté un flux de travail que vous pourriez être amenés à effectuer et de fournir quelques éléments nouveau pour mener à bien une opération. Il peut être frustrant de ne pas saisir toutes les opérations: passez à travers cette section sans jugement. Si vous devez vous frotter à problème semblable, vous saurez que vous trouverez dans ce manuel une recette intéressante. stations &lt;- data.frame(Station = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;), Longitude_m = c(490640, 484870, 485929), Latitude_m = c(5702453, 5701870, 5696421), t_moy_C = c(13.8, 18.2, 16.30), prec_tot_mm = c(687, 714, 732)) stations ## Station Longitude_m Latitude_m t_moy_C prec_tot_mm ## 1 A 490640 5702453 13.8 687 ## 2 B 484870 5701870 18.2 714 ## 3 C 485929 5696421 16.3 732 La fonction suivante calcule la distance entre des coordonnées x et y et chaque station d’un tableau de stations, puis retourne le nom de la station dont la distance est la moindre. dist_station &lt;- function (x, y, stations_df) { # stations est le tableau des stations à trois colonnes # 1iere: nom de la station # 2ieme: longitude # 3ieme: latitude distance &lt;- c() for (i in 1:nrow(stations)) { distance[i] &lt;- sqrt((x - stations[i, 2])^2 + (y - stations[i, 3])^2) } nom_station &lt;- as.character(stations$Station[which.min(distance)]) return(nom_station) } Testons la fonction avec des coordonnées. dist_station(x = 459875, y = 5701988, stations_df = stations) ## [1] &quot;B&quot; Nous appliquons cette fonction à toutes les lignes du tableau, puis en retournons un échantillon. chicoute %&gt;% rowwise() %&gt;% mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = stations)) %&gt;% select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) %&gt;% sample_n(10) ## # A tibble: 10 x 5 ## ID CodeTourbiere Longitude_m Latitude_m Station ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 43 NESP 484866 5701866 B ## 2 17 2 486501 5702627 B ## 3 79 1 486499 5702076 B ## 4 80 1 486504 5702081 B ## 5 67 1 486544 5702078 B ## 6 38 MR 459880 5701971 B ## 7 4 BEAU 490647 5702453 A ## 8 13 2 486538 5702562 B ## 9 65 BS2 486510 5702203 B ## 10 34 MB 491955 5699332 A Cela semble fonctionner. On peut y ajouter un left_join() pour joindre les données météo au tableau principal. chicoute_weather &lt;- chicoute %&gt;% rowwise() %&gt;% mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = stations)) %&gt;% left_join(y = stations, by = &quot;Station&quot;) ## Warning: Column `Station` joining character vector and factor, coercing ## into character vector chicoute_weather %&gt;% sample_n(10) ## # A tibble: 10 x 36 ## ID CodeTourbiere Ordre Site Traitement DemiParcelle SousTraitement ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 89 WTP E 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 35 MB C 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 45 NESP J 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 34 MB C 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 9 BP H 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 24 2 9 17 fertilisa… right Cu ## 7 90 WTP E 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 59 SSP F 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 64 BS2 G 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 73 1 2 4 temoin right Control ## # ... with 29 more variables: Latitude_m.x &lt;dbl&gt;, Longitude_m.x &lt;dbl&gt;, ## # Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;, ## # TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;, ## # TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;, ## # FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;, ## # SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, ## # P_pourc &lt;dbl&gt;, K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, ## # S_pourc &lt;dbl&gt;, B_pourc &lt;dbl&gt;, Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, ## # Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt;, Station &lt;chr&gt;, ## # Longitude_m.y &lt;dbl&gt;, Latitude_m.y &lt;dbl&gt;, t_moy_C &lt;dbl&gt;, ## # prec_tot_mm &lt;dbl&gt; 3.5.7 Exporter un tableau Simplement avec write_csv(). write_csv(chicoute_weather, &quot;data/chicoute_weather.csv&quot;) 3.5.8 Aller plus loin dans le tidyverse Le livre R for Data Science, de Garrett Grolemund et Hadley Wickham, est un incontournable. 3.6 Références Parent L.E., Parent, S.É., Herbert-Gentile, V., Naess, K. et Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183 "],
["chapitre-visualisation.html", "4 Visualisation 4.1 Pourquoi explorer graphiquement? 4.2 Publier un graphique 4.3 Choisir le type de graphique le plus approprié 4.4 Choisir son outils de visualisation 4.5 Visualisation en R 4.6 Module de base pour les graphiques 4.7 La grammaire graphique ggplot2 4.8 Mon premier ggplot 4.9 Les graphiques comme outil d’exploration des données 4.10 Choisir les bonnes couleurs 4.11 Règles particulières", " 4 Visualisation Lorsque j’aborde un document scientifique, la première chose que je fais après avoir lu le résumé est de regarder les graphiques. Un graphique bien conçu est dense en information, de sorte qu’il met en lumière une information qui pourrait passer inaperçue dans un tableau. Reconnaissez-vous cette image? Source: GIEC, Bilan 2001 des changements climatiques : Les éléments scientifiques Elle a été conçue par Michael E. Mann, Raymond S. Bradley et Malcolm K. Hughes. Le graphique montre l’évolution des températures en °C normalisées selon la température moyenne entre 1961 et 1990 sur l’axe des Y en fonction du temps, sur l’axe des X. On le connait aujourd’hui comme le bâton de hockey, et on reconnait son rôle clé pour sensibiliser la civilisation entière face au réchauffement global. On aura recours à la visualisation des données pour plusieurs raison: en particulier, lorsque l’information d’un tableau devient difficile à interpréter. Ainsi, créer des graphiques est une tâche courante dans un flux de travail en science, que ce soit pour explorer les données ou les communiquer… ce à quoi cette section est vouée. 4.1 Pourquoi explorer graphiquement? La plupart des graphiques que vous générerez ne seront pas destinés à être publiés. Ils viseront probablement d’abord à explorer des données. Cela vous permettra de mettre en évidence de nouvelles perspectives. Prenons par exemple deux variables, \\(X\\) et \\(Y\\). Vous calculez leur moyenne, écart-type et la corrélation entre les deux variables (nous verrons les statistiques en plus de détails dans un prochain chapitre). Pour démontrer que ces statistiques ne vous apprendront pas grand chose sur la structure des données, Matejka et Fitzmaurice (2017) ont généré 12 jeux de données \\(X\\) et \\(Y\\), ayant chacun pratiquement les mêmes statistiques. Mais avec des structures bien différentes. Animation montrant la progression du jeu de données Datasaurus pour toutes les formes visées. Source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing. 4.2 Publier un graphique Vous voilà sensibilisé à l’importance d’explorer les données graphiquement. Mais ce qui ultimement émanera d’un projet sera le rapport que vous déposerez, l’article scientifique que vous ferez publier ou le billet de blogue que vous posterez. Les graphiques inclus dans vos publications méritent une attention particulière pour que votre audience puisse comprendre les découvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit évidemment répondre honnêtement à la question posée, sans artifices inutiles, mais tout de même attrayante. 4.2.1 Cinq qualités d’un bon graphique Alberto Cairo, chercheur spécialisé en visualisation de données, a fait paraître en 2016 le livre The Truthful art, note cinq qualités d’une visualisation bien conçue (les citations de cette section proviennent de ma traduction de Alberto Cairo, The Truthful Art (2016), p. 45.). 1- Elle est véritable, puisqu’elle est basée sur une recherche exhaustive et honnête. Cela vaut autant pour les graphiques que pour l’analyse de données. Il s’agit froidement de présenter les données selon l’interprétation la plus exacte. Les pièges à éviter sont le picorage de cerises et la surinterprétation des données. Le picorage, c’est lorsqu’on réduit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des données d’une région ou d’une décennie qui rendraient factice une conclusion fixée a priori. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La surinterprétation, c’est lorsque l’on saute rapidement aux conclusions: par exemple, que l’on génère des corrélations, voire même des relations de causalités à partir de ce qui n’est que du bruit de fond. À ce titre, lors d’une conférence, Heather Krause insiste sur l’importance de faire en sorte que les représentations graphiques répondent correctement aux questions posées dans une étude (à voir!). 2- Elle est fonctionnelle, puisqu’elle constitue une représentation précise des données, et qu’elle est construite de manière à laisser les observateurs.trices prendre des initiatives conséquentes. “La seule chose qui est pire qu’un diagramme en pointe de tarte, c’est d’en présenter plusieurs” (Edward Tufte, designer, cité par Alberto Cairo, 2016, p. 50). Choisir le bon graphique pour représenter vos données est beaucoup moins une question de bon goût qu’une question de démarche rationnelle sur l’objectif visé par la présentation d’un graphique. Je présenterai des lignes guides pour sélectionner le type de graphique qui présentera vos données de manière fonctionnelle en fonction de l’objectif d’un graphique (d’ailleurs, avez-vous vraiment besoin d’un graphique?). 3- Elle est attrayante et intrigante, et même esthétiquement plaisante pour l’audience visée - les scientifiques d’abord, mais aussi le public en général. En sciences naturelles, la pensée rationnelle, la capacité à organiser la connaissance et créer de nouvelles avenues sont des qualités qui sont privilégiées au talent artistique. Que vous ayez où non des aptitudes en art visuel, présentez de l’information, pas des décorations. Excel vous permet d’ajouter une perspective 3D à un diagramme en barres. La profondeur contient-elle de l’information? Non. Cette décoration ne fait qu’ajouter de la confusion. Minimalisez, fournissez le plus d’information possible avec le moins d’éléments possibles. C’est ce que vous proposent les guides graphiques que j’introduirai plus loin. 4- Elle est pertinente, puisqu’elle révèle des évidences scientifiques autrement difficilement accessibles. Il s’agit de susciter un eurêka, dans le sens qu’elle génère une idée, et parfois une initiative, en un coup d’œil. Le graphique en bâton de hockey est un exemple où l’on a spontanément une idée de la situation. Cette situation peut être la présence d’un phénomène comme l’augmentation de la température globale, mais aussi l’absence de phénomènes pourtant attendus. 5- Elle est instructive, parce que si l’on saisit et accepte les évidences scientifiques qu’elle décrit, cela changera notre perception pour le mieux. En présentant cette qualité, Alberto Cairo voulait insister ses lecteurs.trices à choisir des sujets de discussion visuelle de manière à participer à un monde meilleur. En ce qui nous concerne, il s’agit de bien sélectionner l’information que l’on désire transmettre. Imaginez que vous avez travaillé quelques jours pour créer un graphique, sont vous êtes fier, mais vous (ou un collègue hiérarchiquement favorisé) vous rendez compte que le graphique soutient peu ou pas le propos ou l’objectif de votre thèse/mémoire/rapport/article. Si c’est bien le cas, vous feriez mieux de laisser tomber votre oeuvre et considérer votre démarche comme une occasion d’apprentissage. Alberto Cairo résume son livre The Truthful Art dans une entrevue avec le National Geographic. 4.3 Choisir le type de graphique le plus approprié Vous connaissez sans doute les nuages de point, les lignes, les histogrammes, les diagrammes en barre et en pointe de tarte. De nombreuses manières de présenter les données ont été développées. Les principaux types de graphique seront couverts dans ce chapitre. D’autres types spécialisés seront couverts dans les chapitres appropriés (graphiques davantage orientés vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.). La visualisation de données est aujourd’hui devenue une expertise en soi. Plusieurs personnes ayant acquis une expertise dans le domaine partage leurs expériences. À ce titre, le site from data to viz est à conserver dans vos marques-page. Il comprend des arbres décisionnels qui vous guident vers les options appropriées pour présenter vos données, puis fournissent des exemples en R que vous pourrez copier-coller-adapter dans vos feuilles de calcul. Également, je suggère le site internet de Ann K. Emery, qui présente des lignes guide pour présenté le graphique adéquat selon les données en main. De nombreuses recettes sont également proposées sur r-graph-gallery.com. En ce qui a trait aux couleurs, le choix n’est pas anodin. Si vous avez le souci des détails sur les éléments esthétiques de vos graphiques, je recommande la lecture de ce billet de blog de Lisa Charlotte Rost. Cairo (2016) propose de procéder avec ces étapes: Réfléchissez au message que vous désirez transmettre: comparer les catégories \\(A\\) et \\(B\\), visualiser une transition ou un changement de \\(A\\) vers \\(B\\), présenter une relation entre \\(A\\) et \\(B\\) ou la distribution de \\(A\\) et \\(B\\) sur une carte. Essayez différentes représentations: si le message que vous désirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus d’un graphique. Mettez de l’ordre dans vos données. Par ailleurs le prochain chapitre vise à vous sensibilisé à l’importance d’avoir des données bien organisées, qui seront par la suite plus facile à visualiser. Testez le résultat. “Hé, qu’est-ce que tu comprends de cela?” Si la personne hausse les épaules, il va falloir réévaluer votre stratégie. 4.4 Choisir son outils de visualisation Les modules et logiciels de visualisation sont basés sur des approches que l’on pourrait placer sur un spectre allant de l’impératif au déclaratif. 4.4.1 Approche impérative Selon cette approche, vous indiquez comment placer l’information dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatisées, ce qui laisse une grande flexibilité, mais demande de vouer beaucoup d’énergie à la manière de coder pour obtenir le graphique désiré. Le module graphique de Excel, ainsi que le module graphique de base de R, utilisent des approches impératives. 4.4.2 Approche déclarative Les stratégies d’automatisation graphique se sont grandement améliorées au cours des dernières années. Plutôt que de vouer vos énergies à créer un graphique, il est maintenant possible de spécifier ce que l’on veut présenter. La visualisation déclarative vous permet de penser aux données et à leurs relations, plutôt que des détails accessoires. Jake Vanderplas, Declarative Statistical Visualization in Python with Altair (ma traduction) L’approche déclarative passe souvent par une grammaire graphique, c’est-à-dire un langage qui explique ce que l’on veut présenter - en mode impératif, on spécifie plutôt comment on veut présenter les données. Le module ggplot2 est le module déclaratif par excellence en R. 4.5 Visualisation en R En R, votre trousse d’outils de visualisation mériterait de comprendre les modules suivants. base. Le module de base de R contient des fonctions graphique très polyvalentes. Les axes sont générées automatiquement, on peut y ajouter des titres et des légendes, on peut créer plusieurs graphiques sur une même figure, on peut y ajouter différentes géométries (points, lignes et polygones), avec différents types de points ou de trait, et différentes couleurs, etc. Les modules spécialisés viennent souvent avec leurs graphiques spécialisés, construit à partir du module de base. En tant que module graphique impératif, on peut tout faire ou presque (pas d’interactivité), mais l’écriture du code est peut expressive. ggplot2. C’est le module graphique par excellence en R (et j’ose dire: en calcul scientifique). ggplot2 se base sur une grammaire graphique. À partir d’un tableau de données, une colonne peut définir l’axe des x, une autre l’axe des y, une autre la couleur couleur des points ou leur dimension. Une autre colonne définissant des catégories peut segmenter la visualisation en plusieurs graphiques alignés horizontalement ou verticalement. Des extensions de ggplot2 permettent de générer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc. plotly. plotly offre une fonction toute simple pour rendre interactif un graphique ggplot2. plotly est aussi un module graphique en soit, particulièrement utile pour les graphiques interactifs. Nous survolerons rapidement le module de base, irons plus en profondeur avec ggplot2, puis je présenterai brièvement les graphiques interactifs avec plotly. 4.6 Module de base pour les graphiques Nous allons d’abord survoler le module de base, en mode impératif. La fonction de base pour les graphiques en R est plot(). Pour nous exercer avec cette fonction, chargeons d’abord le tableau de données d’exercice iris, publié en 1936 par le célèbre biostatisticien Ronald Fisher. data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Le tableau iris contient 5 colonnes, les 4 premières décrivant les longueurs et largeurs des pétales et sépales de différentes espèces d’iris dont le nom apparaît à la 5ième colonne. La manière la plus rapide d’extraire une colonne d’un tableau est d’appeler le tableau, suivit du $, puis du nom de la colonne, par exemple iris$Species. Pour générer un graphique avec la fonction plot(): plot(iris$Sepal.Length, iris$Petal.Length) Par défaut, le premier argument est le vecteur définissant l’axe des x et le deuxième est celui définissant l’axe des y. Le graphique précédent peut être amplement personnalisé en utilisant différents arguments. Exercice. Utilisez ces arguments dans la cellule de code de la figure plot(iris$Sepal.Length, iris$Petal.Length). Remarquez que la fonction a décidé toute seule de créer un nuage de point. La fonction plot() est conçue pour créer le graphique approprié selon le type des données spécifiées: lignes, boxplot, etc. Si l’on spécifiait les espèces comme argument x… plot(iris$Species, iris$Petal.Length) De même, la fonction plot() appliquée à un tableau de données générera une représentation bivariée. plot(iris) Il est possible d’encoder des attributs grâce à des vecteurs de facteurs (catégories). plot(iris, col = iris$Species) L’argument type = \"\" permet de personnaliser l’apparence: type = \"p\": ligne type = \"l\": ligne type = \"o\" et type = \"b\": ligne et points type = \"n\": ne rien afficher Créons un jeu de données. time &lt;- seq(0, 100, 10) height &lt;- abs(time * 0.1 + rnorm(length(time), 0, 2)) # abs pour forcer les valeurs positives plot(time, height, type = &#39;b&#39;) Le type de ligne est spécifié par l’argument lty et la largeur du trait, par l’argument lwd. La fonction hist() permet quant à elle de créer des histogrammes. Parmi ses arguments, breaks est particulièrement utile, car il permet d’ajuster la segmentation des incréments. hist(iris$Petal.Length, breaks = 60) Exercice. Ajustez le titre de l’axe des x, ainsi que les limites de l’axe des x. Êtes-vous en mesure de colorer l’intérieur des barres en bleu? La fonction plot() peut être suivie de plusieurs autres couches comme des lignes (lines() ou abline()), des points (points()), du texte (text()), des polygones (polygon(), des légendes (legend())), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. L’exemple suivant ajoute une ligne au graphique. Ne prêtez pas trop attention aux fonctions predict() et lm() pour l’instant: nous les verrons au chapitre 5. plot(time, height) lines(time, predict(lm(height ~ time))) Pour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destinés à être publiés, je vous suggère d’exporter vos graphiques avec une haute résolution à la suite de la commande png() (ou jpg() ou svg()). png(filename = &#39;images/mon-graphique.png&#39;, width = 3000, height=2000, res=300) plot(x = iris$Petal.Length, y = iris$Sepal.Length, col = iris$Species, cex=3, # dimension des points pch = 16) # type de points dev.off() ## png ## 2 Ce format crée une version vectorielle du graphique, c’est-à-dire que l’image exportée est un fichier contenant les formes, non pas les pixels. Cela vous permet d’éditer votre graphique dans un logiciel de dessin vectoriel (comme Inkscape). J’ai utilisé le format d’image png, utile pour les images de type graphique, avec des changements de couleurs drastiques. Pour les photos, vous préférerez le format jpg. Des éditeurs demanderont peut-être des formats vectoriels comme pdf ou eps. Si vous ne trouvez pas de moyen de modifié un aspect du graphique dans le code (bouger des étiquettes ou des légendes, ajouter des éléments graphiques), vous pouvez exporter votre graphique en format svg (par la commande svg(). Ce format vectoriel peut être ouvert avec des logiciels de dessin vectoriel comme le logiciel libre Inkscape. Le module de base de R comprend une panoplie d’autres particularités que je ne couvrirai pas ici, en faveur du module ggplot2. 4.7 La grammaire graphique ggplot2 Brièvement, une grammaire graphique permet de schématiser (ma traduction de to map) des données sur des attributs esthétiques sur des géométries. Avec cette définition, nous avons 3 composantes. Les données. Votre tableau est bien sûr un argument nécessaire pour générer le graphique. Les marqueurs. Un terme abstrait pour désigner les points, les lignes, les polygones, les barres, les flèches, etc. Les attributs encodés. La position, la dimension, la couleur ou la forme que prendront les géométries. En ggplot2, on les nomme les aesthetics. Les attributs globaux. Les attributs sont globaux lorsqu’ils sont constant (ils ne dépendent pas d’une variable). Les valeurs par défaut conviennent généralement, mais certains attributs peuvent être spécifiés: par exemple la forme ou la couleur des points, le type de ligne. Les thèmes. Le thème du graphique peut être spécifié dans son ensemble, c’est-à-dire en utilisant un thème prédéfini, mais l’on peut modifier certains détails. Le flux de travail pour créer un graphique à partir d’une grammaire ressemble donc à ceci: Avec mon tableau, Créer un marqueur ( encoder(position X = colonne A, position Y = colonne B, couleur = colonne C) forme globale = 1) Avec un thème noir et blanc Le module tidyverse installera des modules utilisés de manière récurrente dans ce cours, comme ggplot2, dplyr, tidyr et readr. Je vous recommande de l’installer et de le charger au début de vos sessions de travail. library(&quot;tidyverse&quot;) Le tidyverse est le nom d’une nouvelle méthode de travail en R. Par rapport à l’approche classique, l’approche tidyverse est plus intuitive et mieux adaptée pour l’ensemble des tâches à accomplir en science des données. Les deux approches peuvent tout-à-fait être combinées. Nous utiliserons préférablement le tidyverse pour ce cours. 4.8 Mon premier ggplot Pour notre premier exercice, je vais charger un tableau depuis le fichier de données abalone.data depuis un dépôt sur internet. Je n’irai pas dans les détails sur les tableaux de données, puisque c’est le sujet du prochain chapitre. Le fichier de données porte sur un escargot de mer et comprend le sexe (M: mâle, F: femelle et I: enfant), des poids et dimensions des individus observés, et le nombre d’anneaux comptés dans la coquille. abalone &lt;- read_csv(&quot;data/abalone.csv&quot;) ## Parsed with column specification: ## cols( ## Type = col_character(), ## LongestShell = col_double(), ## Diameter = col_double(), ## Height = col_double(), ## WholeWeight = col_double(), ## ShuckedWeight = col_double(), ## VisceraWeight = col_double(), ## ShellWeight = col_double(), ## Rings = col_double() ## ) Inspectons l’entête du tableau avec la fonction head(). head(abalone) ## # A tibble: 6 x 9 ## Type LongestShell Diameter Height WholeWeight ShuckedWeight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 0.455 0.365 0.095 0.514 0.224 ## 2 M 0.35 0.265 0.09 0.226 0.0995 ## 3 F 0.53 0.42 0.135 0.677 0.256 ## 4 M 0.44 0.365 0.125 0.516 0.216 ## 5 I 0.33 0.255 0.08 0.205 0.0895 ## 6 I 0.425 0.3 0.095 0.352 0.141 ## # ... with 3 more variables: VisceraWeight &lt;dbl&gt;, ShellWeight &lt;dbl&gt;, ## # Rings &lt;dbl&gt; Suivant la grammaire graphique ggplot2, on pourra créer ce graphique de points comprenant les attributs suivants suivants. data = abalone, le fichier de données. mapping = aes(...), spécifié comme attribut de la fonction ggplot(), cet encodage (ou aesthetic) reste l’encodage par défaut pour tous les marqueurs du graphique. Toutefois, l’encodage mapping = aes() peut aussi être spécifié dans la fonction du marqueur (par exemple geom_point()). Dans l’encodage global du graphique, on place en x la longueur de la coquille (x = LongestShell) et on place en y le poids de la coquille (y = ShellWeight). Pour ajouter un marqueur, on utilise le +. Généralement, on change aussi de ligne. Le marqueur ajouté est un point, geom_point(), dans lequel on spécifie un encodage de couleur sur la variable Type (colour = Type) et un encodage de dimension du point sur la variable rings (size = Rings). L’attribut alpha = 0.5 se situe hors du mapping et de la fonction aes(): c’est un attribut identique pour tous les points. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) Il existe plusieurs types de marqueurs: geom_point pour les points geom_line pour les lignes geom_bar pour les diagrammes en barre et geom_histogram pour les histogrammes geom_boxplot pour les boxplots geom_errorbar, geom_pointrange ou geom_crossbar pour les marges d’erreur geom_map pour les cartes etc. Il existe plusieurs attributs d’encodage: la position x, y et z (z pertinent notamment pour le marqueur geom_tile()) la taille size la forme des points shape la couleur colour, qui peut être discrète ou continue le type de ligne linetype la transparence alpha et d’autres types spécialisés que vous retrouverez dans la documentation des marqueurs Les types de marqueurs et leurs encodages sont décrits dans la documentation de ggplot2, qui fournit des feuilles aide-mémoire qu’il est commode d’imprimer et d’afficher près de soi. Aide-mémoire de ggplot2, source: https://www.rstudio.com/resources/cheatsheets/ 4.8.0.1 Les facettes Dans ggplot2, les facetttes sont un type spécial d’encodage utilisés pour définir des grilles de graphique. Elles prennent deux formes: Le collage, facet_wrap(). Une variable catégorielle est utilisée pour segmenter les graphiques en plusieurs graphiques, qui sont placés l’un à la suite de l’autre dans un arrangement spécifié par un nombre de colonne ou un nombre de ligne. La grille, facet_grid(). Une ou deux variables segmentent les graphiques selon les colonnes et les lignes. Les facettes peuvent être spécifiées n’importe où dans la chaîne de commande de ggplot2, mais conventionnellement, on les place tout de suite après la fonction ggplot(). ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + facet_wrap(~Type, ncol=2) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) La fonction cut() permet de discrétiser des variables continues en catégories ordonnées - les fonctions peuvent être utilisées à l’intérieur de la fonction ggplot. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) + geom_point(mapping = aes(colour = Type), alpha = 0.5) Par défaut, les axes des facettes, ainsi que leurs dimensions, sont les mêmes. Une telle représentation permet de comparer les facets sur une même échelle. Les axes peuvent être définis selon les données avec l’argument scales, tandis que l’espace des facettes peut être conditionné selon l’argument space - pour plus de détails, voir la fiche de documentation. Exercice. Personnalisez le graphique avec les données abalone en remplaçant les variables et en réorganisant les facettes. 4.8.1 Plusieurs sources de données Il peut arriver que les données pour générer un graphique proviennent de plusieurs tableaux. Lorsqu’on ne spécifie pas la source du tableau dans un marqueur, la valeur par défaut est le tableau spécifier dans l’amorce ggplot(). Il est néanmoins possible de définir une source personnalisée pour chaque marqueur en spécifiant data = ... comme argument du marqueur. abalone_siteA &lt;- data.frame(LongestShell = c(0.3, 0.8, 0.7), ShellWeight = c(0.05, 0.81, 0.77)) ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) + geom_point(data = abalone_siteA, size = 8, shape = 4) 4.8.2 Exporter avec style Le fond gris est une marque distinctive de ggplot2. Il n’est toutefois pas apprécié de tout le monde. D’autres thèmes dits complets peuvent être utilisés (liste des thèmes complets). Les thèmes complets sont appelés avant la fonction theme(), qui permet d’effectuer des ajustements précis dont la liste exhaustive se trouve dans la documentation de ggplot2. Vous pouvez aussi personnaliser le titre des axes (xlab() et ylab()), leur limites (xlim() et ylim()) ou spécifier un titre global (ggtitle()). Pour exporter un ggplot, on pourra utiliser les commandes de R png(), svg() ou pdf(), ou les outils de RStudio. Toutefois, ggplot2 offre la fonction ggsave(), que l’on place en remorque du graphique, en spécifiant les dimensions (width et height) ainsi que la résolution (dpi). La résolution d’un graphique destiné à la publication est typiquement de plus de 300 dpi. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) + xlab(&quot;Length (mm)&quot;) + ylab(&quot;Shell weight (g)&quot;) + ggtitle(&quot;Abalone&quot;) + xlim(c(0, 1)) + theme_classic() + theme(axis.title = element_text(size=20), axis.text = element_text(size=20), axis.text.y = element_text(size=20, angle=90, hjust=0.5), legend.box = &quot;horizontal&quot;) ggsave(&quot;images/abalone.png&quot;, width = 8, height = 8, dpi = 300) Nous allons maintenant couvrir différents types de graphiques, accessibles selon différents marqueurs: les nuages de points les diagrammes en ligne les boxplots les histogrammes les diagrammes en barres 4.8.3 Nuages de points L’exemple précédent est un nuage de points, que nous avons généré avec le marqueur geom_point(), qui a déjà été passablement introduit. L’exploration de ces données a permis de détecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juvéniles (Type I) sont plus petits et moins lourds, mais nous devrons probablement procéder à des tests statistiques pour vérifier s’il y a des différences entre mâles et femelles. Le graphique étant très chargé, nous avons utilisé des stratégies pour l’alléger en utilisant de la transparence et des facettes. Le marqueur geom_jitter() peut permettre de mieux apprécier la dispersion des points en ajoutant une dispersion randomisée en x ou en y. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height=0.1) Dans ce cas-ci, ça ne change pas beaucoup, mais retenons-le pour la suite. 4.8.4 Diagrammes en lignes Les lignes sont utilisées pour exprimer des liens entre une suite d’information. Dans la plupart des cas, il s’agit d’une suite d’information dans le temps que l’on appelle les séries temporelles. En l’occurrence, les lignes devraient être évitées si la séquence entre les variables n’est pas évidente. Nous allons utiliser un tableau de données de R portant sur la croissance des orangers. data(Orange) head(Orange) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 La première colonne spécifie le numéro de l’arbre mesuré, la deuxième son âge et la troisième sa circonférence. Le marqueur geom_line() permet de tracer la tendance de la circonférence selon l’âge. En encodant la couleur de la ligne à l’arbre, nous pourrons tracer une ligne pour chacun d’entre eux. ggplot(data = Orange, mapping = aes(x = age, y = circumference)) + geom_line(aes(colour = Tree)) La légende ne montre pas les numéros d’arbre en ordre croissance. En effet, la légende (tout comme les facettes) classe les catégories prioritairement selon l’ordre des catégories si elles sont ordinales, ou par ordre alphabétique si les catégories sont nominales. Inspectons la colonne Tree en inspectant le tableau avec la commande str() - la commande glimpse() du tidyverse donne un sommaire moins complet que str(). str(Orange) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 35 obs. of 3 variables: ## $ Tree : Ord.factor w/ 5 levels &quot;3&quot;&lt;&quot;1&quot;&lt;&quot;5&quot;&lt;&quot;2&quot;&lt;..: 2 2 2 2 2 2 2 4 4 4 ... ## $ age : num 118 484 664 1004 1231 ... ## $ circumference: num 30 58 87 115 120 142 145 33 69 111 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language circumference ~ age | Tree ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time since December 31, 1968&quot; ## ..$ y: chr &quot;Trunk circumference&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(mm)&quot; En effet, la colonne Tree est un facteur ordinal dont les niveaux sont dans le même ordre que celui la légende. 4.8.5 Les histogrammes Nous avons vu les histogrammes dans la brève section sur les fonctions graphiques de base dans R: il s’agit de segmenter l’axe des x en incréments, puis de présenter sur l’axe de y le nombre de données que l’on retrouve dans cet incrément. Le marqueur à utiliser est geom_histogram(). Revenons à nos escargots. Comment présenteriez-vous la longueur de la coquille selon la variable Type? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer l’intérieur des barres, l’argument à utiliser est fill. ggplot(data = abalone, mapping = aes(x = LongestShell)) + geom_histogram(mapping = aes(fill = Type), colour = &#39;black&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. On n’y voit pas grand chose. Essayons plutôt les facettes. ggplot(data = abalone, mapping = aes(x = LongestShell)) + facet_grid(Type ~ .) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Les facettes permettent maintenant de bien distinguer la distribution des longueur des juvéniles. L’argument bins, tout comme l’argument breaks du module graphique de base, permet de spécifier le nombre d’incréments, ce qui peut être très utile en exploration de données. ggplot(data = abalone, mapping = aes(x = LongestShell)) + facet_grid(Type ~ .) + geom_histogram(bins=60, colour = &#39;white&#39;) Le nombre d’incréments est un paramètre qu’il ne faut pas sous-estimer. À preuve, ce tweet de [@NicholasStrayer](https://twitter.com/NicholasStrayer): Histograms are fantastic, but make sure your bin-width/number is chosen well. This is the exact same data, plotted with different bin-widths. Notice that the pattern doesn't necessarily get clearer as bin num increases. #dataviz pic.twitter.com/3MhSFwTVPH — Nick Strayer (@NicholasStrayer) 7 août 2018 4.8.6 Boxplots Les boxplots sont une autre manière de visualiser des distributions. L’astuce est de créer une boîte qui s’étant du premier quartile (valeur où l’on retrouve 25% de données dont la valeur est inférieure) au troisième quartile (valeur où l’on retrouve 75% de données dont la valeur est inférieure). Une barre à l’intérieur de cette boîte est placée à la médiane (qui est en fait le second quartile). De part et d’autre de la boîte, on retrouve des lignes spécifiant l’étendue hors quartile. Cette étendue peut être déterminée de plusieurs manières, mais dans le cas de ggplot2, il s’agit de 1.5 fois l’étendue de la boîte (l’écart interquartile). Au-delà de ces lignes, on retrouve les points représentant les valeurs extrêmes. Le marqueur à utiliser est geom_boxplot(). L’encodage x est la variable catégorielle et l’encodage y est la variable continue. ggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) + geom_boxplot() Exercice. On suggère parfois de présenter les mesures sur les boxplots. Utiliser geom_jitter avec un bruit horizontal. 4.8.7 Les diagrammes en barre Les diagrammes en barre représente une variable continue associée à une catégorie. Les barres sont généralement horizontales et ordonnées. Nous y reviendrons à la fin de ce chapitre, mais retenez pour l’instant que dans tous les cas, les diagrammes en barre doivent inclure le zéro pour éviter les mauvaises interprétations. Pour les diagrammes en barre, nous allons utiliser les données de l’union internationale pour la conservation de la nature distribuées par l’OCDE. especes_menacees &lt;- read_csv(&#39;data/WILD_LIFE_16082018051732754.csv&#39;) ## Parsed with column specification: ## cols( ## IUCN = col_character(), ## `IUCN Category` = col_character(), ## SPEC = col_character(), ## Species = col_character(), ## COU = col_character(), ## Country = col_character(), ## `Unit Code` = col_character(), ## Unit = col_character(), ## `PowerCode Code` = col_double(), ## PowerCode = col_character(), ## `Reference Period Code` = col_logical(), ## `Reference Period` = col_logical(), ## Value = col_double(), ## `Flag Codes` = col_logical(), ## Flags = col_logical() ## ) ## Warning: 20 parsing failures. ## row col expected actual file ## 2293 Flag Codes 1/0/T/F/TRUE/FALSE E &#39;data/WILD_LIFE_16082018051732754.csv&#39; ## 2293 Flags 1/0/T/F/TRUE/FALSE Estimated value &#39;data/WILD_LIFE_16082018051732754.csv&#39; ## 2296 Flag Codes 1/0/T/F/TRUE/FALSE E &#39;data/WILD_LIFE_16082018051732754.csv&#39; ## 2296 Flags 1/0/T/F/TRUE/FALSE Estimated value &#39;data/WILD_LIFE_16082018051732754.csv&#39; ## 2323 Flag Codes 1/0/T/F/TRUE/FALSE E &#39;data/WILD_LIFE_16082018051732754.csv&#39; ## .... .......... .................. ............... ...................................... ## See problems(...) for more details. head(especes_menacees) ## # A tibble: 6 x 15 ## IUCN `IUCN Category` SPEC Species COU Country `Unit Code` Unit ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 TOT_… Total number o… MAMM… Mammals AUS Austra… NBR Numb… ## 2 ENDA… Number of enda… MAMM… Mammals AUS Austra… NBR Numb… ## 3 CRIT… Number of crit… MAMM… Mammals AUS Austra… NBR Numb… ## 4 VULN… Number of vuln… MAMM… Mammals AUS Austra… NBR Numb… ## 5 THRE… Total number o… MAMM… Mammals AUS Austra… NBR Numb… ## 6 TOT_… Total number o… MAMM… Mammals AUT Austria NBR Numb… ## # ... with 7 more variables: `PowerCode Code` &lt;dbl&gt;, PowerCode &lt;chr&gt;, ## # `Reference Period Code` &lt;lgl&gt;, `Reference Period` &lt;lgl&gt;, Value &lt;dbl&gt;, ## # `Flag Codes` &lt;lgl&gt;, Flags &lt;lgl&gt; Ne vous en faites pas pour le message d’avertissement: le tableau devrait être bien importé. Je verrai à corriger cela dans une future version de ces notes. L’exercice consiste à créer un diagramme en barres horizontales du nombre d’espèces menacées de manière critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques opérations sur ce tableau afin d’en arriver avec un tableau que nous pourrons convenablement mettre en graphique: n’y portez pas trop attention pour l’instant: ces opérations sont un avant-goût du prochain chapitre. Nous allons filtrer le tableau pour obtenir les espèces critiquement menacées, sélectionner seulement le pays et le nombre d’espèces, les grouper par pays, additionner toutes les espèces pour chaque pays, les placer en ordre descendant et enfin sélectionner les 10 premiers. Comme vous le voyez, la création de graphique est liée de près avec la manipulation des tableaux! especes_crit &lt;- especes_menacees %&gt;% filter(IUCN == &#39;CRITICAL&#39;) %&gt;% select(Country, Value) %&gt;% group_by(Country) %&gt;% summarise(n_critical_species = sum(Value)) %&gt;% arrange(desc(n_critical_species)) %&gt;% head(10) especes_crit ## # A tibble: 10 x 2 ## Country n_critical_species ## &lt;chr&gt; &lt;dbl&gt; ## 1 Czech Republic 2159 ## 2 United States 1409 ## 3 Germany 915 ## 4 Japan 628 ## 5 Austria 618 ## 6 Slovak Republic 602 ## 7 Canada 522 ## 8 Poland 485 ## 9 Switzerland 483 ## 10 Brazil 453 Le premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur geom_col(). ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() Ce graphique est perfectible. Les barres sont verticales et non ordonnées. Souvenons-nous que ggplot2 ordonne par ordre alphabétique si aucun autre ordre est spécifié. Nous pouvons changer l’ordre en changeant l’ordre des niveaux de la variable Country selon le nombre d’espèces. Il s’avère que cet ordre est déjà défini par la colonne especes_crit$Country. especes_crit$Country &lt;- factor(especes_crit$Country, levels = especes_crit$Country) Pour faire pivoter le graphique, nous ajoutons coord_flip() à la séquence. ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() + coord_flip() Nous n’y sommes pas encore. Normalement, la valeur la plus élevée se retrouve en haut! Pas de problème, il s’agit de renverser le vecteur d’ordre avec la fonction rev(). especes_crit$Country &lt;- factor(especes_crit$Country, levels = rev(especes_crit$Country)) ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() + coord_flip() Une autre méthode, geom_bar(), est un raccourcis permettant de compter le nombre d’occurrence d’une variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type ggplot(data = abalone, mapping = aes(x = Type)) + geom_bar() + coord_flip() Personnellement, je préfère passer par un diagramme en lignes avec le marqueur geom_segment(). Cela me donne la flexibilité pour définir un largeur de trait et éventuellement d’ajouter un point au bout pour en faire un diagramme en suçon. ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_segment(mapping = aes(xend=Country, yend = 0), lwd = 2) + geom_point(size=6) + coord_flip() + theme_bw() Les diagrammes en barre peuvent être placés en relation avec d’autres. Reprenons notre manipulation de données précédente, mais en incluant tous les pays. especes_pays_iucn &lt;- especes_menacees %&gt;% filter(IUCN %in% c(&#39;ENDANGERED&#39;, &#39;VULNERABLE&#39;,&#39;CRITICAL&#39;)) %&gt;% select(IUCN, Country, Value) %&gt;% group_by(Country, IUCN) %&gt;% summarise(n_species = sum(Value)) %&gt;% group_by(Country) %&gt;% mutate(n_tot = sum(n_species)) %&gt;% arrange(desc(n_tot)) head(especes_pays_iucn) ## # A tibble: 6 x 4 ## # Groups: Country [2] ## Country IUCN n_species n_tot ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Czech Republic CRITICAL 2159 7567 ## 2 Czech Republic ENDANGERED 2386 7567 ## 3 Czech Republic VULNERABLE 3022 7567 ## 4 United States CRITICAL 1409 6005 ## 5 United States ENDANGERED 1631 6005 ## 6 United States VULNERABLE 2965 6005 Pour l’ordre des pays, jouons un peu avec R. ordre_pays &lt;- especes_pays_iucn %&gt;% select(Country, n_tot) %&gt;% unique() %&gt;% arrange(n_tot) %&gt;% select(Country) especes_pays_iucn$Country &lt;- factor(especes_pays_iucn$Country, levels = pull(ordre_pays)) Pour placer les barres les unes à côté des autres, nous spécifions position = \"dodge\". ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + geom_col(aes(fill=IUCN), position = &quot;dodge&quot;) + coord_flip() Il est parfois plus pratique d’utiliser les facettes. ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + facet_grid(IUCN ~ .) + geom_col() + coord_flip() 4.8.8 Exporter un graphique Plus besoin d’utiliser la fonction png() en mode ggplot2. Utilisons plutôt ggsave(). ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + facet_grid(IUCN ~ .) + geom_col(aes(fill=IUCN)) + coord_flip() ggsave(&quot;images/especes_pays_iucn.png&quot;, width = 6, height = 8, dpi = 300) 4.9 Les graphiques comme outil d’exploration des données La plupart des graphiques que vous créerez ne seront pas destinés à être publiés, mais serviront d’outil d’exploration des données. Le jeu de données datasaurus, présenté en début de chapitre, permet de saisir l’importance des outils graphiques pour bien comprendre les données. datasaurus = read_tsv(&#39;data/DatasaurusDozen.tsv&#39;) ## Parsed with column specification: ## cols( ## dataset = col_character(), ## x = col_double(), ## y = col_double() ## ) head(datasaurus) ## # A tibble: 6 x 3 ## dataset x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dino 55.4 97.2 ## 2 dino 51.5 96.0 ## 3 dino 46.2 94.5 ## 4 dino 42.8 91.4 ## 5 dino 40.8 88.3 ## 6 dino 38.7 84.9 Projetons d’abord les coordonnées x et y sur un graphique. J’utilise FacetGrid ici, sachant que ce sera utile pour l’exploration. ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + geom_point() Ce graphique pourrait ressembler à une distribution binormale, ou un coup de 12 dans une porte de grange. Mais on aperçoit des données alignées, parfois de manière rectiligne, parfois en forme d’ellipse. Le tableau datasaurus a une colonne d’information supplémentaire. Utilisons-la comme catégorie pour générer des couleurs différente. ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + geom_point(mapping = aes(colour = dataset)) Ce n’est pas vraiment plus clair. Il y a toutefois des formes qui se dégage, comme des ellipse et des lignes. Et si je regarde bien, j’y vois une étoile. La catégorisation pourrait-elle être mieux utilisée si on segmentait par facettes au lieu de des couleurs? ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + facet_wrap(~dataset, nrow=2) + geom_point(size = 0.5) + coord_equal() Voilà! Fait intéressant, ni les statistiques, ni les algorithmes de regroupement ne nous auraient été utiles pour différencier les groupes! 4.9.1 Des graphiques interactifs! Les graphiques sont traditionnellement des images statiques. Toutefois, les graphiques n’étant pas dépendants de supports papiers peuvent être utilisés de manière différente, en ajoutant une couche d’interaction. Conçue à Montréal, plotly est un module graphique interactif en soi. Il peut être utilisé grâce à son outil web, tout comme il peut être interfacé avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec ggplot2. Les graphiques ggplot2 peuvent être enregistrés en tant qu’objets. Il peuvent conséquemment être manipulés par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif. library(&quot;plotly&quot;) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout especes_crit_bar &lt;- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_segment(mapping = aes(xend=Country, yend = 0), lwd = 2) + geom_point(size=6) + coord_flip() # ggplotly(especes_crit_bar) # erreur en Rmd 4.9.2 Des extensions de ggplot2 ggplot2 est un module graphique élégant et polyvalent. Il a pourtant bien des limitations. Justement, le module est conçu pour être implémenté avec des extensions. Vous en trouverez plusieurs sur ggplot2-exts.org, mais en trouverez de nombreuses autres en cherchant avec le terme ggplot2 sur github.com, probablement la plate-forme (voire un réseau social) de développement de logiciels la plus utilisée dans le monde. En voici quelques unes. cowplot permet de créer des graphiques prêts pour la publication, par exemple en créant des grilles de plusieurs ggplots, en les numérotant, etc. Si les thèmes de base ne vous conviennent pas, vous en trouverez d’autres en installant ggthemes. ggmap et ggspatial sont deux extensions pour créer des cartes. Un chapitre sur les données spatiales est en développement. ggtern permet de créer des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes. Ce sujet est couvert au chapitre 6, en développement. 4.9.3 Aller plus loin avec ggplot2 Claus O. Wilke est professeur en biologie intégrative à l’Université du Texas à Austin. Son livre Fundamentals of Data Visualization est un guide théorique et pratique pour la visualisation de données avec ggplot2. Le site data-to-viz.com vous accompagne dans le choix du graphique à créer selon vos données. Le site r-graph-gallery.com offre des recettes pour créer des graphiques avec ggplot2. 4.10 Choisir les bonnes couleurs La couleur est une information. Les couleurs devraient être sélectionnées d’abord pour être lisibles par les personnes ne percevant pas les couleurs, selon le support (apte à être photocopié, lisible à l’écran, lisible sur des documents imprimés en noir et blanc) et selon le type de données. - Données continues ou catégorielles ordinales: gradient (transition graduelle d’une couleur à l’autre), séquence (transition saccadée selon des groupes de données continues) ou divergentes (transition saccadée d’une couleur à l’autre vers des couleurs divergentes, par exemple orange vers blanc vers bleu). - Données catégorielles nominales: couleurs éloignées d’une catégorie à une autre (plus il y a de catégories, plus les couleurs sont susceptibles de se ressembler). Capture d’écran de colorbrewer2.org, qui propose des palettes de couleurs pour créer des cartes, mais l’information est pertinente pour tout type de graphique. 4.11 Règles particulières Les mauvais graphiques peuvent survenir à cause de l’ignorance, bien sûr, mais souvent ils existent pour la même raison que la boeuferie [bullhist] verbale ou écrite. Parfois, les gens ne se soucient pas de la façon dont ils présentent les données aussi longtemps que ça appuie leurs arguments et, parfois, ils ne se soucient pas que ça porte à confusion tant qu’ils ont l’air impressionnant. \\(-\\) Carl Bergstorm et Jevin West, Calling Bullshit Read-Along Week 6: Data Visualization Une représentation visuelle est un outil tranchant qui peut autant présenter un état véritable des données qu’une perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualités ne sont pas respectées. Les occasions d’erreur ne manquent pas - j’en ferai mention dans la section Choisir le bon type de graphique. Pour l’instant, notons quelques règles particulières. 4.11.1 Ne tronquez pas inutilement l’axe des \\(y\\) Tronquer l’axe vertical peut amener à porter de fausses conclusions. Effets sur la perception d’utiliser différentes références. Source: Yau (2015), Real Chart Rules to Follow. La règle semble simple: les diagrammes en barre (utilisés pour représenter une grandeur) devraient toujours présenter le 0 et les diagrammes en ligne (utilisés pour présenter des tendances) ne requiert pas nécessairement le zéro ((Bergstrom et West, Calling bullshit: Misleading axes on graphs)[http://callingbullshit.org/tools/tools_misleading_axes.html]). Mais le zéro n’est pas toujours lié à une quantité particulière, par exemple, la température ou un log-ratio. De plus, avec un diagramme en ligne on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins à une règle qu’une qualité d’un bon graphique, en particulier la qualité no 1 de Cairo: offrir une représentation honnête des données. Par exemple, Nathan Yau, auteur du blogue Flowing Data, propose de présenter des résultats de manière relative à la mesure initiale. C’est d’ailleurs ce qui a été fait pour générer le graphique de Michael Mann et al., ci-dessus, où le zéro correspond à la moyenne des températures enregistrées entre 1961 et 1990. Il peut être tentant de tronquer l’axe des \\(y\\) lorsque l’on désire superposer deux axes verticaux. Souvent, l’utilisation de plusieurs axes verticaux amène une perception de causalité dans des situations de fausses corrélations. On ne devrait jamais utiliser plusieurs axes verticaux. Ja-mais. 4.11.2 Utilisez un encrage proportionnel Cette règle a été proposée par Edward Tufte dans Visual Display of Quantitative Information. Une des raisons pour lesquelles on évite de tronquer l’axe des \\(y\\) en particulier pour les diagrammes en barre est que l’aire représentant une mesure (la quantité d’“encre” nécessaire pour la dessiner) devrait être proportionnelle à sa magnitude. Les diagrammes en barre sont particulièrement sensibles à cette règle, étant donnée que la largeur des barres peuvent amplifier l’aire occupée. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) préférer des “diagrammes de points” (dot charts, à ne pas confondre aux nuages de points). L’encrage a beau être proportionnel, la difficulté que les humains éprouvent à comparer la dimension des cercles, et a fortiori la dimension de parties de cercle, donne peu d’avantage à utiliser des diagrammes en pointe de tarte, souvent utilisés pour illustrer des proportions. Nathan Yau suggère de les utiliser avec suspicions et d’explorer d’autres options. Pour comparer deux proportions, une avenue intéressante est le diagramme en pente, suggéré notamment par Ann K. Emery. Par extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont comparées, ou lorsque des proportions évoluent selon des données continuent. De la même manière, les diagrammes en bulles ne devraient pas être représentatifs de la quantité, mais plutôt de contextualiser des données. Justement, le graphique tiré des données de Gap minder présenté plus haut est une contextualisation: l’aire d’un cercle ne permet pas de saisir la population d’un pays, mais de comparer grossièrement la population d’un pays par rapport aux autres. 4.11.3 Publiez vos données Vous avez peut-être déjà feuilleté un article et voulu avoir accès aux données incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les données. Mais le processus est fastidieux, long, souvent peu précis. De plus en plus, les chercheurs sont encouragés à publier leurs données et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient être accompagnés des données et calculs ayant servi à les générer. Mais ce n’est pas idéal non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent être exportés en code javascipt, qui contient toutes les informations sur les données et la manière de les représenter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus communément utilisés en calcul scientifique avec Python, mais je vous encourage à explorer la nouvelle génération d’outils graphiques. 4.11.4 Évitez de distraire avec des décorations futiles À venir. 4.11.5 Visitez www.junkcharts.typepad.com de temps à autre Le statisticien et blogueur Kaiser Fung s’affaire quotidiennement à proposer des améliorations à de mauvais graphiques sur son blogue Junk Charts. "],
["chapitre-biostats.html", "5 Biostatistiques 5.1 Populations et échantillons 5.2 Les variables 5.3 Les probabilités 5.4 Les distributions 5.5 Statistiques descriptives 5.6 Tests d’hypothèses à un et deux échantillons 5.7 L’analyse de variance 5.8 Les modèles statistiques", " 5 Biostatistiques Aux chapitres précédents, nous avons vu comment visualiser, organiser et manipuler des tableaux de données. La statistique est une collection de disciplines liées à la collecte, l’organisation, l’analyse, l’interprétation et la présentation de données. Les biostatistiques est l’application de ces disciplines à la biosphère. Dans Principles and procedures of statistics: A biometrical approach, Steel, Torie et Dickey (1997) définissent les statistiques ainsi: Les statistiques forment la science, pure et appliquée, de la création, du développement, et de l’application de techniques par lesquelles l’incertitude de l’induction inférentielle peut être évaluée. (ma traduction) Alors que l’inférence consiste à généraliser des observations sur des échantillons à l’ensemble d’une population, l’induction est un type de raisonnement qui permet de généraliser des observations en théories. Les statistiques permettent d’évaluer l’incertitude découlant du processus qui permet d’abord de passer de l’échantillon à la population représenté par cet échantillon, puis de passer de cette représentation d’une population en lois générales la concernant. La définition de Whitlock et Schuluter (2015), dans The Analysis of Biological Data, est plus simple, insistant sur l’inférence: La statistique est l’étude des méthodes pour décrire et mesures des aspects de la nature à partir d’échantillons. (ma traduction) Les statistiques consistent à faire du sens (anglicisme assumé) avec des observations dans l’objectif de répondre à une question que vous aurez formulée clairement, préalablement à votre expérience. The more time I spend as The Statistician in the room, the more I think the best skill you can cultivate is the ability to remain calm and repeatedly ask “What question are you trying to answer?” — Bryan Howie (@bryan_howie) 13 décembre 2018 Le flux de travail conventionnel consiste à collecter des échantillons, transformer les données, effectuer des tests, analyser les résultats, les interpréter et les visualiser. Bien que ces tâches soient complexes, en particulier en ce qui a trait aux tests statistiques, la plupart des opérations statistiques peuvent être effectuées sans l’assistance de statisticien.ne.s… à condition de comprendre suffisamment les concepts utilisés. Ce chapitre à lui seul est trop court pour permettre d’intégrer toutes les connaissances nécessaires à une utilisation raisonnée des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs d’interprétation statistiques sont courantes et la consultation de spécialistes n’est souvent pas un luxe. Dans ce chapitre, nous verrons comment répondre correctement à une question valide et adéquate avec l’aide d’outils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables aléatoires qui nous permettront d’effectuer des tests statistiques commun avec R. Nous couvrirons aussi les erreurs communément commises en recherche académique et les moyens simples de les éviter. Ce chapitre est une introduction aux statistiques avec R, et ne remplacera pas un bon cours de stats. En plus des modules de base de R nous utiliserons les modules de la tidyverse, le module de données agricoles agridat, ainsi que le module nlme spécialisé pour la modélisation mixte. Avant de survoler les applications statistiques avec R, je vais d’abord et rapidement présenter quelques notions importantes en statistiques: populations et échantillons, variables, probabilités et distributions. Nous allons effectuer des tests d’hypothèse univariés (notamment les tests de t et les analyses de variance) et détailler la notion de p-value. Mais avant tout, je vais m’attarder plus longuement aux modèles linéaires généralisés, incluant en particulier des effets fixes et aléatoires (modèles mixtes), qui fournissent une trousse d’analyse polyvalente en analyse multivariée. Je terminerai avec les perspectives multivariés que sont les matrices de covariance et de corrélation. 5.1 Populations et échantillons Le principe d’inférence consiste à généraliser des conclusions à l’échelle d’une population à partir d’échantillons issus de cette population. Alors qu’une population contient tous les éléments étudiés, un échantillon d’une population est une observation unique. Une expérience bien conçue fera en sorte que les échantillons sont représentatifs de la population qui, la plupart du temps, ne peut être observée entièrement pour des raisons pratiques. Les principes d’expérimentation servant de base à la conception d’une bonne méthodologie sont présentés dans le cours Dispositifs expérimentaux (BVG-7002). Également, je recommande le livre Principes d’expérimentation: planification des expériences et analyse de leurs résultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aperçu des dispositifs expérimentaux est aussi présenté dans Introductory Statistics with R, de Peter Dalgaard (2008). Une population est échantillonnée pour induire des paramètres: un rendement typique dans des conditions météorologiques, édaphiques et managériales données, la masse typique des faucons pèlerins, mâles et femelles, le microbiome typique d’un sol agricole ou forestier, etc. Une statistique est une estimation d’un paramètre calculée à partir des données, par exemple une moyenne et un écart-type. Par exemple, la moyenne (\\(\\mu\\)) et l’écart-type (\\(\\sigma\\)) d’une population sont estimés par les moyennes (\\(\\bar{x}\\)) et écarts-types (\\(s\\)) calculés sur les données issues de l’échantillonnage. Chaque paramètre est liée à une perspective que l’on désire connaître chez une population. Ces angles d’observations sont les variables. 5.2 Les variables Nous avons abordé au chapitre 4 la notion de variable par l’intermédiaire d’une donnée. Une variable est l’observation d’une caractéristique décrivant un échantillon et qui est susceptible de varier d’un échantillon à un autre. Si les observations varient en effet d’un échantillon à un autre, on parlera de variable aléatoire. Même le hasard est régit par certaines loi: ce qui est aléatoire dans une variable peut être décrit par des lois de probabilité, que nous verrons plus bas. Mais restons aux variables pour l’instant. Par convention, on peut attribuer aux variables un symbole mathématique. Par exemple, on peut donner à la masse volumique d’un sol (qui est le résultat d’une méthodologie précise) le symbole \\(\\rho\\). Lorsque l’on attribue une valeur à \\(\\rho\\), on parle d’une donnée. Chaque donnée d’une observation a un indice qui lui est propre, que l’on désigne souvent par \\(i\\), que l’on place en indice \\(\\rho_i\\). Pour la première donnée, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) d’échantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\). En R, une variable est associée à un vecteur ou une colonne d’un tableau. rho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D data &lt;- data.frame(rho = rho) # tableau data ## rho ## 1 1.34 ## 2 1.52 ## 3 1.26 ## 4 1.43 ## 5 1.39 Il existe plusieurs types de variables, qui se regroupe en deux grandes catégories: les variables quantitatives et les variables qualitatives. 5.2.1 Variables quantitatives Ces variables peuvent être continuent dans un espace échantillonnal réel ou discrètes dans un espace échantillonnal ne considérant que des valeurs fixes. Notons que la notion de nombre réel est toujours une approximation en sciences expérimentales comme en calcul numérique, étant donnée que l’on est limité par la précision des appareils comme par le nombre d’octets à utiliser. Bien que les valeurs fixes des distributions discrètes ne soient pas toujours des valeurs entières, c’est bien souvent le cas en biostatistiques comme en démographie, où les décomptes d’individus sont souvent présents (et où la notion de fraction d’individus n’est pas acceptée). 5.2.2 Variables qualitatives On exprime parfois qu’une variable qualitative est une variable impossible à mesurer numériquement: une couleur, l’appartenance à espèce ou à une série de sol. Pourtant, dans bien des cas, les variables qualitatives peut être encodées en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile à un loam sableux, qui autrement est décrit par la classe texturale d’un sol. Pour une couleur, on peut lui associer des pourcentages de rouge, vert et bleu, ainsi qu’un ton. En ce qui a trait aux variables ordonnées, il est possible de supposer un étalement. Par exemple, une variable d’intensité faible-moyenne-forte peut être transformée linéairement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l’étalement peut parfois être quadratique ou logarithmique. Les séries de sol peuvent être encodées par la proportion de gleyfication (Parent et al., 2017). Quant aux catégories difficilement transformables en quantités, on pourra passer par l’encodage catégoriel, souvent appelé dummyfication, qui nous verrons plus loin. 5.3 Les probabilités « Nous sommes si éloignés de connaître tous les agens de la nature, et leurs divers modes d’action ; qu’il ne serait pas philosophique de nier les phénomènes, uniquement parce qu’ils sont inexplicables dans l’état actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d’autant plus scrupuleuse, qu’il paraît plus difficile de les admettre ; et c’est ici que le calcul des probabilités devient indispensable, pour déterminer jusqu’à quel point il faut multiplier les observations ou les expériences, afin d’obtenir en faveur des agens qu’elles indiquent, une probabilité supérieure aux raisons que l’on peut avoir d’ailleurs, de ne pas les admettre. » — Pierre-Simon de Laplace Une probabilité est la vraisemblance qu’un évènements se réalise chez un échantillon. Les probabilités forment le cadre des systèmes stochastiques, c’est-à-dire des systèmes trop complexes pour en connaître exactement les aboutissants, auxquels ont attribue une part de hasard. Ces systèmes sont prédominants dans les processus vivants. On peut dégager deux perspectives sur les probabilités: l’une passe par une interprétation fréquentielle, l’autre bayésienne. L’interprétation fréquentielle représente la fréquence des occurrences après un nombre infini d’évènements. Par exemple, si vous jouez à pile ou face un grand nombre de fois, le nombre de pile sera égal à la moitié du nombre de lancés. Il s’agit de l’interprétation communément utilisée. L’interprétation bayésienne vise à quantifier l’incertitude des phénomènes. Dans cette perspective, plus l’information s’accumule, plus l’incertitude diminue. Cette approche gagne en notoriété notamment parce qu’elle permet de décrire des phénomènes qui, intrinsèquement, ne peuvent être répétés infiniment (absence d’asymptote), comme celles qui sont bien définis dans le temps ou sur des populations limités. L’approche fréquentielle teste si les données concordent avec un modèle du réel, tandis que l’approche bayésienne évalue la probabilité que le modèle soit réel. Une erreur courante consiste à aborder des statistiques fréquentielles comme des statistiques bayésiennes. Par exemple, si l’on désire évaluer la probabilité de l’existence de vie sur Mars, on devra passer par le bayésien, car avec les stats fréquentielles, l’on devra plutôt conclure si les données sont conforme ou non avec l’hypothèse de la vie sur Mars (exemple tirée du blogue Dynamic Ecology). Des rivalités factices s’installent enter les tenants des différentes approches, dont chacune, en réalité, répond à des questions différentes dont il convient réfléchir sur les limitations. Bien que les statistiques bayésiennes soient de plus en plus utilisées, nous ne couvrirons dans ce chapitre que l’approche fréquentielle. L’approche bayésienne est néanmoins traitée dans le document complémentaire statistiques_bayes.ipynb (section en développement). 5.4 Les distributions Une variable aléatoire peut prendre des valeurs selon des modèles de distribution des probabilités. Une distribution est une fonction mathématique décrivant la probabilité d’observer une série d’évènements. Ces évènements peuvent être des valeurs continues, des nombres entiers, des catégories, des valeurs booléennes (Vrai/Faux), etc. Dépendamment du type de valeur et des observations obtenues, on peut associer des variables à différentes lois de probabilité. Toujours, l’aire sous la courbe d’une distribution de probabilité est égale à 1. En statistiques inférentielle, les distributions sont les modèles, comprenant certains paramètres comme la moyenne et la variance pour les distributions normales, à partir desquelles les données sont générées. Il existe deux grandes familles de distribution: discrètes et continues. Les distributions discrètes sont contraintes à des valeurs prédéfinies (finies ou infinies), alors que les distributions continues prennent nécessairement un nombre infinie de valeur, dont la probabilité ne peut pas être évaluée ponctuellement, mais sur un intervalle. L’espérance mathématique est une fonction de tendance centrale, souvent décrite par un paramètre. Il s’agit de la moyenne d’une population pour une distribution normale. La variance, quant à elle, décrit la variabilité d’une population, i.e. son étalement autour de l’espérance. Pour une distribution normale, la variance d’une population est aussi appelée variance, souvent présentée par l’écart-type. 5.4.1 Distribution binomiale En tant que scénario à deux issues possibles, des tirages à pile ou face suivent une loi binomiale, comme toute variable booléenne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la présence/absence d’une espèce, d’une maladie, d’un trait phylogénétique, ainsi que les catégories encodées. Lorsque l’opération ne comprend qu’un seul échantillon (i.e. un seul tirage à pile ou face), il s’agit d’un cas particulier d’une loi binomiale que l’on nomme une loi de Bernouilli. Pour 25 tirages à pile ou face indépendants (i.e. dont l’ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilités est de 1. La fonction dbinom est une fonctions de distribution de probabilités. Les fonctions de distribution de probabilités discrètes sont appelées des fonctions de masse. library(&quot;tidyverse&quot;) x &lt;- 0:25 y &lt;- dbinom(x = x, size = 25, prob = 0.5) print(paste(&#39;La somme des probabilités est de&#39;, sum(y))) ## [1] &quot;La somme des probabilités est de 1&quot; ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 5.4.2 Distribution de Poisson La loi de Poisson (avec un P majuscule, introduite par le mathématicien français Siméon Denis Poisson et non pas l’animal) décrit des distributions discrètes de probabilité d’un nombre d’évènements se produisant dans l’espace ou dans le temps. Les distributions de Poisson décrive ce qui tient du décompte. Il peut s’agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d’asclépiades se trouvant sur une terre cultivée, ou du nombre d’évènements de précipitation au mois de juin, etc. La distribution de Poisson n’a qu’un seul paramètre, \\(\\lambda\\), qui décrit tant la moyenne des décomptes. Par exemple, en un mois de 30 jours, et une moyenne de 8 évènements de précipitation pour ce mois, on obtient la distribution suivante. x &lt;- 1:30 y &lt;- dpois(x, lambda = 8) print(paste(&#39;La somme des probabilités est de&#39;, sum(y))) ## [1] &quot;La somme des probabilités est de 0.999664536835124&quot; ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 5.4.3 Distribution uniforme La distribution la plus simple est probablement la distribution uniforme. Si la variable est discrète, chaque catégorie est associé à une probabilité égale. Si la variable est continue, la probabilité est directement proportionnelle à la largeur de l’intervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour décrire des a priori vagues pour l’analyse bayésienne (ce sujet est traité dans le document 5.1_bayes.ipynb). Nous utilisons la fonction dunif. À la différence des distributions discrètes, les fonctions de distribution de probabilités continues sont appelées des fonctions de densité d’une loi de probabilité (probability density function). increment &lt;- 0.01 x &lt;- seq(-4, 4, by = increment) y1 &lt;- dunif(x, min = -3, max = 3) y2 &lt;- dunif(x, min = -2, max = 2) y3 &lt;- dunif(x, min = -1, max = 1) print(paste(&#39;La somme des probabilités est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilités est de 1.005&quot; gg_unif &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_unif, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) 5.4.4 Distribution normale La plus répandue de ces lois est probablement la loi normale, parfois nommée loi gaussienne et plus rarement loi laplacienne. Il s’agit de la distribution classique en forme de cloche. La loi normale est décrite par une moyenne, qui désigne la tendance centrale, et une variance, qui désigne l’étalement des probabilités autours de la moyenne. La racine carrée de la variance est l’écart-type. Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approximées par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne d’une loi log-normale est la moyenne géométrique. increment &lt;- 0.01 x &lt;- seq(-10, 10, by = increment) y1 &lt;- dnorm(x, mean = 0, sd = 1) y2 &lt;- dnorm(x, mean = 0, sd = 2) y3 &lt;- dnorm(x, mean = 0, sd = 3) print(paste(&#39;La somme des probabilités est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilités est de 0.999147010743368&quot; gg_norm &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_norm, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) Quelle est la probabilité d’obtenir le nombre 0 chez une observation continue distribuée normalement dont la moyenne est 0 et l’écart-type est de 1? Réponse: 0. La loi normale étant une distribution continue, les probabilités non-nulles ne peuvent être calculés que sur des intervalles. Par exemple, la probabilité de retrouver une valeur dans l’intervalle entre -1 et 2 est calculée en soustrayant la probabilité cumulée à -1 de la probabilité cumulée à 2. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) prob_between &lt;- c(-1, 2) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data.frame(x, y), aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadécimal geom_line() prob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1) print(paste(&quot;La probabilité d&#39;obtenir un nombre entre&quot;, prob_between[1], &quot;et&quot;, prob_between[2], &quot;est d&#39;environ&quot;, round(prob_norm_between, 2) * 100, &quot;%&quot;)) ## [1] &quot;La probabilité d&#39;obtenir un nombre entre -1 et 2 est d&#39;environ 82 %&quot; La courbe normale peut être utile pour évaluer la distribution d’une population. Par exemple, on peut calculer les limites de région sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d’autre de la moyenne. Il s’agit ainsi de l’intervalle de confiance sur la déviation de la distribution. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1), qnorm(p = 1 - alpha/2, mean = 0, sd = 1)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadécimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) On pourrait aussi être intéressé à l’intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l’écart-type est noté erreur standard. On calcule cette erreur en divisant la variance par le nombre d’observation, ou en divisant l’écart-type par la racine carrée du nombre d’observations. Ainsi, pour 10 échantillons: increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10), qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadécimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) 5.5 Statistiques descriptives On a vu comment générer des statistiques sommaires en R avec la fonction summary(). Reprenons les données d’iris. data(&quot;iris&quot;) summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Pour précisément effectuer une moyenne et un écart-type sur un vecteur, passons par les fonctions mean() et sd(). mean(iris$Sepal.Length) ## [1] 5.843333 sd(iris$Sepal.Length) ## [1] 0.8280661 Pour effectuer un sommaire de tableau piloté par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par espèce pour effectuer un sommaire sur toutes les variables. iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Vous pourriez être intéressé par les quartiles à 25, 50 et 75%. Mais la fonction summarise() n’autorise que les fonctions dont la sortie est d’un seul objet, alors faisons sorte que l’objet soit une liste - lorsque l’on imbrique une fonction funs, le tableau à insérer dans la fonction est indiqué par un .. iris %&gt;% group_by(Species) %&gt;% summarise_all(funs(list(quantile(.)))) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 setosa &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; ## 2 versicolor &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; ## 3 virginica &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; En mode programmation classique de R, on pourra générer les quartiles à la pièce. quantile(iris$Sepal.Length[iris$Species == &#39;setosa&#39;]) ## 0% 25% 50% 75% 100% ## 4.3 4.8 5.0 5.2 5.8 quantile(iris$Sepal.Length[iris$Species == &#39;versicolor&#39;]) ## 0% 25% 50% 75% 100% ## 4.9 5.6 5.9 6.3 7.0 quantile(iris$Sepal.Length[iris$Species == &#39;virginica&#39;]) ## 0% 25% 50% 75% 100% ## 4.900 6.225 6.500 6.900 7.900 La fonction table() permettra d’obtenir des décomptes par catégorie, ici par plages de longueurs de sépales. Pour obtenir les proportions du nombre total, il s’agit d’encapsuler le tableau croisé dans la fonction prop.table(). tableau_croise &lt;- table(iris$Species, cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length))) tableau_croise ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 35 14 0 0 ## versicolor 4 20 17 9 ## virginica 1 5 18 26 prop.table(tableau_croise) ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 0.234899329 0.093959732 0.000000000 0.000000000 ## versicolor 0.026845638 0.134228188 0.114093960 0.060402685 ## virginica 0.006711409 0.033557047 0.120805369 0.174496644 5.6 Tests d’hypothèses à un et deux échantillons Un test d’hypothèse permet de décider si une hypothèse est confirmée ou rejetée à un seuil de probabilité prédéterminé. Cette section est inspirée du chapitre 5 de Dalgaard, 2008. 5.6.0.1 Information: l’hypothèse nulle Les tests d’hypothèse évalue des effets statistiques (qui ne sont pas nécessairement des effets de causalité). L’effet à évaluer peut être celui d’un traitement, d’indicateurs météorologiques (e.g. précipitations totales, degré-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est menée pour évaluer l’hypothèse que l’on retrouve des différences entre des unités expérimentales. Par convention, l’hypothèse nulle (écrite \\(H_0\\)) est l’hypothèse qu’il n’y ait pas d’effet (c’est l’hypothèse de l’avocat du diable 😈) à l’échelle de la population (et non pas à l’échelle de l’échantillon). À l’inverse, l’hypothèse alternative (écrite \\(H_1\\)) est l’hypothèse qu’il y ait un effet à l’échelle de la population. À titre d’exercice en stats, on débute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations à moyennes différentes ou si un vecteur de valeurs a été généré à partir d’une population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appelé de Mann-Whitney). 5.6.1 Test de t à un seul échantillon Nous devons assumer, pour ce test, que l’échantillon est recueillit d’une population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque échantillon est indépendant l’un de l’autre. L’hypothèse nulle est souvent celle de l’avocat du diable, c’est-à-dire: ici, que \\(\\mu = \\bar{x}\\). L’erreur standard sur la moyenne (ESM) de l’échantillon, \\(\\bar{x}\\) est calculée comme suit. \\[ESM = \\frac{s}{\\sqrt{n}}\\] où \\(s\\) est l’écart-type de l’échantillon et \\(n\\) est le nombre d’échantillons. Pour tester l’intervalle de confiance de l’échantillon, on multiplie l’ESM par l’aire sous la courbe de densité couvrant une certaine proportion de part et d’autre de l’échantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d’autre. set.seed(33746) x &lt;- rnorm(20, 16, 4) level &lt;- 0.95 alpha &lt;- 1-level x_bar &lt;- mean(x) s &lt;- sd(x) n &lt;- length(x) error &lt;- qnorm(1 - alpha/2) * s / sqrt(n) error ## [1] 1.483253 intervalle de confiance est l’erreur de par et d’autre de la moyenne. c(x_bar - error, x_bar + error) ## [1] 14.35630 17.32281 Si la moyenne de la population est de 16, un nombre qui se situe dans l’intervalle de confiance on accepte l’hypothèse nulle au seuil 0.05. Si le nombre d’échantillon est réduit (généralement &lt; 30), on passera plutôt par une distribution de t, avec \\(n-1\\) degrés de liberté. error &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n) c(x_bar - error, x_bar + error) ## [1] 14.25561 17.42351 Plus simplement, on pourra utiliser la fonction t.test() en spécifiant la moyenne de la population. Nous avons généré 20 données avec une moyenne de 16 et un écart-type de 4. Nous savons donc que la vraie moyenne de l’échantillon est de 16. Mais disons que nous testons l’hypothèse que ces données sont tirées d’une population dont la moyenne est 18 (et implicitement que sont écart-type est de 4). t.test(x, mu = 18) ## ## One Sample t-test ## ## data: x ## t = -2.8548, df = 19, p-value = 0.01014 ## alternative hypothesis: true mean is not equal to 18 ## 95 percent confidence interval: ## 14.25561 17.42351 ## sample estimates: ## mean of x ## 15.83956 La fonction retourne la valeur de t (t-value), le nombre de degrés de liberté (\\(n-1 = 19\\)), une description de l’hypothèse alternative (alternative hypothesis: true mean is not equal to 18), ainsi que l’intervalle de confiance au niveau de 95%. Le test contient aussi la p-value. Bien que la p-value soit largement utilisée en science 5.6.1.1 Information: la p-value La p-value, ou valeur-p ou p-valeur, est utilisée pour trancher si, oui ou non, un résultat est significatif (en langage scientifique, le mot significatif ne devrait être utilisé que lorsque l’on réfère à un test d’hypothèse statistique). Vous retrouverez des p-value partout en stats. Les p-values indiquent la confiance que l’hypothèse nulle soit vraie, selon les données et le modèle statistique utilisées. La p-value est la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l’hypothèse nulle est vraie. Une p-value élevée indique que le modèle appliqué à vos données concordent avec la conclusion que l’hypothèse nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilisée en écologie et en agriculture, comme dans plusieurs domaines, est 0.05. Les six principes de l’American Statistical Association guident l’interprétation des p-values. [ma traduction] Les p-values indique l’ampleur de l’incompatibilité des données avec le modèle statistique Les p-values ne mesurent pas la probabilité que l’hypothèse étudiée soit vraie, ni la probabilité que les données ont été générées uniquement par la chance. Les conclusions scientifiques et décisions d’affaire ou politiques ne devraient pas être basées sur si une p-value atteint un seuil spécifique. Une inférence appropriée demande un rapport complet et transparent. Une p-value, ou une signification statistique, ne mesure pas l’ampleur d’un effet ou l’importance d’un résultat. En tant que tel, une p-value n’offre pas une bonne mesure des évidences d’un modèle ou d’une hypothèse. Cet encadré est inspiré d’un billet de blogue de Jim Frost et d’un rapport de l’American Statistical Association. Dans le cas précédent, la p-value était de 0.01014. Pour aider notre interprétation, prenons l’hypothèse alternative: true mean is not equal to 18. L’hypothèse nulle était bien que la vraie moyenne est égale à 18. Insérons la p-value dans la définition: la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l’hypothèse nulle est vraie est de 0.01014. Il est donc très peu probable que les données soient tirées d’un échantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette l’hypothèse nulle et l’on conclu qu’à ce seuil de confiance, l’échantillon ne provient pas d’une population ayant une moyenne de 18. 5.6.2 Attention: mauvaises interprétations des p-values “La p-value n’a jamais été conçue comme substitut au raisonnement scientifique” Ron Wasserstein, directeur de l’American Statistical Association [ma traduction]. Un résultat montrant une p-value plus élevée que 0.05 est-il pertinent? Lors d’une conférence, Dr Evil ne présentent que les résultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants… En écartant ces résultats, Dr Evil commet 3 erreurs: La p-value n’est pas un bon indicateur de l’importance d’un test statistique. L’importance d’une variable dans un modèle devrait être évaluée par la valeur de son coefficient. Son incertitude devrait être évaluée par sa variance. Une manière d’évaluer plus intuitive la variance est l’écart-type ou l’intervalle de confiance. À un certain seuil d’intervalle de confiance, la p-value traduira la probabilité qu’un coefficient soit réellement nul ait pu générer des données démontrant un coefficient égal ou supérieur. Il est tout aussi important de savoir que le traitement fonctionne que de savoir qu’il ne fonctionne pas. Les résultats démontrant des effets sont malheureusement davantage soumis aux journaux et davantage publiés que ceux ne démontrant pas d’effets (Decullier et al., 2005). Le seuil de 0.05 est arbitraire. 5.6.2.1 Attention au p-hacking Le p-hacking (ou data dredging) consiste à manipuler les données et les modèles pour faire en sorte d’obtenir des p-values favorables à l’hypothèse testée et, éventuellement, aux conclusions recherchées. À éviter dans tous les cas. Toujours. Toujours. Toujours. Vidéo suggérée (en anglais). 5.6.3 Test de Wilcoxon à un seul échantillon Le test de t suppose que la distribution des données est normale… ce qui est rarement le cas, surtout lorsque les échantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c’est un test non-paramétrique basé sur le tri des valeurs. wilcox.test(x, mu = 18) ## ## Wilcoxon signed rank test ## ## data: x ## V = 39, p-value = 0.01208 ## alternative hypothesis: true location is not equal to 18 Le V est la somme des rangs positifs. Dans ce cas, la p-value est semblable à celle du test de t, et les mêmes conclusions s’appliquent. 5.6.4 Tests de t à deux échantillons Les tests à un échantillon servent plutôt à s’exercer: rarement en aura-t-on besoin en recherche, où plus souvent, on voudra comparer les moyennes de deux unités expérimentales. L’expérience comprend donc deux séries de données continues, \\(x_1\\) et \\(x_2\\), issus de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons l’hypothèse nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calculée comme suit. \\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\] L’ESDM est l’erreur standard de la différence des moyennes: \\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\] Si vous supposez que les variances sont identiques, l’erreur standard (s) est calculée pour les échantillons des deux groupes, puis insérée dans le calcul des ESM. La statistique t sera alors évaluée à \\(n_1 + n_2 - 2\\) degrés de liberté. Si vous supposez que la variance est différente (procédure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degrés de liberté calculé à partir des erreurs standards et du nombre d’échantillon dans les groupes: cette procédure est considérée comme plus prudente (Dalgaard, 2008, page 101). Prenons les données d’iris pour l’exemple en excluant l’iris setosa étant donnée que les tests de t se restreignent à deux groupes. Nous allons tester la longueur des pétales. iris_pl &lt;- iris %&gt;% filter(Species != &quot;setosa&quot;) %&gt;% select(Species, Petal.Length) sample_n(iris_pl, 5) ## Species Petal.Length ## 98 virginica 5.2 ## 55 virginica 5.8 ## 92 virginica 5.1 ## 13 versicolor 4.0 ## 46 versicolor 4.2 Dans la prochaine cellule, nous introduisons l’interface-formule de R, où l’on retrouve typiquement le ~, entre les variables de sortie à gauche et les variables d’entrée à droite. Dans notre cas, la variable de sortie est la variable testée, Petal.Length, qui varie en fonction du groupe Species, qui est la variable d’entrée (variable explicative) - nous verrons les types de variables plus en détails dans la section Les modèles statistiques, plus bas. t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: Petal.Length by Species ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean in group versicolor mean in group virginica ## 4.260 5.552 Nous obtenons une sortie similaire aux précédentes. L’intervalle de confiance à 95% exclu le zéro, ce qui est cohérent avec la p-value très faible, qui nous indique le rejet de l’hypothèse nulle au seuil 0.05. Les groupes ont donc des moyennes de longueurs de pétale significativement différentes. 5.6.4.1 Enregistrer les résultats d’un test Il est possible d’enregistrer un test dans un objet. tt_pl &lt;- t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) summary(tt_pl) ## Length Class Mode ## statistic 1 -none- numeric ## parameter 1 -none- numeric ## p.value 1 -none- numeric ## conf.int 2 -none- numeric ## estimate 2 -none- numeric ## null.value 1 -none- numeric ## alternative 1 -none- character ## method 1 -none- character ## data.name 1 -none- character str(tt_pl) ## List of 9 ## $ statistic : Named num -12.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;t&quot; ## $ parameter : Named num 95.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 4.9e-22 ## $ conf.int : num [1:2] -1.5 -1.09 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num [1:2] 4.26 5.55 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mean in group versicolor&quot; &quot;mean in group virginica&quot; ## $ null.value : Named num 0 ## ..- attr(*, &quot;names&quot;)= chr &quot;difference in means&quot; ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;Welch Two Sample t-test&quot; ## $ data.name : chr &quot;Petal.Length by Species&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; 5.6.5 Comparaison des variances Pour comparer les variances, on a recours au test de F (F pour Fisher). var.test(formula = Petal.Length ~ Species, data = iris_pl) ## ## F test to compare two variances ## ## data: Petal.Length by Species ## F = 0.72497, num df = 49, denom df = 49, p-value = 0.2637 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.411402 1.277530 ## sample estimates: ## ratio of variances ## 0.7249678 Il semble que l’on pourrait relancer le test de t sans la procédure Welch, avec var.equal = TRUE. 5.6.6 Tests de Wilcoxon à deux échantillons Cela ressemble au test de t! wilcox.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = TRUE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Petal.Length by Species ## W = 44.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 5.6.7 Les tests pairés Les tests pairés sont utilisés lorsque deux échantillons proviennent d’une même unité expérimentale: il s’agit en fait de tests sur la différences entre deux observations. set.seed(2555) n &lt;- 20 avant &lt;- rnorm(n, 16, 4) apres &lt;- rnorm(n, 18, 3) Il est important de spécifier que le test est pairé, la valeur par défaut de paired étant FALSE. t.test(avant, apres, paired = TRUE) ## ## Paired t-test ## ## data: avant and apres ## t = -1.5168, df = 19, p-value = 0.1458 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.5804586 0.7311427 ## sample estimates: ## mean of the differences ## -1.924658 L’hypothèse nulle qu’il n’y ait pas de différence entre l’avant et l’après traitement est acceptée au seuil 0.05. Exercice. Effectuer un test de Wilcoxon pairé. 5.7 L’analyse de variance L’analyse de variance consiste à comparer des moyennes de plusieurs groupe distribués normalement et de même variance. Cette section sera élaborée prochainement plus en profondeur. Considérons-la pour le moment comme une régression sur une variable catégorielle. pl_aov &lt;- aov(Petal.Length ~ Species, iris) summary(pl_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.1 218.55 1180 &lt;2e-16 *** ## Residuals 147 27.2 0.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La prochaine section, justement, est vouée aux modèles statistiques explicatifs, qui incluent la régression. 5.8 Les modèles statistiques La modélisation statistique consiste à lier de manière explicite des variables de sortie \\(y\\) (ou variables-réponse ou variables dépendantes) à des variables explicatives \\(x\\) (ou variables prédictives / indépendantes / covariables). Les variables-réponse sont modélisées par une fonction des variables explicatives ou prédictives. Pourquoi garder les termes explicatives et prédictives? Parce que les modèles statistiques (basés sur des données et non pas sur des mécanismes) sont de deux ordres. D’abord, les modèles prédictifs sont conçus pour prédire de manière fiable une ou plusieurs variables-réponse à partir des informations contenues dans les variables qui sont, dans ce cas, prédictives. Ces modèles sont couverts dans le chapitre 11 de ce manuel (en développement). Lorsque l’on désire tester des hypothèses pour évaluer quelles variables expliquent la réponse, on parlera de modélisation (et de variables) explicatives. En inférence statistique, on évaluera les corrélations entre les variables explicatives et les variables-réponse. Un lien de corrélation n’est pas un lien de causalité. L’inférence causale peut en revanche être évaluée par des modèles d’équations structurelles, sujet qui fera éventuellement partie de ce cours. Cette section couvre la modélisation explicative. Les variables qui contribuent à créer les modèles peuvent être de différentes natures et distribuées selon différentes lois de probabilité. Alors que les modèles linéaires simples (lm) impliquent une variable-réponse distribuée de manière continue, les modèles linéaires généralisés peuvent aussi expliquer des variables de sorties discrètes. Dans les deux cas, on distinguera les variables fixes et les variables aléatoires. Les variables fixes sont des les variables testées lors de l’expérience: dose du traitement, espèce/cultivar, météo, etc. Les variables aléatoires sont les sources de variation qui génèrent du bruit dans le modèle: les unités expérimentales ou le temps lors de mesures répétées. Les modèles incluant des effets fixes seulement sont des modèles à effets fixes. Généralement, les modèles incluant des variables aléatoires incluent aussi des variables fixes: on parlera alors de modèles mixtes. Nous couvrirons ces deux types de modèle. 5.8.1 Modèles à effets fixes Les tests de t et de Wilcoxon, explorés précédemment, sont des modèles statistiques à une seule variable. Nous avons vu dans l’interface-formule qu’une variable-réponse peut être liée à une variable explicative avec le tilde ~. En particulier, le test de t est régression linéaire univariée (à une seule variable explicative) dont la variable explicative comprend deux catégories. De même, l’anova est une régression linéaire univariée dont la variable explicative comprend plusieurs catégories. Or l’interface-formule peut être utilisé dans plusieurs circonstance, notamment pour ajouter plusieurs variables de différents types: on parlera de régression multivariée. La plupart des modèles statistiques peuvent être approximés comme une combinaison linéaire de variables: ce sont des modèles linéaires. Les modèles non-linéaires impliquent des stratégies computationnelles complexes qui rendent leur utilisation plus difficile à manœuvrer. Un modèle linéaire univarié prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), où \\(\\beta_0\\) est l’intercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est l’erreur. Vous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime qu’il s’agit des valeurs générées par le modèle. En fait, \\(y = \\hat{y} - \\epsilon\\). 5.8.1.1 Modèle linéaire univarié avec variable continue Prenons les données lasrosas.corn incluses dans le module agridat, où l’on retrouve le rendement d’une production de maïs à dose d’azote variable, en Argentine. library(&quot;agridat&quot;) data(&quot;lasrosas.corn&quot;) sample_n(lasrosas.corn, 10) ## year lat long yield nitro topo bv rep nf ## 2172 2001 -33.05109 -63.84451 94.15 50.6 E 173.54 R1 N2 ## 3066 2001 -33.04908 -63.84830 90.19 99.8 W 91.74 R3 N4 ## 949 1999 -33.05165 -63.84257 73.60 0.0 LO 173.23 R2 N0 ## 1909 2001 -33.05175 -63.84320 108.95 75.4 LO 163.38 R1 N3 ## 2173 2001 -33.05111 -63.84444 101.67 50.6 E 174.00 R1 N2 ## 2770 2001 -33.05139 -63.84244 98.74 50.6 LO 167.86 R2 N2 ## 1737 1999 -33.05121 -63.84224 81.70 29.0 LO 166.59 R3 N1 ## 2261 2001 -33.05088 -63.84491 50.59 39.0 E 179.56 R1 N1 ## 1501 1999 -33.05103 -63.84404 61.44 0.0 E 166.84 R3 N0 ## 2149 2001 -33.05052 -63.84613 46.06 50.6 HT 178.54 R1 N2 Ces données comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose d’azote (nitro) comme variable explicative: il s’agit d’une régression univariée. Les deux variables sont continuent. Explorons d’abord le nuage de points de l’une et l’autre. ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) + geom_point() L’hypothèse nulle est que la dose d’azote n’affecte pas le rendement, c’est à dire que le coefficient de pente et nul. Une autre hypothèse est que l’intercept est nul: donc qu’à dose de 0, rendement de 0. Un modèle linéaire à variable de sortie continue est créé avec la fonction lm(), pour linear model. modlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn) summary(modlin_1) ## ## Call: ## lm(formula = yield ~ nitro, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.183 -15.341 -3.079 13.725 45.897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.843213 0.608573 108.193 &lt; 2e-16 *** ## nitro 0.061717 0.007868 7.845 5.75e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.66 on 3441 degrees of freedom ## Multiple R-squared: 0.01757, Adjusted R-squared: 0.01728 ## F-statistic: 61.54 on 1 and 3441 DF, p-value: 5.754e-15 Le diagnostic du modèle comprend plusieurs informations. D’abord la formule utilisée, affichée pour la traçabilité. Viens ensuite un aperçu de la distribution des résidus. La médiane devrait s’approcher de la moyenne des résidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en considération de l’échelle de y, et ce -3.079 est exprimé en terme de rendement, ici en quintaux (i.e. 100 kg) par hectare. La distribution des résidus mérite d’être davantage investiguée. Nous verrons cela un peu plus tard. Les coefficients apparaissent ensuite. Les estimés sont les valeurs des effets. R fournit aussi l’erreur standard associée, la valeur de t ainsi que la p-value (la probabilité d’obtenir cet effet ou un effet plus extrême si en réalité il y avait absence d’effet). L’intercept est bien sûr plus élevé que 0 (à dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation d’un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de maïs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l’intercept. Soulignons que l’ampleur du coefficient est très important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu’elle est inférieure à 0.05 (ce qui arrive souvent dans la littérature), serait très insuffisant pour l’interprétation des statistiques. La p-value nous indique néanmoins qu’il serait très improbable qu’une telle pente ait été générée alors que celle-ci est nulle en réalité. Les étoiles à côté des p-values indiquent l’ampleur selon l’échelle Signif. codes indiquée en-dessous du tableau des coefficients. Sous ce tableau, R offre d’autres statistiques. En outre, les R² et R² ajustés indiquent si la régression passe effectivement par les points. Le R² prend un maximum de 1 lorsque la droite passe exactement sur les points. Enfin, le test de F génère une p-value indiquant la probabilité que les coefficients de pente ait été générés si les vrais coefficients étaient nuls. Dans le cas d’une régression univariée, cela répète l’information sur l’unique coefficient. On pourra également obtenir les intervalles de confiance avec la fonction confint(). confint(modlin_1, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 64.65001137 67.03641474 ## nitro 0.04629164 0.07714271 Ou soutirer l’information de différentes manières, comme avec la fonction coefficients(). coefficients(modlin_1) ## (Intercept) nitro ## 65.84321305 0.06171718 Également, on pourra exécuter le modèle sur les données qui ont servi à le générer: predict(modlin_1)[1:5] ## 1 2 3 4 5 ## 73.95902 73.95902 73.95902 73.95902 73.95902 Ou sur des données externes. nouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5)) predict(modlin_1, newdata = nouvelles_donnees)[1:5] ## 1 2 3 4 5 ## 65.84321 66.15180 66.46038 66.76897 67.07756 5.8.1.2 Analyse des résidus Les résidus sont les erreurs du modèle. C’est le vecteur \\(\\epsilon\\), qui est un décalage entre les données et le modèle. Le R² est un indicateur de l’ampleur du décalage, mais une régression linéaire explicative en bonne et due forme devrait être accompagnée d’une analyse des résidus. On peut les calculés par \\(\\epsilon = y - \\hat{y}\\), mais aussi bien utiliser la fonction residuals(). res_df &lt;- data.frame(nitro = lasrosas.corn$nitro, residus_lm = residuals(modlin_1), residus_calcul = lasrosas.corn$yield - predict(modlin_1)) sample_n(res_df, 10) ## nitro residus_lm residus_calcul ## 124 66.0 -18.5165468 -18.5165468 ## 691 66.0 -7.5965468 -7.5965468 ## 459 53.0 -2.6342234 -2.6342234 ## 2783 39.0 26.5998170 26.5998170 ## 2519 99.8 -20.7825873 -20.7825873 ## 954 0.0 -0.1132131 -0.1132131 ## 2039 0.0 -16.9032131 -16.9032131 ## 651 131.5 2.6109781 2.6109781 ## 2856 39.0 36.5598170 36.5598170 ## 128 66.0 -20.2665468 -20.2665468 Dans une bonne régression linéaire, on ne retrouvera pas de structure identifiable dans les résidus, c’est-à-dire que les résidus sont bien distribués de part et d’autre du modèle de régression. ggplot(res_df, aes(x = nitro, y = residus_lm)) + geom_point() + labs(x = &quot;Dose N&quot;, y = &quot;Résidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) Bien que le jugement soit subjectif, on peut dire avec confiance qu’il n’y a pas structure particulière. En revanche, on pourrait générer un \\(y\\) qui varie de manière quadratique avec \\(x\\), un modèle linéaire montrera une structure évidente. set.seed(36164) x &lt;- 0:100 y &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50) modlin_2 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_2)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;Résidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) De même, les résidus ne devraient pas croître avec \\(x\\). set.seed(3984) x &lt;- 0:100 y &lt;- 10 + x + x * rnorm(length(x), 0, 2) modlin_3 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_3)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;Résidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) On pourra aussi inspecter les résidus avec un graphique de leur distribution. Reprenons notre modèle de rendement du maïs. ggplot(res_df, aes(x = residus_lm)) + geom_histogram(binwidth = 2, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) L’histogramme devrait présenter une distribution normale. Les tests de normalité comme le test de Shapiro-Wilk peuvent aider, mais ils sont généralement très sévères. shapiro.test(res_df$residus_lm) ## ## Shapiro-Wilk normality test ## ## data: res_df$residus_lm ## W = 0.94868, p-value &lt; 2.2e-16 L’hypothèse nulle que la distribution est normale est rejetée au seuil 0.05. Dans notre cas, il est évident que la sévérité du test n’est pas en cause, car les résidus semble générer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilité de la variable-réponse. 5.8.1.3 Régression multiple Comme c’est le cas pour bien des phénomènes en écologie, le rendement d’une culture n’est certainement pas expliqué seulement par la dose d’azote. Lorsque l’on combine plusieurs variables explicatives, on crée un modèle de régression multivariée, ou une régression multiple. Bien que les tendances puissent sembler non-linéaires, l’ajout de variables et le calcul des coefficients associés reste un problème d’algèbre linéaire. On pourra en effet généraliser les modèles linéaires, univariés et multivariés, de la manière suivante. \\[ y = X \\beta + \\epsilon \\] où: \\(X\\) est la matrice du modèle à \\(n\\) observations et \\(p\\) variables. \\[ X = \\left( \\begin{matrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{np} \\end{matrix} \\right) \\] \\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) étant l’intercept qui multiplie la première colonne de la matrice \\(X\\). \\[ \\beta = \\left( \\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{matrix} \\right) \\] \\(\\epsilon\\) est l’erreur de chaque observation. \\[ \\epsilon = \\left( \\begin{matrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{matrix} \\right) \\] 5.8.1.4 Modèles linéaires univariés avec variable catégorielle nominale Une variable catégorielle nominale (non ordonnée) utilisée à elle seule dans un modèle comme variable explicative, est un cas particulier de régression multiple. En effet, l’encodage catégoriel (ou dummyfication) transforme une variable catégorielle nominale en une matrice de modèle comprenant une colonne désignant l’intercept (une série de 1) désignant la catégorie de référence, ainsi que des colonnes pour chacune des autres catégories désignant l’appartenance (1) ou la non appartenance (0) de la catégorie désignée par la colonne. 5.8.1.4.1 L’encodage catégoriel Une variable à \\(C\\) catégories pourra être déclinée en \\(C\\) variables dont chaque colonne désigne par un 1 l’appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l’exemple, créons un vecteur désignant le cultivar de pomme de terre. data &lt;- data.frame(cultivar = c(&#39;Superior&#39;, &#39;Superior&#39;, &#39;Superior&#39;, &#39;Russet&#39;, &#39;Kenebec&#39;, &#39;Russet&#39;)) model.matrix(~cultivar, data) ## (Intercept) cultivarRusset cultivarSuperior ## 1 1 0 1 ## 2 1 0 1 ## 3 1 0 1 ## 4 1 1 0 ## 5 1 0 0 ## 6 1 1 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Nous avons trois catégories, encodées en trois colonnes. La première colonne est un intercept et les deux autres décrivent l’absence (0) ou la présence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l’appartenance à une catégorie est mutuellement exclusive, c’est-à-dire qu’un échantillon ne peut être assigné qu’à une seule catégorie, on peut déduire une catégorie à partir de l’information sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux égales à \\(0\\), on conclura que cultivar_Kenebec est nécessairement égal à \\(1\\). Et si l’un d’entre cultivar_Russet et cultivar_Superior est égal à \\(1\\), cultivar_Kenebec est nécessairement égal à \\(0\\). L’information contenue dans un nombre \\(C\\) de catégorie peut être encodée dans un nombre \\(C-1\\) de colonnes. C’est pourquoi, dans une analyse statistique, on désignera une catégorie comme une référence, que l’on détecte lorsque toutes les autres catégories sont encodées avec des \\(0\\): cette référence sera incluse dans l’intercept. La catégorie de référence par défaut en R est celle la première catégorie dans l’ordre alphabétique. On pourra modifier cette référence avec la fonction relevel(). data$cultivar &lt;- relevel(data$cultivar, ref = &quot;Superior&quot;) model.matrix(~cultivar, data) ## (Intercept) cultivarKenebec cultivarRusset ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 1 ## 5 1 1 0 ## 6 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Pour certains modèles, vous devrez vous assurer vous-même de l’encodage catégoriel. Pour d’autre, en particulier avec l’interface par formule de R, ce sera fait automatiquement. 5.8.1.4.2 Exemple d’application Prenons la topographie du terrain, qui peut prendre plusieurs niveaux. levels(lasrosas.corn$topo) ## [1] &quot;E&quot; &quot;HT&quot; &quot;LO&quot; &quot;W&quot; Explorons le rendement selon la topographie. ggplot(lasrosas.corn, aes(x = topo, y = yield)) + geom_boxplot() Les différences sont évidentes, et la modélisation devrait montrer des effets significatifs. L’encodage catégoriel peut être visualisé en générant la matrice de modèle avec la fonction model.matrix() et l’interface-formule - sans la variable-réponse. model.matrix(~ topo, data = lasrosas.corn) %&gt;% tbl_df() %&gt;% # tbl_df pour transformer la matrice en tableau sample_n(10) ## # A tibble: 10 x 4 ## `(Intercept)` topoHT topoLO topoW ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 1 0 ## 2 1 0 0 1 ## 3 1 0 0 0 ## 4 1 0 0 1 ## 5 1 0 0 1 ## 6 1 0 0 1 ## 7 1 0 0 0 ## 8 1 0 0 0 ## 9 1 1 0 0 ## 10 1 0 0 1 Dans le cas d’un modèle avec une variable catégorielle nominale seule, l’intercept représente la catégorie de référence, ici E. Les autres colonnes spécifient l’appartenance (1) ou la non-appartenance (0) de la catégorie pour chaque observation. Cette matrice de modèle utilisée pour la régression donnera un intercept, qui indiquera l’effet de la catégorie de référence, puis les différences entre les catégories subséquentes et la catégorie de référence. modlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn) summary(modlin_4) ## ## Call: ## lm(formula = yield ~ topo, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.371 -11.933 -1.593 11.080 44.119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6653 0.5399 145.707 &lt;2e-16 *** ## topoHT -30.0526 0.7500 -40.069 &lt;2e-16 *** ## topoLO 6.2832 0.7293 8.615 &lt;2e-16 *** ## topoW -11.8841 0.7039 -16.883 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.59 on 3439 degrees of freedom ## Multiple R-squared: 0.4596, Adjusted R-squared: 0.4591 ## F-statistic: 975 on 3 and 3439 DF, p-value: &lt; 2.2e-16 Le modèle linéaire est équivalent à l’anova, mais les résultats de lm sont plus élaborés. summary(aov(yield ~ topo, data = lasrosas.corn)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## topo 3 622351 207450 975 &lt;2e-16 *** ## Residuals 3439 731746 213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L’analyse de résidus peut être effectuée de la même manière. 5.8.1.5 Modèles linéaires univariés avec variable catégorielle ordinale Bien que j’introduise la régression sur variable catégorielle ordinale à la suite de la section sur les variables nominales, nous revenons dans ce cas à une régression simple, univariée. Voyons un cas à 5 niveaux. statut &lt;- c(&quot;Totalement en désaccord&quot;, &quot;En désaccord&quot;, &quot;Ni en accord, ni en désaccord&quot;, &quot;En accord&quot;, &quot;Totalement en accord&quot;) statut_o &lt;- factor(statut, levels = statut, ordered=TRUE) model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) où 5 est le nombre de niveaux ## (Intercept) statut_o.L statut_o.Q statut_o.C statut_o^4 ## 1 1 -6.324555e-01 0.5345225 -3.162278e-01 0.1195229 ## 2 1 -3.162278e-01 -0.2672612 6.324555e-01 -0.4780914 ## 3 1 -3.510833e-17 -0.5345225 1.755417e-16 0.7171372 ## 4 1 3.162278e-01 -0.2672612 -6.324555e-01 -0.4780914 ## 5 1 6.324555e-01 0.5345225 3.162278e-01 0.1195229 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$statut_o ## [1] &quot;contr.poly&quot; La matrice de modèle a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres désignant différentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils linéairement? De manière quadratique, cubique ou plus loin dans des distributions polynomiales? modmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) %&gt;% gather(variable, valeur, -statut) modmat_tidy$statut &lt;- factor(modmat_tidy$statut, levels = statut, ordered=TRUE) ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) + facet_wrap(. ~ variable) + geom_point() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Règle générale, pour les variables ordinales, on préférera une distribution linéaire, et c’est l’option par défaut de la fonction lm(). L’utilisation d’une autre distribution peut être effectuée à la mitaine en utilisant dans le modèle la colonne désirée de la sortie de la fonction model.matrix(). 5.8.1.6 Régression multiple à plusieurs variables Reprenons le tableau de données du rendement de maïs. head(lasrosas.corn) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05113 -63.84886 72.14 131.5 W 162.60 R1 N5 ## 2 1999 -33.05115 -63.84879 73.79 131.5 W 170.49 R1 N5 ## 3 1999 -33.05116 -63.84872 77.25 131.5 W 168.39 R1 N5 ## 4 1999 -33.05117 -63.84865 76.35 131.5 W 176.68 R1 N5 ## 5 1999 -33.05118 -63.84858 75.55 131.5 W 171.46 R1 N5 ## 6 1999 -33.05120 -63.84851 70.24 131.5 W 170.56 R1 N5 Pour ajouter des variables au modèle dans l’interface-formule, on additionne les noms de colonne. La variable lat désigne la latitude, la variable long désigne la latitude et la variable bv (brightness value) désigne la teneur en matière organique du sol (plus bv est élevée, plus faible est la teneur en matière organique). modlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) summary(modlin_5) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.946e+05 3.309e+04 5.882 4.45e-09 *** ## lat 5.541e+03 4.555e+02 12.163 &lt; 2e-16 *** ## long 1.776e+02 4.491e+02 0.395 0.693 ## nitro 6.867e-02 5.451e-03 12.597 &lt; 2e-16 *** ## topoHT -2.665e+01 1.087e+00 -24.520 &lt; 2e-16 *** ## topoLO 5.565e+00 1.035e+00 5.378 8.03e-08 *** ## topoW -1.465e+01 1.655e+00 -8.849 &lt; 2e-16 *** ## bv -5.089e-01 3.069e-02 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 L’ampleur des coefficients est relatif à l’échelle de la variable. En effet, un coefficient de 5541 sur la variable lat n’est pas comparable au coefficient de la variable bv, de -0.5089, étant donné que les variables ne sont pas exprimées avec la même échelle. Pour les comparer sur une même base, on peut centrer (soustraire la moyenne) et réduire (diviser par l’écart-type). scale_vec &lt;- function(x) as.vector(scale(x)) # la fonction scale génère une matrice: nous désirons un vecteur lasrosas.corn_sc &lt;- lasrosas.corn %&gt;% mutate_at(c(&quot;lat&quot;, &quot;long&quot;, &quot;nitro&quot;, &quot;bv&quot;), scale_vec) modlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.9114 0.6666 118.376 &lt; 2e-16 *** ## lat 3.9201 0.3223 12.163 &lt; 2e-16 *** ## long 0.3479 0.8796 0.395 0.693 ## nitro 2.9252 0.2322 12.597 &lt; 2e-16 *** ## topoHT -26.6487 1.0868 -24.520 &lt; 2e-16 *** ## topoLO 5.5647 1.0347 5.378 8.03e-08 *** ## topoW -14.6487 1.6555 -8.849 &lt; 2e-16 *** ## bv -4.9253 0.2971 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Typiquement, les variables catégorielles, qui ne sont pas mises à l’échelle, donneront des coefficients plus élevées, et devrons être évaluées entre elles et non comparativement aux variables mises à l’échelle. Une manière conviviale de représenter des coefficients consiste à créer un tableau (fonction tibble()) incluant les coefficients ainsi que leurs intervalles de confiance, puis à les porter graphiquement. intervals &lt;- tibble(Estimate = coefficients(modlin_5_sc)[-1], # [-1] enlever l&#39;intercept LL = confint(modlin_5_sc)[-1, 1], # [-1, ] enlever la première ligne, celle de l&#39;intercept UL = confint(modlin_5_sc)[-1, 2], variable = names(coefficients(modlin_5_sc)[-1])) intervals ## # A tibble: 7 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3.92 3.29 4.55 lat ## 2 0.348 -1.38 2.07 long ## 3 2.93 2.47 3.38 nitro ## 4 -26.6 -28.8 -24.5 topoHT ## 5 5.56 3.54 7.59 topoLO ## 6 -14.6 -17.9 -11.4 topoW ## 7 -4.93 -5.51 -4.34 bv ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient standardisé&quot;, y = &quot;&quot;) On y voit qu’à l’exception de la variable long, tous les coefficients sont différents de 0. Le coefficient bv est négatif, indiquant que plus la valeur de bv est élevé (donc plus le sol est pauvre en matière organique), plus le rendement est faible. Plus la latitude est élevée (plus on se dirige vers le Nord de l’Argentine), plus le rendement est élevé. La dose d’azote a aussi un effet statistique positif sur le rendement. Quant aux catégories topographiques, elles sont toutes différentes de la catégorie E, ne croisant pas le zéro. De plus, les intervalles de confiance ne se chevauchant pas, on peut conclure en une différence significative d’une à l’autre. Bien sûr, tout cela au seuil de confiance de 0.05. On pourra retrouver des cas où l’effet combiné de plusieurs variables diffère de l’effet des deux variables prises séparément. Par exemple, on pourrait évaluer l’effet de l’azote et celui de la topographie dans un même modèle, puis y ajouter une interaction entre l’azote et la topographie, qui définira des effets supplémentaires de l’azote selon chaque catégorie topographique. C’est ce que l’on appelle une interaction. Dans l’interface-formule, l’interaction entre l’azote et la topographie est notée nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche équivalente est d’utiliser le raccourci yield ~ nitro*topo. modlin_5_sc &lt;- lm(yield ~ nitro*topo, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.984 -11.985 -1.388 10.339 40.636 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6999 0.5322 147.870 &lt; 2e-16 *** ## nitro 1.8131 0.5351 3.388 0.000711 *** ## topoHT -30.0052 0.7394 -40.578 &lt; 2e-16 *** ## topoLO 6.2026 0.7190 8.627 &lt; 2e-16 *** ## topoW -11.9628 0.6939 -17.240 &lt; 2e-16 *** ## nitro:topoHT 1.2553 0.7461 1.682 0.092565 . ## nitro:topoLO 0.5695 0.7186 0.792 0.428141 ## nitro:topoW 0.7702 0.6944 1.109 0.267460 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.38 on 3435 degrees of freedom ## Multiple R-squared: 0.4756, Adjusted R-squared: 0.4746 ## F-statistic: 445.1 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Les résultats montre des effets de l’azote et des catégories topographiques, mais il y a davantage d’incertitude sur les interactions, indiquant que l’effet statistique de l’azote est sensiblement le même indépendamment des niveaux topographiques. 5.8.1.7 Attention à ne pas surcharger le modèle Il est possible d’ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a d’interactions, plus votre modèle comprendra de variables et vos tests d’hypothèse perdront en puissance statistique. 5.8.1.8 Les modèles linéaires généralisés Dans un modèle linéaire ordinaire, un changement constant dans les variables explicatives résulte en un changement constant de la variable-réponse. Cette supposition ne serait pas adéquate si la variable-réponse était un décompte, si elle est booléenne ou si, de manière générale, la variable-réponse ne suivait pas une distribution continue. Ou, de manière plus spécifique, il n’y a pas moyen de retrouver une distribution normale des résidus? On pourra bien sûr transformer les variables (sujet du chapitre 6, en développement). Mais il pourrait s’avérer impossible, ou tout simplement non souhaitable de transformer les variables. Le modèle linéaire généralisé (MLG, ou generalized linear model - GLM) est une généralisation du modèle linéaire ordinaire chez qui la variable-réponse peut être caractérisé par une distribution de Poisson, de Bernouilli, etc. Prenons d’abord cas d’un décompte de vers fil-de-fer (worms) retrouvés dans des parcelles sous différents traitements (trt). Les décomptes sont typiquement distribué selon une loi de Poisson. cochran.wireworms %&gt;% ggplot(aes(x = worms)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Explorons les décomptes selon les traitements. cochran.wireworms %&gt;% ggplot(aes(x = trt, y = worms)) + geom_boxplot() Les traitements semble à première vue avoir un effet comparativement au contrôle. Lançons un MLG avec la fonction glm(), et spécifions que la sortie est une distribution de Poisson. modglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = &quot;poisson&quot;) summary(modglm_1) ## ## Call: ## glm(formula = worms ~ trt, family = &quot;poisson&quot;, data = cochran.wireworms) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8279 -0.9455 -0.2862 0.6916 3.1888 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.1823 0.4082 0.447 0.655160 ## trtM 1.6422 0.4460 3.682 0.000231 *** ## trtN 1.7636 0.4418 3.991 6.57e-05 *** ## trtO 1.5755 0.4485 3.513 0.000443 *** ## trtP 1.3437 0.4584 2.931 0.003375 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 64.555 on 24 degrees of freedom ## Residual deviance: 38.026 on 20 degrees of freedom ## AIC: 125.64 ## ## Number of Fisher Scoring iterations: 5 Il est très probable (p-value de ~0.66) qu’un intercept de 0.18 ayant une erreur standard de 0.4082 ait été généré depuis une population dont l’intercept est nul: autrement dit, le contrôle n’a probablement pas eu d’effet. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils être considérés comme équivalents? intervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l&#39;intercept LL = confint(modglm_1)[, 1], # [-1, ] enlever la première ligne, celle de l&#39;intercept UL = confint(modglm_1)[, 2], variable = names(coefficients(modglm_1))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... intervals ## # A tibble: 5 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.182 -0.740 0.888 (Intercept) ## 2 1.64 0.840 2.62 trtM ## 3 1.76 0.972 2.74 trtN ## 4 1.58 0.766 2.56 trtO ## 5 1.34 0.509 2.34 trtP ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) Les intervalles de confiance se superposant, on ne peut pas conclure qu’un traitement est lié à une réduction plus importante de vers qu’un autre, au seuil 0.05. Maintenant, à défaut de trouver un tableau de données plus approprié, prenons le tableau mtcars, qui rassemble des données sur des modèles de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droit et 1 s’ils sont placés en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du véhicule (wt)? mtcars %&gt;% sample_n(6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 mtcars %&gt;% ggplot(aes(x = wt, y = vs)) + geom_point() Il semble y avoir une tendance: les véhicules plus lourds ont plutôt des pistons droits (vs = 0). Vérifions cela. modglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = binomial) summary(modglm_2) ## ## Call: ## glm(formula = vs ~ wt, family = binomial, data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9003 -0.7641 -0.1559 0.7223 1.5736 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.7147 2.3014 2.483 0.01302 * ## wt -1.9105 0.7279 -2.625 0.00867 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.860 on 31 degrees of freedom ## Residual deviance: 31.367 on 30 degrees of freedom ## AIC: 35.367 ## ## Number of Fisher Scoring iterations: 5 Exercice. Analyser les résultats. 5.8.1.9 Les modèles non-linéaires La hauteur d’un arbre en fonction du temps n’est typiquement pas linéaire. Elle tend à croître de plus en plus lentement jusqu’à un plateau. De même, le rendement d’une culture traité avec des doses croissantes de fertilisants tend à atteindre un maximum, puis à se stabiliser. Ces phénomènes ne peuvent pas être approximés par des modèles linéaires. Examinons les données du tableau engelstad.nitro. engelstad.nitro %&gt;% sample_n(10) ## loc year nitro yield ## 38 Knoxville 1963 67 73.2 ## 40 Knoxville 1963 201 91.2 ## 21 Jackson 1965 134 60.5 ## 35 Knoxville 1962 268 78.4 ## 22 Jackson 1965 201 70.2 ## 12 Jackson 1963 335 87.0 ## 24 Jackson 1965 335 73.0 ## 20 Jackson 1965 67 47.6 ## 15 Jackson 1964 134 55.2 ## 48 Knoxville 1964 335 84.5 engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line() + geom_point() Le modèle de Mitscherlich pourrait être utilisé. \\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\] où \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est l’asymptote vers laquelle la courbe converge à dose croissante, \\(E\\) est l’équivalent de dose fourni par l’environnement et \\(R\\) est le taux de réponse. Explorons la fonction. mitscherlich_f &lt;- function(x, A, E, R) { A * (1 - exp(-R*(E + x))) } x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Exercice. Changez les paramètres pour visualiser comment la courbe réagit. Nous pouvons décrire le modèle grâce à l’interface formule dans la fonction nls(). Notez que les modèles non-linéaires demandent des stratégies de calcul différentes de celles des modèles linéaires. En tout temps, nous devons identifier des valeurs de départ raisonnables pour les paramètres dans l’argument start. Vous réussirez rarement à obtenir une convergence du premier coup avec vos paramètres de départ. Le défi est d’en trouver qui permettront au modèle de converger. Parfois, le modèle ne convergera jamais. D’autres fois, il convergera vers des solutions différentes selon les variables de départ choisies. &lt; #modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), # data = engelstad.nitro, # start = list(A = 50, E = 10, R = 0.2)) Le modèle ne converge pas. Essayons les valeurs prises plus haut, lors de la création du graphique, qui semblent bien s’ajuster. modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = list(A = 75, E = 30, R = 0.02)) Bingo! Voyons maintenant le sommaire. summary(modnl_1) ## ## Formula: yield ~ A * (1 - exp(-R * (E + nitro))) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## A 75.023427 3.331860 22.517 &lt;2e-16 *** ## E 66.164112 27.251590 2.428 0.0184 * ## R 0.012565 0.004881 2.574 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.34 on 57 degrees of freedom ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 8.053e-06 Les paramètres sont significativement différents de zéro au seuil 0.05, et donnent la courbe suivante. x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = coefficients(modnl_1)[1], E = coefficients(modnl_1)[2], R = coefficients(modnl_1)[3]) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Et les résidus… tibble(res = residuals(modnl_1)) %&gt;% ggplot(aes(x = res)) + geom_histogram(bins = 20) tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %&gt;% ggplot(aes(x = nitro, y = res)) + geom_point() + geom_hline(yintercept = 0, colour = &quot;red&quot;) Les résidus ne sont pas distribués normalement, mais semble bien partagés de part et d’autre de la courbe. 5.8.2 Modèles à effets mixtes Lorsque l’on combine des variables fixes (testées lors de l’expérience) et des variables aléatoire (variation des unités expérimentales), on obtient un modèle mixte. Les modèles mixtes peuvent être univariés, multivariés, linéaires ordinaires ou généralisés ou non linéaires. À la différence d’un effet fixe, un effet aléatoire sera toujours distribué normalement avec une moyenne de 0 et une certaine variance. Dans un modèle linéaire où l’effet aléatoire est un décalage d’intercept, cet effet s’additionne aux effets fixes: \\[ y = X \\beta + Z b + \\epsilon \\] où: \\(Z\\) est la matrice du modèle à \\(n\\) observations et \\(p\\) variables aléatoires. Les variables aléatoires sont souvent des variables nominales qui subissent un encodage catégoriel. \\[ Z = \\left( \\begin{matrix} z_{11} &amp; \\cdots &amp; z_{1p} \\\\ z_{21} &amp; \\cdots &amp; z_{2p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ z_{n1} &amp; \\cdots &amp; z_{np} \\end{matrix} \\right) \\] \\(b\\) est la matrice des \\(p\\) coefficients aléatoires. \\[ b = \\left( \\begin{matrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_p \\end{matrix} \\right) \\] Le tableau lasrosas.corn, utilisé précédemment, contenait trois répétitions effectués au cours de deux années, 1999 et 2001. Étant donné que la répétition R1 de 1999 n’a rien à voir avec la répétition R1 de 2001, on dit qu’elle est emboîtée dans l’année. Le module nlme nous aidera à monter notre modèle mixte. library(&quot;nlme&quot;) mmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv, random = ~ 1|year/rep, data = lasrosas.corn) À ce stade vous devriez commencer à être familier avec l’interface formule et vous deviez saisir l’argument fixed, qui désigne l’effet fixe. L’effet aléatoire, random, suit un tilde ~. À gauche de la barre verticale |, on place les variables désignant les effets aléatoire sur la pente. Nous n’avons pas couvert cet aspect, alors nous le laissons à 1. À droite, on retrouve un structure d’emboîtement désignant l’effet aléatoire: le premier niveau est l’année, dans laquelle est emboîtée la répétition. summary(mmodlin_1) ## Linear mixed-effects model fit by REML ## Data: lasrosas.corn ## AIC BIC logLik ## 26535.37 26602.93 -13256.69 ## ## Random effects: ## Formula: ~1 | year ## (Intercept) ## StdDev: 20.35425 ## ## Formula: ~1 | rep %in% year ## (Intercept) Residual ## StdDev: 11.17447 11.35617 ## ## Fixed effects: yield ~ lat + long + nitro + topo + bv ## Value Std.Error DF t-value p-value ## (Intercept) -1379436.9 55894.55 3430 -24.679273 0.000 ## lat -25453.0 1016.53 3430 -25.039084 0.000 ## long -8432.3 466.05 3430 -18.092988 0.000 ## nitro 0.0 0.00 3430 1.739757 0.082 ## topoHT -27.7 0.92 3430 -30.122438 0.000 ## topoLO 6.8 0.88 3430 7.804733 0.000 ## topoW -16.7 1.40 3430 -11.944793 0.000 ## bv -0.5 0.03 3430 -19.242424 0.000 ## Correlation: ## (Intr) lat long nitro topoHT topoLO topoW ## lat 0.897 ## long 0.866 0.555 ## nitro 0.366 0.391 0.247 ## topoHT 0.300 -0.017 0.582 0.024 ## topoLO -0.334 -0.006 -0.621 -0.038 -0.358 ## topoW 0.403 -0.004 0.762 0.027 0.802 -0.545 ## bv -0.121 -0.012 -0.214 -0.023 -0.467 0.346 -0.266 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.32360269 -0.66781575 -0.07450856 0.61587533 3.96434001 ## ## Number of Observations: 3443 ## Number of Groups: ## year rep %in% year ## 2 6 La sortie est semblable à celle de la fonction lm(). 5.8.2.1 Modèles mixtes non-linéaires Le modèle non linéaire créé plus haut liait le rendement à la dose d’azote. Toutefois, les unités expérimentales (le site loc et l’année year) n’étaient pas pris en considération. Nous allons maintenant les considérer. Nous devons décider la structure de l’effet aléatoire, et sur quelles variables il doit être appliqué - la décision appartient à l’analyste. Il me semble plus convenable de supposer que le site et l’année affectera le rendement maximum plutôt que l’environnement et le taux: les effets aléatoires seront donc affectés à la variable A. Les effets aléatoires n’ont pas de structure d’emboîtement. L’effet de l’année sur A sera celui d’une pente et l’effet de site sera celui de l’intercept. La fonction que nous utiliserons est nlme(). mm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = c(A = 75, E = 30, R = 0.02), fixed = list(A ~ 1, E ~ 1, R ~ 1), random = A ~ year | loc) summary(mm) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: yield ~ A * (1 - exp(-R * (E + nitro))) ## Data: engelstad.nitro ## AIC BIC logLik ## 477.2286 491.889 -231.6143 ## ## Random effects: ## Formula: A ~ year | loc ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## A.(Intercept) 4.977351383 A.(In) ## A.year 0.002531519 -0.483 ## Residual 11.152773395 ## ## Fixed effects: list(A ~ 1, E ~ 1, R ~ 1) ## Value Std.Error DF t-value p-value ## A.(Intercept) 74.56787 4.722754 56 15.789063 0.0000 ## E 65.57001 25.534762 56 2.567872 0.0129 ## R 0.01308 0.004807 56 2.720242 0.0087 ## Correlation: ## A.(In) E ## E 0.379 ## R -0.483 -0.934 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.83352051 -0.89275605 0.07417141 0.68349756 1.82442946 ## ## Number of Observations: 60 ## Number of Groups: 2 Les modèles mixtes non linéaires peuvent devenir très complexes lorsque les paramètres, par exemple A, E et R, sont eux-même affectés linéairement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al. (2017) ainsi que les calculs associés à l’article. Ou écrivez-moi un courriel pour en discuter! Note. L’interprétation de p-values sur les modèles mixtes est controversée. À ce sujet, ??? Bates a écrit une longue lettre à la communauté de développement du module lme4, une alternative à nlme, qui remet en cause l’utilisation des p-values, ici. De plus en plus, pour les modèles mixtes, on se tourne vers les statistiques bayésiennes, couvertes dans l’annexe de cette section (statitiques_bayes.ipynb, en développement). À cet effet, le module brms automatise bien des aspects de la modélisation mixte bayésienne. 5.8.3 Aller plus loin 5.8.3.1 Statistiques générales: The analysis of biological data 5.8.3.2 Statistiques avec R Disponibles en version électronique à la bibliothèque de l’Université Laval: Introduction aux statistiques avec R: Introductory statistics with R Approfondir les statistiques avec R: The R Book, Second edition Approfondir les modèles à effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R ModernDive, un livre en ligne offrant une approche moderne avec le package moderndive. "],
["chapitre-biostats-bayes.html", "6 Introduction à l’analyse bayésienne en écologie 6.1 Qu’est-ce que c’est? 6.2 Pourquoi l’utiliser? 6.3 Comment l’utiliser? 6.4 Faucons pélerins 6.5 Statistiques d’une population 6.6 Test de t: Différence entre des groupes 6.7 Pour aller plus loin", " 6 Introduction à l’analyse bayésienne en écologie Les statistiques bayésiennes forment une trousse d’outils à garder dans votre pack sack. 6.1 Qu’est-ce que c’est? En deux mots: modélisation probabiliste. Un approche de modélisation probabiliste se servant au mieux de l’information disponible. Pour calculer les probabilités d’une variable inconnu en mode bayésien, nous avons besoin: De données D’un modèle D’une idée plus ou moins précise du résultat avant d’avoir analysé les données De manière plus formelle, le théorème de Bayes (qui forme la base de l’analyse bayéseienne), dit que la distribution de probabilité des paramètres d’un modèle (par exemple, la moyenne ou une pente) est proportionnelle à la mutliplication de la distribution de probabilité estimée des paramètres et la distribution de probabilité émergeant des données. Plus formellement, \\[P\\left(\\theta | y \\right) = \\frac{P\\left(y | \\theta \\right) \\times P\\left(\\theta\\right)}{P\\left(y \\right)}\\], où \\(P\\left(\\theta | y \\right)\\) \\(-\\) la probabilité d’obtenir des paramètres \\(\\theta\\) à partir des données \\(y\\) \\(-\\) est la distribution de probabilité a posteriori, calculée à partir de votre a prioti \\(P\\left(\\theta\\right)\\) \\(-\\) la probabilité d’obtenir des paramètres \\(\\theta\\) sans égard aux données, selon votre connaissance du phénomène \\(-\\) et vos données observées \\(P\\left(y | \\theta \\right)\\) \\(-\\) la probabilité d’obtenir les données \\(y\\) étant donnés les paramètres \\(\\theta\\) qui régissent le phénomène. \\(P\\left(y\\right)\\), la probabilité d’observer les données, est appellée la vraissemblance marginale, et assure que la somme des probabilités est nulle. 6.2 Pourquoi l’utiliser? Avec la notion fréquentielle de probabilité, on teste la probabilité d’observer les données recueillies étant donnée l’absence d’effet réel (qui est l’hypothèse nulle généralement adoptée). La notion bayésienne de probabilité combine la connaissance que l’on a d’un phénomène et les données observées pour estimer la probabilité qu’il existe un effet réel. En d’autre mots, les stats fréquentielles testent si les données concordent avec un modèle du réel, tandis que les stats bayésiennes évaluent, selon les données, la probabilité que le modèle soit réel. Le hic, c’est que lorsqu’on utilise les statistiques fréquentielles pour répondre à une question bayésienne, on s’expose à de mauvaises interprétations. Par exemple, lors d’un projet considérant la vie sur Mars, les stats fréquentielles évalueront si les données recueillies sont conformes ou non avec l’hypothèse de la vie sur Mars. Par contre, pour évaluer la probabilité de l’existance de vie sur Mars, on devra passer par les stats bayésiennes (exemple tirée du billet Dynamic Ecology – Frequentist vs. Bayesian statistics: resources to help you choose). 6.3 Comment l’utiliser? Bien que la formule du théorème de Bayes soit plutôt simple, calculer une fonction a posteriori demandera de passer par des algorithmes de simulation, ce qui pourrait demander une bonne puissance de calcul, et des outils appropriés. R comporte une panoplie d’outils pour le calcul bayésien générique (rstan, rjags, MCMCpack, etc.), et d’autres outils pour des besoins particuliers (brms: R package for Bayesian generalized multivariate non-linear multilevel models using Stan). Nous utiliserons ici le module générique greta, qui permet de générer de manière conviviale plusieurs types de modèles bayésiens. Pour installer greta, vous devez préalablement installer Python, gréé des modules tensorflow et tensorflow-probability en suivant le guide. En somme, vous devez d’abord installer greta (install.packages(\"greta\")). Puis vous devez installer une distribution de Python – je vous suggère Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes (une connection internet est nécessaire pour télécharger les modules). ``` install_tensorflow(method = “conda”) reticulate::conda_install(“r-tensorflow”, “tensorflow-probability”, pip = TRUE) ``` 6.4 Faucons pélerins Empruntons un exemple du livre Introduction to WinBUGS for Ecologists: A Bayesian Approach to Regression, ANOVA and Related Analyses, de Marc Kéry et examinons la masse de faucons pélerins. Mais alors que Marc Kéry utilise WinBUGS, un logiciel de résolution de problème en mode bayésien, nous utiliserons greta. Source: Wikimedia Commons Pour une première approche, nous allons estimer la masse moyenne d’une population de faucons pélerins. À titre de données, générons des nombres aléatoires. Cette stratégie permet de valider les statistiques en les comparant aux paramètre que l’on impose. Ici, nous imposons une moyenne de 600 grammes et un écart-type de 30 grammes. Générons une séries de données avec 20 échantillons. library(&quot;tidyverse&quot;) set.seed(5682) y20 &lt;- rnorm(n = 20, mean=600, sd = 30) y200 &lt;- rnorm(n = 200, mean=600, sd = 30) par(mfrow = c(1, 2)) hist(y20, breaks=5) hist(y200, breaks=20) Je crée une fonction qui retourne la moyenne et l’erreur sur la moyenne ou sur la distribution. Calculons les statistiques classiques. confidence_interval &lt;- function(x, on=&quot;deviation&quot;, distribution=&quot;t&quot;, level=0.95) { m &lt;- mean(x) se &lt;- sd(x) n &lt;- length(x) if (distribution == &quot;t&quot;) { error &lt;- se * qt((1+level)/2, n-1) } else if (distribution == &quot;normal&quot;) { error &lt;- se * qnorm((1+level)/2) } if (on == &quot;error&quot;) { error &lt;- error/sqrt(n) } return(c(ll = m-error, mean = m, ul = m+error)) } print(&quot;Déviation, 95%&quot;) ## [1] &quot;Déviation, 95%&quot; print(round(confidence_interval(y20, on=&#39;deviation&#39;, level=0.95), 2)) ## ll mean ul ## 532.23 598.85 665.47 print(&quot;Erreur, 95%&quot;) ## [1] &quot;Erreur, 95%&quot; print(round(confidence_interval(y20, on=&#39;error&#39;, level=0.95), 2)) ## ll mean ul ## 583.96 598.85 613.75 print(&quot;Écart-type&quot;) ## [1] &quot;Écart-type&quot; print(round(sd(y20), 2)) ## [1] 31.83 En faisant cela, nous prenons pour acquis que les données sont distribuées normalement. En fait, nous savons qu’elles devraient l’être pour de grands échantillons, puisque nous avons nous-même généré les données. Par contre, comme observateur par exemple de la série de 20 données générées, la distribution est définitivement asymétrique. Sous cet angle, la moyenne, ainsi que l’écart-type, pourraient être des paramètres biaisés. Nous pouvons justifier le choix d’une loi normale par des connaissances a priori des distributions de masse parmi des espèces d’oiseau. Ou bien transformer les données pour rendre leur distribution normale (chapitre 7). 6.5 Statistiques d’une population 6.5.1 greta En mode bayésien, nous devons définir la connaissance a priori sous forme de variables aléatoires non-observées selon une distribution. Prenons l’exemple des faucons pélerins. Disons que nous ne savons pas à quoi ressemble la moyenne du groupe a priori. Nous pouvons utiliser un a priori vague, où la masse moyenne peut prendre n’importe quelle valeur entre 0 et 2000 grammes, sans préférence: nous lui imposons donc un a priori selon une distribution uniforme. Idem pour l’écart-type library(&quot;greta&quot;) ## ## Attaching package: &#39;greta&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## slice ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice ## The following objects are masked from &#39;package:stats&#39;: ## ## binomial, cov2cor, poisson ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, apply, backsolve, beta, chol2inv, colMeans, colSums, ## diag, eigen, forwardsolve, gamma, identity, rowMeans, rowSums, ## sweep, tapply library(&quot;DiagrammeR&quot;) library (&quot;bayesplot&quot;) ## This is bayesplot version 1.6.0 ## - Online documentation and vignettes at mc-stan.org/bayesplot ## - bayesplot theme set to bayesplot::theme_default() ## * Does _not_ affect other ggplot2 plots ## * See ?bayesplot_theme_set for details on theme setting library(&quot;tidybayes&quot;) ## NOTE: As of tidybayes version 1.0, several functions, arguments, and output column names ## have undergone significant name changes in order to adopt a unified naming scheme. ## See help(&#39;tidybayes-deprecated&#39;) for more information. param_mean &lt;- uniform(min = 0, max = 2000) param_sd &lt;- uniform(min = 0, max = 100) La fonction a porteriori inclue la fonction de vraissemblance ainsi que la connaissancew a priori. distribution(y20) &lt;- normal(param_mean, param_sd) Le tout forme un modèle pour apprécier y, la masse des faucons pélerins. m &lt;- model(param_mean, param_sd) plot(m) Légende: Nous pouvons enfin lancer le modèle . draws &lt;- mcmc(m, n_samples = 1000) ## ## running 4 chains simultaneously on up to 4 cores ## warmup 0/1000 | eta: ?s warmup == 50/1000 | eta: 20s warmup ==== 100/1000 | eta: 14s warmup ====== 150/1000 | eta: 12s warmup ======== 200/1000 | eta: 11s warmup ========== 250/1000 | eta: 9s warmup =========== 300/1000 | eta: 8s warmup ============= 350/1000 | eta: 8s warmup =============== 400/1000 | eta: 7s warmup ================= 450/1000 | eta: 6s warmup =================== 500/1000 | eta: 5s warmup ===================== 550/1000 | eta: 5s warmup ======================= 600/1000 | eta: 4s warmup ========================= 650/1000 | eta: 4s warmup =========================== 700/1000 | eta: 3s warmup ============================ 750/1000 | eta: 3s warmup ============================== 800/1000 | eta: 2s warmup ================================ 850/1000 | eta: 2s warmup ================================== 900/1000 | eta: 1s warmup ==================================== 950/1000 | eta: 1s warmup ====================================== 1000/1000 | eta: 0s ## sampling 0/1000 | eta: ?s sampling == 50/1000 | eta: 5s sampling ==== 100/1000 | eta: 4s sampling ====== 150/1000 | eta: 4s sampling ======== 200/1000 | eta: 4s sampling ========== 250/1000 | eta: 4s sampling =========== 300/1000 | eta: 3s sampling ============= 350/1000 | eta: 3s sampling =============== 400/1000 | eta: 3s sampling ================= 450/1000 | eta: 3s sampling =================== 500/1000 | eta: 2s sampling ===================== 550/1000 | eta: 2s sampling ======================= 600/1000 | eta: 2s sampling ========================= 650/1000 | eta: 2s sampling =========================== 700/1000 | eta: 1s sampling ============================ 750/1000 | eta: 1s sampling ============================== 800/1000 | eta: 1s sampling ================================ 850/1000 | eta: 1s sampling ================================== 900/1000 | eta: 1s sampling ==================================== 950/1000 | eta: 0s sampling ====================================== 1000/1000 | eta: 0s L’inspection de l’échantillonnage peut être effectuée grâce au module bayesplot. mcmc_combo(draws, combo = c(&quot;hist&quot;, &quot;trace&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. L’échantillonnage semble stable. Voyons la distribution a posteriori des paramètres. draws_tidy &lt;- draws %&gt;% spread_draws(param_mean, param_sd) print(&quot;Moyenne:&quot;) ## [1] &quot;Moyenne:&quot; confidence_interval(x = draws_tidy$param_mean, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) ## ll mean ul ## 583.6427 599.1018 614.5610 print(&quot;Écart-type:&quot;) ## [1] &quot;Écart-type:&quot; confidence_interval(x = draws_tidy$param_sd, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) ## ll mean ul ## 22.12636 34.26526 46.40416 L’a priori étant vague, les résultats de l’analyse bayésienne sont comparables aux statistiques fréquentielles. print(&quot;Erreur, 95%&quot;) ## [1] &quot;Erreur, 95%&quot; print(round(confidence_interval(y20, on=&#39;error&#39;, level=0.95), 2)) ## ll mean ul ## 583.96 598.85 613.75 Les résultats des deux approches peuvent néanmoins être interprétés de manière différente. En ce qui a trait à la moyenne: Fréquentiel. Il y a une probabilité de 95% que mes données aient été générées à partir d’une moyenne se situant entre 584 et 614 grammes. Bayésien. Étant donnée mes connaissances (vagues) de la moyenne et de l’écart-type avant de procéder à l’analyse (a priori), il y a une probabilité de 95% que la moyenne de la masse de la population se situe entre 583 et 614 grammes. Nous avons maintenant une idée de la distribution de moyenne de la population. Mais, rarement, une analyse s’arrêtera à ce stade. Il arrive souvent que l’on doive comparer les pparamètres de deux, voire plusieurs groupes. Par exemple, comparer des populations vivants dans des écosystèmes différents, ou comparer un traitement à un placébo. Ou bien, comparer, dans une même population de faucons pélerins, l’envergure des ailes des mâles et celle des femelles. 6.6 Test de t: Différence entre des groupes Pour comparer des groupes, on exprime généralement une hypothèse nulle, qui typiquement pose qu’il n’y a pas de différence entre les groupes. Puis, on choisit un test statistique pour déterminer si les distributions des données observées sont plausibles dans si l’hypothèse nulle est vraie. En d’autres mots, le test statistique exprime la probabilité que l’on obtienne les données obtenues s’il n’y avait pas de différence entre les groupes. Par exemple, si vous obtenez une p-value de moins de 0.05 après un test de comparaison et l’hypothèse nulle pose qu’il n’y a pas de différence entre les groupes, cela signifie qu’il y a une probabilité de 5% que vous ayiez obtenu ces données s’il n’y avait en fait pas de différence entre les groupe. Il serait donc peu probable que vos données euent été générées comme telles s’il n’y avait en fait pas de différence. n_f &lt;- 30 moy_f &lt;- 105 n_m &lt;- 20 moy_m &lt;- 77.5 sd_fm &lt;- 2.75 set.seed(21526) envergure_f &lt;- rnorm(mean=moy_f, sd=sd_fm, n=n_f) envergure_m &lt;- rnorm(mean=moy_m, sd=sd_fm, n=n_m) envergure_f_df &lt;- data.frame(Sex = &quot;Female&quot;, Wingspan = envergure_f) envergure_m_df &lt;- data.frame(Sex = &quot;Male&quot;, Wingspan = envergure_m) envergure_df &lt;- rbind(envergure_f_df, envergure_m_df) envergure_df %&gt;% ggplot(aes(x=Wingspan)) + geom_histogram(aes(y=..density.., fill=Sex)) + geom_density(aes(linetype=Sex, y=..density..)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Et les statistiques des deux groupesL envergure_df %&gt;% group_by(Sex) %&gt;% summarise(mean = mean(Wingspan), sd = sd(Wingspan), n = n()) ## # A tibble: 2 x 4 ## Sex mean sd n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Female 105. 2.46 30 ## 2 Male 77.0 3.19 20 Évaluer s’il y a une différence significative peut se faire avec un test de t (ou de Student). t.test(envergure_f, envergure_m) ## ## Welch Two Sample t-test ## ## data: envergure_f and envergure_m ## t = 33.235, df = 33.665, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 26.29232 29.71848 ## sample estimates: ## mean of x mean of y ## 105.04267 77.03727 La probabilité que les données ait été générées de la sorte si les deux groupes n’était semblables est très faible (p-value &lt; 2.2e-16). On obtiendrait sensiblement les mêmes résultats avec une régression linéaire. linmod &lt;- lm(Wingspan ~ Sex, envergure_df) summary(linmod) ## ## Call: ## lm(formula = Wingspan ~ Sex, data = envergure_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.221 -1.938 0.219 2.046 4.686 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 105.0427 0.5062 207.51 &lt;2e-16 *** ## SexMale -28.0054 0.8004 -34.99 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.773 on 48 degrees of freedom ## Multiple R-squared: 0.9623, Adjusted R-squared: 0.9615 ## F-statistic: 1224 on 1 and 48 DF, p-value: &lt; 2.2e-16 Le modèle linéaire est plus informatif. Il nous apprend que l’envergure des ailes des mâles est en moyenne plus faible de 28.0 cm que celle des femelles… confint(linmod, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 104.02487 106.06047 ## SexMale -29.61468 -26.39612 … avec un intervalle de confiance entre -29.6 cm à -26.4 cm. Utilisons l’information dérivée de statistiques classiques dans nos a priori. Oui-oui, on peut faire ça. Mais attention, un a priori trop précis ou trop collé sur nos données orientera le modèle vers une solution préalablement établie: ce qui constituerait aucune avancée par rapport à l’a priori. Nous allons utiliser a priori pour les deux groupes la moyenne des deux groupes, et comme dispersion la moyenne le double de l’écart-type. Rappelons que cet écart-type est l’a priori de écart-type sur la moyenne, non pas de la population. Procédons à la création d’un modèle greta. Nous utiliserons la régression linéaire préférablement au test de t. is_female &lt;- model.matrix(~envergure_df$Sex)[, 2] int &lt;- normal(600, 30) coef &lt;- normal(30, 10) sd &lt;- cauchy(0, 10, truncation = c(0, Inf)) mu &lt;- int + coef * is_female distribution(envergure_df$Wingspan) &lt;- normal(mu, sd) m &lt;- model(int, coef, sd, mu) plot(m) Go! draws &lt;- mcmc(m, n_samples = 1000) ## ## running 4 chains simultaneously on up to 4 cores ## warmup 0/1000 | eta: ?s warmup == 50/1000 | eta: 20s | 18% bad warmup ==== 100/1000 | eta: 15s | 9% bad warmup ====== 150/1000 | eta: 13s | 6% bad warmup ======== 200/1000 | eta: 11s | 4% bad warmup ========== 250/1000 | eta: 10s | 4% bad warmup =========== 300/1000 | eta: 9s | 3% bad warmup ============= 350/1000 | eta: 8s | 3% bad warmup =============== 400/1000 | eta: 7s | 2% bad warmup ================= 450/1000 | eta: 7s | 2% bad warmup =================== 500/1000 | eta: 6s | 2% bad warmup ===================== 550/1000 | eta: 5s | 2% bad warmup ======================= 600/1000 | eta: 5s | 2% bad warmup ========================= 650/1000 | eta: 4s | 1% bad warmup =========================== 700/1000 | eta: 3s | 1% bad warmup ============================ 750/1000 | eta: 3s | 1% bad warmup ============================== 800/1000 | eta: 2s | 1% bad warmup ================================ 850/1000 | eta: 2s | 1% bad warmup ================================== 900/1000 | eta: 1s | 1% bad warmup ==================================== 950/1000 | eta: 1s | &lt;1% bad warmup ====================================== 1000/1000 | eta: 0s | &lt;1% bad ## sampling 0/1000 | eta: ?s sampling == 50/1000 | eta: 5s sampling ==== 100/1000 | eta: 6s sampling ====== 150/1000 | eta: 5s sampling ======== 200/1000 | eta: 6s sampling ========== 250/1000 | eta: 5s sampling =========== 300/1000 | eta: 5s sampling ============= 350/1000 | eta: 4s sampling =============== 400/1000 | eta: 4s sampling ================= 450/1000 | eta: 4s sampling =================== 500/1000 | eta: 3s sampling ===================== 550/1000 | eta: 3s sampling ======================= 600/1000 | eta: 3s sampling ========================= 650/1000 | eta: 2s sampling =========================== 700/1000 | eta: 2s sampling ============================ 750/1000 | eta: 2s sampling ============================== 800/1000 | eta: 1s sampling ================================ 850/1000 | eta: 1s sampling ================================== 900/1000 | eta: 1s sampling ==================================== 950/1000 | eta: 0s sampling ====================================== 1000/1000 | eta: 0s Et les résultats. mcmc_combo(draws, combo = c(&quot;dens&quot;, &quot;trace&quot;), pars = c(&quot;int&quot;, &quot;coef&quot;, &quot;sd&quot;)) draws_tidy &lt;- draws %&gt;% spread_draws(int, coef, sd) draws_tidy ## # A tibble: 4,000 x 6 ## .chain .iteration .draw int coef sd ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 105. -28.8 2.32 ## 2 1 2 2 105. -26.9 2.65 ## 3 1 3 3 106. -27.9 2.43 ## 4 1 4 4 105. -27.0 2.79 ## 5 1 5 5 106. -28.8 2.97 ## 6 1 6 6 105. -27.8 2.43 ## 7 1 7 7 105. -28.2 2.45 ## 8 1 8 8 105. -27.6 2.49 ## 9 1 9 9 105. -28.2 2.62 ## 10 1 10 10 105. -27.9 3.03 ## # ... with 3,990 more rows print(&quot;Intercept:&quot;) ## [1] &quot;Intercept:&quot; confidence_interval(x = draws_tidy$int, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) ## ll mean ul ## 104.0278 105.0478 106.0678 print(&quot;Pente:&quot;) ## [1] &quot;Pente:&quot; confidence_interval(x = draws_tidy$coef, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) ## ll mean ul ## -29.38090 -27.77528 -26.16965 6.7 Pour aller plus loin Le module greta est conçu et maintenu par Nick Golding, du Quantitative &amp; Applied Ecology Group de l’University of Melbourne, Australie. La documentation de greta offre des recettes pour toutes sortes d’analyses en écologie. Les livres de Mark Kéry, bien que rédigés pour les calculs en langage R et WinBUGS, offre une approche bien structurée et traduisible en greta, qui est plus moderne que WinBUGS. Introduction to WinBUGS for Ecologists (2010) Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective (2011) Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS (2015) "],
["chapitre-preprocess.html", "7 Prétraitement des données 7.1 Transformation 7.2 Standardisation 7.3 À l’échelle de la plage 7.4 Box-Cox 7.5 Encodage catégoriel 7.6 Analyse compositionnelle", " 7 Prétraitement des données 7.1 Transformation 7.2 Standardisation 7.3 À l’échelle de la plage 7.4 Box-Cox 7.5 Encodage catégoriel 7.6 Analyse compositionnelle Transforamtion de données Encodage catégoriel Standardisation Logarithme Log-ratios et analyse compositionnelle "],
["chapitre-ordination.html", "8 Association, partitionnement et ordination 8.1 Espaces d’analyse 8.2 Analyse d’association", " 8 Association, partitionnement et ordination library(&quot;tidyverse&quot;) 8.1 Espaces d’analyse 8.1.1 Abondance et occurence L’abondance est le décompte d’espèces observées, tandis que l’occurence est la présence ou l’absence d’une espèce. Le tableau suivant contient des données d’abondance. abundance &lt;- data_frame(&#39;Bruant familier&#39; = c(1, 0, 0, 3), &#39;Citelle à poitrine rousse&#39; = c(1, 0, 0, 0), &#39;Colibri à gorge rubis&#39; = c(0, 1, 0, 0), &#39;Geai bleu&#39; = c(3, 2, 0, 0), &#39;Bruant chanteur&#39; = c(1, 0, 5, 2), &#39;Chardonneret&#39; = c(0, 9, 6, 0), &#39;Bruant à gorge blanche&#39; = c(1, 0, 0, 0), &#39;Mésange à tête noire&#39; = c(20, 1, 1, 0), &#39;Jaseur boréal&#39; = c(66, 0, 0, 0)) Ce tableau peut être rapidement transformé en données d’occurence, qui ne comprennent que l’information booléenne de présence (noté 1) et d’absence (noté 0). occurence &lt;- abundance %&gt;% transmute_all(funs(if_else(. &gt; 0, 1, 0))) occurence ## # A tibble: 4 x 9 ## `Bruant familie… `Citelle à poit… `Colibri à gorg… `Geai bleu` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 1 ## 2 0 0 1 1 ## 3 0 0 0 0 ## 4 1 0 0 0 ## # ... with 5 more variables: `Bruant chanteur` &lt;dbl&gt;, Chardonneret &lt;dbl&gt;, ## # `Bruant à gorge blanche` &lt;dbl&gt;, `Mésange à tête noire` &lt;dbl&gt;, `Jaseur ## # boréal` &lt;dbl&gt; L’espace des espèces (ou des variables ou descripteurs) est celui où les espèces forment les axes et où les sites sont positionnés dans cet espace. Il s’agit d’une perspective en mode R, qui permet principalement d’identifier quels espèces se retrouvent plus courrament ensemble. library(&quot;scatterplot3d&quot;) species &lt;- c(&quot;Bruant chanteur&quot;, &quot;Chardonneret&quot;, &quot;Mésange à tête noire&quot;) x &lt;- abundance %&gt;% pull(species[1]) y &lt;- abundance %&gt;% pull(species[2]) z &lt;- abundance %&gt;% pull(species[3]) scatterplot3d(x, y, z, angle = 20, asp = 0.3, xlab = species[1], ylab = species[2], zlab = species[3]) Dans l’espace des sites (ou les échantillons ou objets), on transpose la matrice d’abondance. On passe ici en mode Q, où chaque point est une espèce, et où l’on peut observer quels échantillons sont similaires. site1 &lt;- t(abundance)[, 1] site2 &lt;- t(abundance)[, 2] site3 &lt;- t(abundance)[, 3] scatterplot3d(site1, site2, site3, angle = 20, asp = 10, xlab = &quot;Site 1&quot;, ylab = &quot;Site 2&quot;, zlab = &quot;Site 3&quot;) 8.1.2 Environnement L’espace de l’environnement comprend souvent un autre tableau contenant l’information sur l’environnement où se trouve les espèces: les coordonnées et l’élévation, la pente, le pH du sol, la pluviométrie, etc. 8.2 Analyse d’association Nous utiliserons le terme association come une mesure pour quantifier la ressemblance ou la différence entre deux objets (échantillons) ou variables (descripteurs). Alors que la corrélation et la covariance sont des mesures d’association entre des variables (analyse en mode R), la similarité et la distance sont deux types de une mesure d’association entre des objets (analyse en mode Q). Une distance de 0 est mesuré chez deux objets identiques. La distance augmente au fur et à mesure que les objets sont dissociés. Une similarité ayant une valeur de 0 indique aucune association, tandis qu’une valeur de 1 indique une association parfaite. À l’opposé, la dissimilarité est égale à 1-similarité. La distance peut être liée à la similarité par la relation: \\[distance=\\sqrt{1-similarité}\\] ou \\[distance=\\sqrt{dissimilarité}\\] La racine carrée permet, pour certains indices de similarité, d’obtenir des propriétés euclédiennes. Pour plus de détails, voyez le tableau 7.2 de Legendre et Legendre (2012). Les matrices d’association sont généralement présentées comme des matrices carrées, dont les dimensions sont égales au nombre d’objets (mode Q) ou de vrariables (mode R) dans le tableau. Chaque élément (“cellule”) de la matrice est un indice d’association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarité) ou unitaire (similarité), car elle correspond à l’association entre un objet et lui-même. Puisque l’association entre A et B est la même qu’entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d’exprimer une matrice d’association en mode “compact”, sous forme de vecteur. Le vecteur d’association entre des objets A, B et C contiendra toute l’information nécessaire en un vecteur de trois chiffres, [AB, AC, BC], plutôt qu’une matrice de dimension \\(3 \\times 3\\). L’impact sur la mémoire vive peut être considérable pour les calculs comprenant de nombreuses dimensions. En R, les calculs de similarité et de distances peuvent être effectués avec le module vegan. La fonction vegdist permet de calculer les indices d’association en forme carrée. Nous verons plus tard les méthodes de mesure de similarité et de distance plus loin. Pour l’instant, utilisons la méthode de Jaccard pour une démonstration sur des données d’occurence. library(&quot;vegan&quot;) vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) ## 1 2 3 4 ## 1 0.0000000 0.7777778 0.7500000 0.7142857 ## 2 0.7777778 0.0000000 0.6000000 1.0000000 ## 3 0.7500000 0.6000000 0.0000000 0.7500000 ## 4 0.7142857 1.0000000 0.7500000 0.0000000 Remarquez que vegdist retourne une matrice dont la diagonale est de 0 (on l’affiche en spécifiant diag = TRUE). La diagonale est l’association d’un objet avec lui-même. Or la similarité d’un objet avec lui-même devrait être de 1! En fait, par convention vegdist retourne des dissimilarités, non pas des similarités. La matrice de distance serait donc calculée en extrayant la racine carrée des éléments de la matrice de dissimilarité: dissimilarity &lt;- vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) distance &lt;- sqrt(dissimilarity) distance ## 1 2 3 4 ## 1 0.0000000 0.8819171 0.8660254 0.8451543 ## 2 0.8819171 0.0000000 0.7745967 1.0000000 ## 3 0.8660254 0.7745967 0.0000000 0.8660254 ## 4 0.8451543 1.0000000 0.8660254 0.0000000 Dans le chapitre sur l’analyse compositionnelle, nous avons abordé les significations différentes que peuvent prendre le zéro. L’information fournie par un zéro peut être différente selon les circonstances. Dans le cas d’une variable continue, un zéro signifie généralement une mesure sous le seuil de détection. Deux tissus dont la concentration en cuivre est nulle ont une afinité sous la perspective de la concentration en cuivre. Dans le cas de mesures d’abondance (décompte) ou d’occurence (présence-absence), on pourra décrire comme similaires deux niches écologiques où l’on retrouve une espèce en particulier. Mais deux sites où l’on de retouve pas d’ours polaires ne correspondent pas nécessairement à des niches similaires! En effet, il peut exister de nombreuses raisons écologiques et méthodologiques pour lesquelles l’espèces ou les espèces n’ont pas été observées. C’est le problème des double-zéros (espèces non observées à deux sites), problème qui est amplifié avec les grilles comprenant des espèces rares. La ressemblance entre des objets comprenant des données continues devrait être calculée grâce à des indicateurs symétriques. Inversement, les affinités entre les objets décrits par des données d’abondance ou d’occurence susceptibles de générer des problèmes de double-zéros devraient être évaluées grâce à des indicateurs asymétriques. Un défi supplémentaire arrive lorsque les données sont de type mixte. Nous utiliserons la convention de scipy et nous calculerons la dissimilarité, non pas la similarité. Les mesures de dissimilarité sont calculées sur des données d’abondance ou des données d’occurence. Notons qu’il existe beaucoup de confusion dans la littérature sur la manière de nommer les dissimilarités (ce qui n’est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarité avec un \\(d\\) minuscule et la distance avec un \\(D\\) majuscule. 8.2.1 Association entre objets (mode Q) 8.2.1.1 Objets: Abondance La dissimilarité de Bray-Curtis est asymétrique. Elle est aussi appelée l’indice de Steinhaus, de Czekanowski ou de Sørensen. Il est important de s’assurer de bien s’entendre la méthode à laquelle on fait référence. L’équation enlève toute ambiguité. La dissimilarité de Bray-Curtis entre les points A et B est calculée comme suit. \\[d_{AB} = \\frac {\\sum \\left| A_{i} - B_{i} \\right| }{\\sum \\left(A_{i}+B_{i}\\right)}\\] Utilisons vegdist pour générer les matrices d’association. Le format “liste” de R est pratique pour enregistrer la collection d’objets, dont les matrice d’association que nous allons créer dans cette section. associations_abund &lt;- list() associations_abund[[&#39;BrayCurtis&#39;]] &lt;- vegdist(abundance, method = &quot;bray&quot;) associations_abund[[&#39;BrayCurtis&#39;]] ## 1 2 3 ## 2 0.9433962 ## 3 0.9619048 0.4400000 ## 4 0.9591837 1.0000000 0.7647059 La dissimilarité de Bray-Curtis est souvent utilisée dans la littérature. Toutefois, la version originale de Bray-Curtis n’est pas tout à fait métrique (semimétrique). Conséquemment, la dissimilarité de Ruzicka (une variante de la dissimilarité de Jaccard pour les données d’abondance) est métrique, et devrait probablement être préféré à Bary-Curtis (Oksanen, 2006). \\[d_{AB, Ruzicka} = \\frac { 2 \\times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}\\] associations_abund[[&#39;Ruzicka&#39;]] &lt;- associations_abund[[&#39;BrayCurtis&#39;]] * 2 / (1 + associations_abund[[&#39;BrayCurtis&#39;]]) La dissimilarité de Kulczynski (aussi écrit Kulsinski) est asymétrique et semimétrique, tout comme celle de Bray-Curtis. Elle est calculée comme suit. \\[d_{AB} = 1-\\frac{1}{2} \\times \\left[ \\frac{\\sum min(A_i, B_i)}{\\sum A_i} + \\frac{\\sum min(A_i, B_i)}{\\sum B_i} \\right]\\] associations_abund[[&#39;Kulczynski&#39;]] &lt;- vegdist(abundance, method = &quot;kulczynski&quot;) Une approche commune pour mesurer l’association entre sites décrits par des données d’abondance est la distance de Hellinger. Notez qu’il s’agit ici d’une distance, non pas d’une dissimilarité. Pour l’obtenir, on doit d’abord diviser chaque donnée d’abondance par l’abondance totale pour chaque site pour obtenir les espèces en tant que proportions, puis on extrait la racine carrée de chaque élément. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la généralisation en plusieurs dimensions du théorème de Pythagore, \\(c = \\sqrt{a^2 + b^2}\\). \\[D_{AB} = \\sqrt {\\sum \\left( \\frac{A_i}{\\sum A_i} - \\frac{B_i}{\\sum B_i} \\right)^2}\\] 😱 Attention La distance d’Hellinger hérite des biais liées aux données compositionnelles. Elle peut être substitiée par une matrice de distances d’Aitchison. associations_abund[[&#39;Hellinger&#39;]] &lt;- dist(decostand(abundance, method=&quot;hellinger&quot;)) Toute comme la distance d’Hellinger, la distance de chord est calculée par une distance euclidienne sur des données d’abondance transformées de sorte que chaque ligne ait une longueur (norme) de 1. associations_abund[[&#39;Chord&#39;]] &lt;- dist(decostand(abundance, method=&quot;normalize&quot;)) La métrique du chi-carré, ou \\(\\chi\\)-carré, ou chi-square, donne davantage de poids aux espèces rares qu’aux espèces communes. Son utilisation est recommandée lorsque les espèces rares sont de bons indicateurs de conditions écologiques particulières (Legendre et Legendre, 2012, p. 308). \\[ d_{AB} = \\sqrt{\\sum _j \\frac{1}{\\sum y_j} \\left( \\frac{A_j}{\\sum A} - \\frac{B_j}{\\sum B} \\right)^2 } \\] La métrique peut être transformée en distance en la multipliant par la racine carrée de la somme totale des espèces dans la matric d’abondance (\\(X\\)). \\[ D_{AB} = \\sqrt{\\sum X} \\times d_{AB} \\] associations_abund[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(abundance, method=&quot;chi.square&quot;)) Une mannière visuellement plus intéressante de présenter une matrice d’association est un graphique de type heatmap. library(&quot;pheatmap&quot;) par(mfrow = c(2, 3)) for (i in seq_along(associations_abund)) { pheatmap(associations_abund[[i]], cluster_rows = FALSE, cluster_cols = FALSE, legend = TRUE, display_numbers = TRUE) } "],
["chapitre-outliers.html", "9 Détection de valeurs aberrantes et imputation", " 9 Détection de valeurs aberrantes et imputation détection univariée et multivariée imputation par la moyenne, projection linéaire et autoapprentissage "],
["chapitre-temps.html", "10 Les séries temporelles", " 10 Les séries temporelles À développer "],
["chapitre-git.html", "11 Science ouverte et suivi de version", " 11 Science ouverte et suivi de version "],
["chapitre-ml.html", "12 Autoapprentissage 12.1 Objectifs 12.2 Lexique 12.3 Démarche 12.4 Algorithmes 12.5 L’autoapprentissage en R 12.6 Les k plus proches voisins 12.7 Les arbres décisionnels 12.8 Les réseaux neuronaux 12.9 Les processus gaussiens", " 12 Autoapprentissage Plusieurs cas d’espèces en sciences et génies peuvent être approchés en liant un variable avec une ou plusieurs autres à l’aide de régressions linéaires, polynomiales, sinusoïdales, exponentielle, sigmoïdales, etc. Encore faut-il s’assurer que ces formes préétablies représentent le phénomène de manière fiable. Lorsque la forme de la réponse est difficile à envisager, en particulier dans des cas non-linéaires ou impliquant plusieurs variables, on pourra faire appel à des modèles dont la structure n’est pas contrôlée par une équation rigide gouvernée par des paramètres (comme la pente ou l’intercept). L’autoapprentissage, apprentissage automatique, ou machine learning, vise à détecter des structures complexes émergeant d’ensembles de données à l’aide des mathématiques et de processus automatisés afin de prédire l’émergence de futures occurrences. Comme ensemble de techniques empiriques, l’autoapprentissage est un cas particulier de l’intelligence artificielle, qui elle inclut aussi les mécanismes déterministes et des ensembles d’opérations logiques. Par exemple, les premiers ordinateurs à compétitionner aux échecs se basaient sur des règles de logique (si la reine noire est positionnée en c3 et qu’un le fou blanc est en position f6 et que … alors bouge la tour en g5 - j’écris n’importe quoi). Il s’agissait d’intelligence artificielle, mais pas d’autoapprentissage. L’autoapprentissage passera davantage par la simulation de nombreuses parties et dégagera la structure optimale pour l’emporter considérant les positions des pièces sur l’échiquier. 12.1 Objectifs Comprendre les applications possibles de l’autoapprentissage Comprendre le flux de travail d’une opération d’autoapprentissage Comprendre les principes soutenant les techniques des k plus proches voisins, des arbres décisionnels, des réseaux neuronaux et des processus gaussiens. Plus spécifiquement, vous devrez à la fin de cette section être en mesure de prédire une variable catégorie ou numérique à partir de données observées. 12.2 Lexique L’autoapprentissage possède son jargon particulier. Puisque certains termes peuvent porter à confusion, voici quelques définitions de termes que j’utiliserai dans ce chapitre. Réponse. La variable que l’on cherche à obtenir. Il peut s’agir d’une variable continue comme d’une variable catégorielle. On la nomme aussi la cible. Prédicteur. Une variable utilisée pour prédire une réponse. Les prédicteurs sont des variables continues. Les prédicteurs de type catégoriel doivent préalablement être dummifiés (voir chapitre 5). On nomme les prédicteurs les entrées. Apprentissage supervisé et non-supervisé. Si vous avez suivi le cours jusqu’ici, vous avez déjà utilisé des outils entrant dans la grande famille de l’apprentissage automatique. La régression linéaire, par exemple, vise à minimiser l’erreur sur la réponse en optimisant les coefficients de pente et l’intercept. Un apprentissage supervisé a une cible, comme c’est le cas de la régression linéaire. En revanche, un apprentissage non supervisé n’en a pas: on laisse l’algorithme le soin de détecter des structures intéressantes. Nous avons déjà utilisé cette approche. Pensez-y un peu… l’analyse en composante principale ou en coordonnées principales, ainsi que le partitionnement hiérarchique ou non sont des exemples d’apprentissage non supervisé. En revanche, l’analyse de redondance a une réponse. L’analyse discriminante aussi, bien que sa réponse soit catégorielle. L’apprentissage non supervisé ayant déjà été couvert au chapitre 7, ce chapitre ne s’intéresse qu’à l’apprentissage supervisé. Régression et Classification. Alors que la régression est un type d’apprentissage automatique pour les réponses continues, la classification vise à prédire une réponse catégorielle. Il existe des algorithmes uniquement application à la régression, uniquement applicables à la classification, et plusieurs autres adaptable aux deux situations. Données d’entraînement et données de test. Lorsque l’on génère un modèle, on désire qu’il sache comment réagir à ses prédicteurs. Cela se fait avec des données d’entraînement, sur lesquelles on calibre et valide le modèle. Les données de test servent à vérifier si le modèle est en mesure de prédire des réponses sur lesquelles il n’a pas été entraîné. Fonction de perte. Une fonction qui mesure l’erreur d’un modèle. 12.3 Démarche La première tâche est d’explorer les données, ce que nous avons couvert au chapitres 3 et 4. 12.3.1 Prétraitement Pour la plupart des techniques d’autoapprentissage, le choix de l’échelle de mesure est déterminant sur la modélisation subséquente. Par exemple, un algorithme basé sur la distance comme les k plus proches voisins ne mesurera pas les mêmes distances entre deux observations si l’on change l’unité de mesure d’une variable du mètre au kilomètre. Il est donc important d’effectuer, ou d’envisager la possibilité d’effectuer un prétraitement sur les données. Je vous réfère au chapitre 6 (en développement) pour plus de détails sur le prétraitement. 12.3.2 Entraînement et test Vous connaissez peut-être l’expression sportive “avoir l’avantage du terrain”. Il s’agit d’un principe prétendant que les athlètes performent mieux en terrain connu. Idem pour les modèles phénoménologiques. Il est possible qu’un modèle fonctionne très bien sur les données avec lesquelles il a été entraîné, mais très mal sur des données externes. De mauvaises prédictions effectuées à partir d’un modèle qui semblait bien se comporter peut mener à des décisions qui, pourtant prises de manière confiante, se révèlent fallacieuses au point d’aboutir à de graves conséquences. C’est pourquoi, en mode prédictif, on doit évaluer la précision et la justesse d’un modèle sur des données qui n’ont pas été utilisés dans son entraînement. En pratique, il convient de séparer un tableau de données en deux: un tableau d’entraînement et un tableau de test. Il n’existe pas de standards sur le ratio à utiliser. Cela dépend de la prudence de l’analyse et de l’ampleur de son tableau de données. Certaines personnes préférerons couper le tableau à 50%. D’autres préférerons réserver le deux-tiers des données pour l’entraînement, ou 70%, 75%. Rarement, réservera-t-on moins plus de 50% et moins de 20% à la phase de test. Si les données sont peu équilibrées (par exemple, on retrouve peu de données de l’espèce \\(A\\), que l’on retrouve peu de données à un pH inférieur à 5 ou que l’on a peu de données croisées de l’espèce \\(A\\) à ph inférieur à 5), il y a un danger qu’une trop grande part, voire toute les données, se retrouvent dans le tableau d’entraînement (certaines situations ne seront ainsi pas testées) ou dans le tableau de test (certaines situations ne seront pas couvertes par le modèle). L’analyste doit s’assurer de séparer le tableau au hasard, mais de manière consciencieuse. 12.3.3 Sousapprentissage et surapprentissage Une difficulté en modélisation phénoménologique est ce qui tient de la structure et ce qui tient du bruit. Lorsque l’on considère une structure comme du bruit, on est dans un cas de sousapprentissage. Lorsque, au contraire, on interprète du bruit comme une structure, on est en cas de surapprentissage. Les graphiques suivant présentent ces deux cas, avec au centre un cas d’apprentissage conforme. set.seed(35473) n &lt;- 50 x &lt;- seq(0, 20, length = n) y &lt;- 500 + 0.4 * (x-10)^3 + rnorm(n, mean=10, sd=80) # le bruit est généré par rnorm() par(mfrow = c(1, 3)) plot(x, y, main = &quot;Sousapprentissage&quot;, col = &quot;#46c19a&quot;, pch=16) lines(x, predict(lm(y~x)), col = &quot;#b94a73&quot;) plot(x, y, main = &quot;Apprentissage conforme&quot;, col = &quot;#46c19a&quot;, pch=16) lines(x, predict(lm(y~x + I(x^2) + I(x^3))), col = &quot;#b94a73&quot;) plot(x, y, main = &quot;Surapprentissage&quot;, col = &quot;#46c19a&quot;, pch=16) lines(x, predict(lm(y~x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14) + I(x^15) + I(x^16))), col = &quot;#b94a73&quot;) Afin d’éviter les cas de mésapprentissage on peut avoir recours à la validation croisée. 12.3.4 Validation croisée Souvent confondue avec le fait de séparer le tableau en phases d’entraînement et de test, la validation croisée est un principe incluant plusieurs algorithmes qui consiste à entraîner le modèle sur un échantillonnage aléatoire des données d’entraînement. La technique la plus utilisée est le k-fold, où l’on sépare aléatoirement le tableau d’entraînement en un nombre k de tableaux. À chaque étape de la validation croisée, on calibre le modèle sur tous les tableaux sauf un, puis on valide le modèle sur le tableau exclu. La performance du modèle en entraînement est jugée sur les validations. 12.3.5 Choix de l’algorithme d’apprentissage Face aux centaines d’algorithmes d’apprentissages qui vous sont offertes, choisir l’algorithme ou les algorithmes adéquats pour vos données n’est pas facile. Ce choix sera motivé par les tenants et aboutissants des algorithmes, votre expérience, l’expérience de la littérature, l’expérience de vos collègues. Une approche raisonnable est de tester plusieurs modèles et d’approfondir si ce n’est déjà fait la mathématique des options retenues. Il existe des algorithmes génétiques, qui ne sont pas couverts ici, permettent de sélectionner des modèles d’autoapprentissages optimaux. Un de ces algorithmes est offert par le module Python tpot. 12.3.6 Déploiement RData, Shiny En résumé, Explorer les données Sélectionner des algorithmes Effectuer un prétraitement Créer un ensemble d’entraînement et un ensemble de test Lisser les données sur les données d’entraînement avec validation croisée Tester le modèle Déployer le modèle 12.4 Algorithmes Il existe des centaines d’algorithmes d’apprentissage. Je n’en couvrirai que quatre, qui me semblent être appropriés pour la modélisation phénoménologique des systèmes vivants, et utilisables pour la régression et la classification. Les k plus proches voisins Les arbres de décision Les réseaux neuronaux Les processus gaussiens 12.5 L’autoapprentissage en R Plusieurs options sont disponibles. Les modules que l’on retrouve en R pour l’autoapprentissage sont nombreux, et parfois spécialisés. Il est possible de les utiliser individuellement. Chacun de ces modules fonctionne à sa façon. Le module caret de R a été conçu pour donner accès à des centaines de fonctions d’autoapprentissage via une interface commune. Le module mlr occupe sensiblement le même créneau que caret, mais utilise plutôt une approche par objets connectés. Au moment d’écrire ces lignes, mlr est peu documenté, donc a priori plus complexe à prendre en main. En Python, le module scikit-learn offre un interface unique pour l’utilisation de nombreuses techniques d’autoapprentissage. Il est possible d’appeler des fonctions de Python à partir de R grâce au module reticulate. Dans ce chapitre, nous verrons comment fonctionnent certains algorithmes sélectionnés, puis nous les appliquerons avec le module respectif qui m’a semblé le plus approprié. Vous remarquerez néanmoins des références récurrentes aux modules de Python. En ce moment, la force de R réside dans la gestion des tableaux, les tests statistiques, l’exploration heuristique et la visualisation de données. Néanmoins, Python le surpasse pour l’autoapprentissage… library(&quot;tidyverse&quot;) # évidemment library(&quot;caret&quot;) ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:vegan&#39;: ## ## tolerance ## The following object is masked from &#39;package:purrr&#39;: ## ## lift 12.6 Les k plus proches voisins “Le… l’idée en arrière pour être… euh… simpliste, là c’est que c’est un peu de… euhmm… de la vitamine de vinyle.” - Georges (Les voisins, une pièce de Claude Meunier) Pour dire comme Georges, le… l’idée en arrière des KNN pour être… euh… simpliste, c’est qu’un objet va ressembler à ce qui se trouve dans son voisinage. Les KNN se basent en effet sur une métrique de distance pour rechercher un nombre k de points situés à proximité de la mesure. Les k points les plus proches sont retenus, k étant un entier non nul à optimiser. Un autre paramètre parfois utilisé est la distance maximale des voisins à considérer: un voisin trop éloigné pourra être discarté. La réponse attribuée à la mesure est calculée à partir de la réponse des k voisins retenus. Dans le cas d’une régression, on utiliser généralement la moyenne. Dans le cas de la classification, la mesure prendra la catégorie qui sera la plus présente chez les k plus proches voisins. L’algorithme des k plus proches voisins est relativement simple à comprendre. Certains pièges sont, de même, peuvent être contournés facilement. Imaginez que vous rechercher les points les plus rapprochés dans un système de coordonnées géographiques où les coordonnées \\(x\\) sont exprimées en mètres et les coordonnées \\(y\\), en centimètres. Vous y projetez trois points. data &lt;- data.frame(X = c(0, 1, 0), Y = c(0, 0, 1), row.names = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;)) options(repr.plot.width = 4, repr.plot.height = 4) par(pty=&quot;s&quot;) plot(data, cex=3, xlab = &#39;Position X (m)&#39;, ylab = &#39;Position Y (cm)&#39;) text(data, labels = rownames(data)) Techniquement la distance A-B est 100 plus élevée que la distance A-C, mais l’algorithme ne se soucie pas de la métrique que vous utilisez. Il est primordial dans ce cas d’utiliser la même métrique. Cette stratégie est évidente lorsque les variables sont comparables. C’est rarement le cas, que ce soit lorsque l’on compare des dimensions physionomiques (la longueur d’une phalange ou celle d’un fémur) mais lorsque les variables incluent des mélanges de longueurs, des pH, des décomptes, etc., il est important de bien identifier la métrique et le type de distance qu’il convient le mieux d’utiliser. En outre, la standardisation des données à une moyenne de zéro et à un écart-type de 1 est une approche courrament utilisée. 12.6.1 Exemple d’application Pour ce premier exemple, je présenterai un cheminement d’autoapprentissage, du prétraitement au test. # ionome 12.7 Les arbres décisionnels Les Ents, tiré du film le Seigneur des anneaux Un arbre décisionnel est une collection hiérarchisée de décisions, le plus souvent binaires. Chaque embranchement est un test à vrai ou faux sur une variable. La réponse, que ce soit une catégorie ou une valeur numérique, se trouve au bout de la dernière branche. Les suites de décisions sont organisées de manière à ce que la précision de la réponse soit optimisée. Par exemple, … 12.8 Les réseaux neuronaux Après les KNN et les random forests, nous passons au domaine plus complexe des réseaux neuronaux. Le terme réseau neuronal est une métaphore liée à une perception que l’on avait du fonctionnement du cerveau humain lorsque la technique des réseaux neuronaux a été développée dans les années 1950. Un réseau neuronal comprend une série de boîtes d’entrées liée à des fonctions qui transforment et acheminent successivement l’information jusqu’à la sortie d’une ou plusieurs réponse. Il existe plusieurs formes de réseaux neuronnaux, dont la plus simple manifestation est le perceptron multicouche. Dans l’exemple suivant, on retrouve 4 variables d’entrée et trois variables de sortie entre lesquelles on retrouve 5 couches dont le nombre de neurones varient entre 3 et 6. Source: Neural designer Entre la première couche de neurones (les variables prédictives) et la dernière couche (les variables réponse), on retrouve des couches cachées. Chaque neurone est relié à tous les neurones de la couche suivante. Les liens sont des poids, qui peuvent prendre des valeurs dans l’ensemble des nombres réels. À chaque neurone suivant la première couche, on fait la somme des poids multipliés par la sortie du neurone. Le nombre obtenu entre dans chaque neurone de la couche. Le neurone est une fonction, souvent très simple, qui transforme le nombre. La fonction plus utilisée est probablement la fonction ReLU, pour rectified linear unit, qui expulse le même nombre aux neurones de la prochaine couche s’il est positif: sinon, il expulse un zéro. Exercice. Si tous les neurones sont des fonctions ReLU, calculez la sortie de ce petit réseau neuronal. Vous trouverez la réponse sur l’image images/11_nn_ex1_R.jpg. Il est aussi possible d’ajouter un biais à chaque neurone, qui est un nombre réel additionné à la somme des neurones pondérée par les poids. L’optimisation les poids pour chaque lien et les biais pour chaque neurone (grâce à des algorithmes dont le fonctionnement sort du cadre de ce cours) constitue le processus d’apprentissage. Avec l’aide de logiciels et de modules spécialisés, la construction de réseaux de centaines de neurones organisés en centaines de couches vous permettra de capter des patrons complexes dans des ensembles de données. Vous avez peut-être déjà entendu parler d’apprentissage profond (ou deep learning). Il s’agit simplement d’une appellation des réseaux neuronaux modernisé pour insister sur la présence de plusieurs couches de neurones. C’est un terme à la mode. 12.8.1 Les réseaux neuronaux sur R avec Keras Plusieurs modules sont disponibles sur R pour l’apprentissage profond. Certains utilisent le module H2O.ia, propulsé en Java, d’autres utilisent plutôt Keras, propulsé en Python par l’intermédiaire de tensorflow. J’ai une préférence pour Keras, puisqu’il supporte les réseaux neuronaux classiques (perceptrons multicouche) autant que convolutifs ou récurrents. Keras pourrait néanmoins être difficile à installer sur Windows, où Python ne vient pas par défaut. Sur Windows, Keras ne fonctionne qu’avec Anaconda: vous devez donc installez Anaconda ou Miniconda (Miniconda offre une installation minimaliste). Pour installer Keras, il suffit d’installer le module (avec devtools pour obtenir la version la plus récente, i.e. devtools::install_github(\"rstudio/keras\")), puis de lancer la fonction install_keras(). Dans mon cas, j’utilise conda. library(&quot;keras&quot;) #install_keras(method = &quot;conda&quot;) La fonction install_keras() installera keras dans un environnement virtuel nommé r-tensorflow. Si vous utilisez conda, vous y accéderez avec cette commande. use_condaenv(&quot;r-tensorflow&quot;) Chargeons les données. abalone &lt;- read_csv(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data&quot;, col_names = c(&#39;sex&#39;, &#39;length&#39;, &#39;diameter&#39;, &#39;height&#39;, &#39;whole.weight&#39;, &#39;shucked.weight&#39;, &#39;viscera.weight&#39;, &#39;shell.weight&#39;, &#39;rings&#39;)) ## Parsed with column specification: ## cols( ## sex = col_character(), ## length = col_double(), ## diameter = col_double(), ## height = col_double(), ## whole.weight = col_double(), ## shucked.weight = col_double(), ## viscera.weight = col_double(), ## shell.weight = col_double(), ## rings = col_double() ## ) Prenons soin de segmenter nos données en entraînement et en test. set.seed(8453668) abal_tr_index &lt;- createDataPartition(y=abalone$sex, p = 0.75, list = FALSE) Nous pouvons ainsi créer nos tableaux d’entraînement et de test pour les variables prédictives. x &lt;- abalone %&gt;% select(-sex) %&gt;% as.matrix() x_tr &lt;- x[abal_tr_index, ] x_te &lt;- x[-abal_tr_index, ] Pour les variables-réponse, keras exige une transformation préalable. Les réseaux neuronnaux sont aptes à générer des sorties multiples. Lors de la prédiction d’une catégorie, nous devons générée des sorties multiples qui permettront de décider de l’appartenance exclusive à une catégorie ou une autre. Nous avons abordé l’encodage catégoriel aux chapitres 5 et 7. C’est exactement ce que nous ferons ici, mais en utilisant les fonctions du module keras. D’abord nous allons transformer les catégories de la colonne sex en nombres entiers. Étant donnée que keras désire que la première catégorie commence à 0 et que la transformation de factor à integer démarre la première catégorie à 1, nous soustrayons 1 de toutes les valeurs. y_num &lt;- as.integer(as.factor(abalone$sex)) - 1 Nous encodons ensuite ces valeurs entières en plusieurs colonnes catégorielles. y &lt;- to_categorical(y_num) head(y) ## [,1] [,2] [,3] ## [1,] 0 0 1 ## [2,] 0 0 1 ## [3,] 1 0 0 ## [4,] 0 0 1 ## [5,] 0 1 0 ## [6,] 0 1 0 Nous pouvons finalement générer nos sorties pour l’entraînement et le test. y_tr &lt;- y[abal_tr_index, ] y_te &lt;- y[-abal_tr_index, ] Le module keras construit des modèles avec l’opérateur pipe %&gt;%. Nous couvrirons ici seulement les modèles séquentiels, le plus communs, qui permettent de construire notre séquence de neuronnes. Le modèle séquentiel est initié de cette manière. keras_model &lt;- keras_model_sequential() Une fois inité, nous lui ajoutons des couches avec layer_dense(), une fonction qui définit principalement le nombre de neuronne de la couche (units =) et la fonction d’activation (activation =). Entre chaque couche, je coupe 20% des synapses (layer_dropout(rate = 0.2)): une technique pour éviter le surlissage. Enfin, la couche de sortie comporte trois neuronnes, un par catégorie, avec l’activation \"softmax\", qui permet de générer une sortie catégorielle. keras_model %&gt;% layer_dense(units = 256, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.2) %&gt;% layer_dense(units = 256 * 2, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.2) %&gt;% layer_dense(units = 256 * 2, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.2) %&gt;% layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) Il n’existe pas de règle stricte sur le nombre de couche et le nombre de noeud par couche. Il est néanmoins conseillé de générer d’abord un modèle simple, puis au besoin de le complexifier graduellement en terme de nombre de noeuds, puis de nombre de couches. Le module autokeras, disponible seulement en Python, est conçu pour optimiser un modèle Keras - je vous laisse le soin d’explorer ses capcités. Le modèle étant maintenant créé, on doit le compiler en spécifiant la fonction de perte (la fonction qui permet de mesurer la performance d’un modèle) ainsi que d’autres option. keras_model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_rmsprop(), metrics = c(&#39;accuracy&#39;) ) Enfin, le modèle est lissé sur les données en paramétrant le solveur. history &lt;- keras_model %&gt;% fit( x_tr, y_tr, epochs = 5, batch_size = 128, validation_split = 0.25 ) plot(history) Les informations du modèle lissé se trouvent dans l’objet du modèle, ici keras_model. Évaluons le modèle sur nos données de test. keras_model %&gt;% evaluate(x_te, y_te) ## $loss ## [1] 0.9286488 ## ## $acc ## [1] 0.5100671 pred_te &lt;- keras_model %&gt;% predict_classes(x_te) pred_te[pred_te == 0] &lt;- &#39;F&#39; pred_te[pred_te == 1] &lt;- &#39;I&#39; pred_te[pred_te == 2] &lt;- &#39;M&#39; pred_te &lt;- factor(pred_te, levels = c(&quot;F&quot;, &quot;I&quot;, &quot;M&quot;)) confusionMatrix(factor(abalone$sex[-abal_tr_index]), pred_te) ## Confusion Matrix and Statistics ## ## Reference ## Prediction F I M ## F 2 49 275 ## I 0 238 97 ## M 6 84 292 ## ## Overall Statistics ## ## Accuracy : 0.5101 ## 95% CI : (0.4793, 0.5408) ## No Information Rate : 0.6366 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.2465 ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Statistics by Class: ## ## Class: F Class: I Class: M ## Sensitivity 0.250000 0.6415 0.4398 ## Specificity 0.686957 0.8557 0.7625 ## Pos Pred Value 0.006135 0.7104 0.7644 ## Neg Pred Value 0.991632 0.8121 0.4372 ## Prevalence 0.007670 0.3557 0.6366 ## Detection Rate 0.001918 0.2282 0.2800 ## Detection Prevalence 0.312560 0.3212 0.3663 ## Balanced Accuracy 0.468478 0.7486 0.6011 12.8.1.1 Pour aller plus loin En une heure divisée en 4 vidéos, Grant Sanderson explique les réseaux neuronaux de manière intuitive. En ce qui a trait à Keras, je recommande le livre Deep learning with R, de François Allaire, auquel vous avez accès avec un IDUL de l’Université Laval. Si vous vous sentez à l’aise à utiliser Keras avec le langage Python, je vous recommande le cours gratuit en ligne Applications of deep neural networks, de Jeff Heaton. Des types de réseaux neuronaux spécialisés ont été développés. Je les présente sans aller dans les détails. Réseaux neuronaux convolutif. Ce type de réseau neuronal est surtout utilisé en reconnaissance d’image. Les couches de neurones convolutifs possèdent, en plus des fonctions des perceptrons classiques, des filtres permettant d’intégrer les variables descriptives connexes à l’observation: dans le cas d’une image, il s’agit de scanner les pixels au pourtour du pixel traité. Une brève introduction sur Youtube. Réseaux neuronaux récurrents. Prédire des occurrences futures à partir de séries temporelles implique que la réponse au temps t dépend non seulement de conditions externes, mais aussi le la réponse au temps t-1. Les réseaux neuronaux récurrents. Vous devrez ajouter des neurones particuliers pour cette tâche, qui pourra être pris en charge par Keras grâce aux couches de type Long Short-Term Memory network, ou LSTM. Réseaux neuronaux probabilistes. Les réseaux neuronaux non-probabilistes offre une estimation de la variable réponse. Mais quelle est la crédibilité de la réponse selon les variables descriptives? Question qui pourrait se révéler cruciale en médecine ou en ingénierie, à la laquelle on pourra répondre en mode probabiliste. Pour ce faire, on pose des distributions a priori sur les poids du réseau neuronal. Le module edward, programmé et distribué en Python, offre cette possibilité. Vous pourrez accéder à edward grâce au module reticulate, mais à ce stade mieux vaudra basculer en Python. Pour en savoir davantage, considérez cette conférence de Andrew Rowan. 12.9 Les processus gaussiens Les sorties des techniques que sont les KNN, les arbres ou les forêts ainsi que les réseaux neuronaux sont (classiquement) des nombres réels ou des catégories. Dans les cas où la crédibilité de la réponse est importante, il devient pertinent que la sortie soit probabiliste: les prédictions seront alors présentées sous forme de distributions de probabilité. Dans le cas d’une classification, la sortie du modèle sera un vecteur de probabilité qu’une observation appartienne à une classe ou à une autre. Dans celui d’une régression, on obtiendra une distribution continue. Les processus gaussiens tirent profit des statistiques bayésiennes pour effectuer des prédictions probabilistes. D’autres techniques peuvent être utilisées pour effectuer des prédictions probabilistes, comme les réseaux neuronaux probabilistes, que j’ai introduits précédemment. Bien que les processus gaussiens peuvent être utilisés pour la classification, son fonctionnement s’explique favorablement, de manière intuitive, pas la régression. 12.9.1 Un approche intuitive Ayant acquis de l’expérience en enseignement des processus gaussiens, John Cunningham a développé une approche intuitive permettant de saisir les mécanismes des processus gaussiens. lors de conférences disponible sur YouTube (1, 2), il aborde le sujet par la nécessité d’effectuer une régression non-linéaire. Générons d’abord une variable prédictive x, l’heure, et une variable réponse y, le rythme cardiaque d’un individu en battements par minute (bpm). x &lt;- c(7, 8, 10, 14, 17) y &lt;- c(61, 74, 69, 67, 78) plot(x, y, xlab=&quot;Heure&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) abline(v=12, lty=3, col=&#39;gray50&#39;);text(12, 67, &#39;?&#39;, cex=2) abline(v=16, lty=3, col=&#39;gray50&#39;);text(16, 72, &#39;?&#39;, cex=2) Poser un problème par un processus gaussien, c’est se demander les valeurs crédibles qui pourraient être obtenues hors du domaine d’observations (par exemple, dans la figure ci-dessus, à x=12 et x=16)? Ou bien, de manière plus générale, quelles fonctions ont pu générer les variables réponse à partir d’une structure dans les variables prédictives? Les distributions normales, que nous appellerons gaussiennes dans cette section par concordance avec le terme processus gaussien, sont particulièrement utiles pour répondre à cette question. Nous avons vu précédemment ce que sont les distributions de probabilité: des outils mathématiques permettant d’appréhender la structure des processus aléatoires. Une distribution gaussienne représente une situation où l’on tire au hasard des valeurs continues. Une distribution gaussienne de la variable aléatoire \\(X\\) de moyenne \\(0\\) et de variance de \\(1\\) est notée ainsi: \\[ X \\sim \\mathcal{N} \\left( 0, 1\\right)\\] Par exemple, une courbe de distribution gaussienne du rythme cardiaque à 7:00 pourrait prendre la forme suivante. \\[ bpm \\sim \\mathcal{N} \\left( 65, 5\\right)\\] En R: x_sequence &lt;- seq(50, 80, length=100) plot(x_sequence, dnorm(x_sequence, mean=65, sd=5), type=&quot;l&quot;, xlab=&quot;Rythme cardiaque (bpm)&quot;, ylab=&quot;Densité&quot;) Une distribution binormale, un cas particulier de la distribution multinormale, comprendra deux vecteurs, \\(x_1\\) et \\(x_2\\). Elle aura donc deux moyennes. Puisqu’il s’agit d’une distribution binormale, et non pas deux distributions normales, les deux variables ne sont pas indépendantes et l’on utilisera une matrice de covariance au lieu de deux variances indépendantes. \\[ \\binom{x_1}{x_2} \\sim \\mathcal{N} \\Bigg( \\binom{\\mu_1}{\\mu_2}, \\left[ {\\begin{array}{cc} \\Sigma_{x_1} &amp; \\Sigma_{x_1,x_2} \\\\ \\Sigma_{x_1,x_2}^T &amp; \\Sigma_{x_2} \\\\ \\end{array} } \\right] \\Bigg) \\] La matrice \\(\\Sigma\\), dite de variance-covariance, indique sur sa diagonale les variances des variables (\\(\\Sigma_{x_1}\\) et \\(\\Sigma_{x_2}\\)). Les covariances \\(\\Sigma_{x_1,x_2}\\) et \\(\\Sigma_{x_1,x_2}^T\\) sont symétriques et indiquent le lien entre les variables. On pourrait supposer que le rythme cardiaque à 8:00 soit corrélé avec celui à 7:00. Mises ensembles, les distributions gaussiennes à 7:00 et à 8:00 formeraient une distribution gaussienne binormale. \\[ \\binom{bpm_7}{bpm_8} \\sim \\mathcal{N} \\Bigg( \\binom{65}{75}, \\left[ {\\begin{array}{cc} 10 &amp; 6 \\\\ 6 &amp; 15 \\\\ \\end{array} } \\right] \\Bigg) \\] En R: library(&quot;ellipse&quot;) ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs means_vec &lt;- c(65, 75) covariance_mat &lt;- matrix(c(10, 6, 6, 15), ncol=2) par(pty=&#39;s&#39;) plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type=&#39;l&#39;, xlab=&quot;Rythme cardiaque à 7:00 (bpm)&quot;, ylab=&quot;Rythme cardiaque à 8:00 (bpm)&quot;) #lines(ellipse(x=covariance_mat, centre=means_vec, level=0.8)) On peut se poser la question: étant donnée que \\(x_1 = 68\\), quelle serait la distribution de \\(x_2\\)? Dans ce cas bivariée, la distribution marginale serait univariée, mais dans le cas multivarié en \\(D\\) dimensions, la distribution marginale où l’on spécifie \\(m\\) variables serait de \\(D-m\\). de Une propriété fondamentale d’une distribution gaussienne est que peu importe l’endroit où l’angle selon lequel on la tranche, la distribution marginale sera aussi gaussienne. Lorsque l’on retranche une ou plusieurs variables en spécifiant la valeur qu’elles prennent, on applique un conditionnement à la distribution. library(&quot;condMVNorm&quot;) ## Loading required package: mvtnorm condition_x1 &lt;- 61 # changer ce chiffre pour visualiser l&#39;effet cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent=2, given=1, X.given=condition_x1) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- sqrt(cond_parameters$condVar) x2_sequence &lt;- seq(50, 90, length=100) x2_dens &lt;- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd) par(pty=&#39;s&#39;) plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type=&#39;l&#39;, xlab=&quot;Rythme cardiaque à 7:00 (bpm)&quot;, ylab=&quot;Rythme cardiaque à 8:00 (bpm)&quot;) abline(v=condition_x1, col=&#39;#f8ad00&#39;, lwd=2, lty=2) lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col=&quot;#f8ad00&quot;, lwd=2) lines(x = c(condition_x1, condition_x1), y = c(cond_mean-cond_sd, cond_mean+cond_sd), lwd=3, col=&#39;#46c19a&#39;) points(condition_x1, cond_mean, col=&#39;#46c19a&#39;, pch=16, cex=2) n_sample &lt;- 20 points(x = rep(condition_x1, n_sample), y = rnorm(n_sample, cond_mean, cond_sd), pch=4, col = rgb(0, 0, 0, 0.5)) Les points sur l’axe (symbole x) conditionnés sont des échantillons tirés au hasard dans la distribution conditionnée. Une autre manière de visualiser la distribution gaussienne binormale est de placer \\(x_1\\) et \\(x_2\\) côte à côte en abscisse, avec leur valeur en ordonnée. Le bloc de code suivant peut sembler lourd au premier coup d’œil: pas de panique, il s’agit surtout d’instructions graphiques. Vous pouvez vous amuser à changer les paramètres de la distribution binormale (section 1) ainsi que la valeur de \\(x_1\\) à laquelle est conditionnée la distribution de \\(x_2\\) (section 2). source(&quot;lib/plot_matrix.R&quot;) # 1. Distribution means_vec &lt;- c(65, 65) covariance_mat &lt;- matrix(c(10, 6, 6, 15), ncol=2) # 2. Condition condition_x1 &lt;- 61 # changer ce chiffre pour visualiser l&#39;effet # 3. Densité conditionnée cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent=2, given=1, X.given=condition_x1) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- sqrt(cond_parameters$condVar) x2_sequence &lt;- seq(50, 90, length=100) x2_dens &lt;- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd) x2_draw &lt;- rnorm(1, cond_mean, cond_sd) # 4. Graphiques options(repr.plot.width = 8, repr.plot.height = 5) layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2)) par(mar=c(4, 4, 1, 1), pty=&#39;s&#39;) ## 4.1 Ellipse plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type=&#39;l&#39;, xlab=&quot;BPM à 7:00&quot;, ylab=&quot;BPM à 8:00&quot;) abline(v=condition_x1, col=&#39;#f8ad00&#39;, lwd=1) lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col=&quot;#f8ad00&quot;, lwd=1) lines(x = c(condition_x1, condition_x1), y = c(cond_mean-cond_sd, cond_mean+cond_sd), lwd=2, col=&#39;#46c19a&#39;) points(condition_x1, cond_mean, col=&#39;#46c19a&#39;, pch=16, cex=1) points(condition_x1, x2_draw, pch=16, col=&quot;#b94a73&quot;) ## 4.2 Covariance plot_matrix(covariance_mat) ## 4.3 Série plot(c(1, 2), c(condition_x1, x2_draw), xlim=c(0, 6), ylim=c(55, 75), type=&#39;l&#39;, xlab=&quot;Indice de la variable&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) points(1, condition_x1, pch=16, col=&#39;#46c19a&#39;, cex=3) points(2, x2_draw, pch=16, col=&#39;#b94a73&#39;, cex=3) Les valeurs que peuvent prendre le rythme cardiaque en \\(x_2\\) sont tirées aléatoirement d’une distribution conditionnée. Sautons maintenant au cas multinormal, incluant 6 variables (hexanormal!). Afin d’éviter de composer une matrice de covariance à la mitaine, je me permets de la générer avec une fonction. Cette fonction particulière est nommée fonction de base radiale ou exponentiel de la racine. \\[K_{RBF} \\left( x_i, x_j \\right) = \\sigma^2 exp \\left( -\\frac{\\left( x_i - x_j \\right)^2}{2 l^2} \\right) \\] RBF_kernel &lt;- function(x, sigma, l) { n &lt;- length(x) k &lt;- matrix(ncol = n, nrow = n) for (i in 1:n) { for (j in 1:n) { k[i, j] = sigma^2 * exp(-1/(2*l^2) * (x[i] - x[j])^2) } } colnames(k) &lt;- paste0(&#39;x&#39;, 1:n) rownames(k) &lt;- colnames(k) return(k) } Dans la fonction RBF_kernel, x désigne les dimensions, sigma désigne un écart-type commun à chacune des dimensions et l est la longueur désignant l’amplification de la covariance entre des dimensions éloignées (dans le sens que la première dimension est éloignée de la dernière). Pour 6 dimensions, avec un écart-type de 4 et une longueur de 2. covariance_6 &lt;- RBF_kernel(1:6, sigma=4, l=2) round(covariance_6, 2) ## x1 x2 x3 x4 x5 x6 ## x1 16.00 14.12 9.70 5.19 2.17 0.70 ## x2 14.12 16.00 14.12 9.70 5.19 2.17 ## x3 9.70 14.12 16.00 14.12 9.70 5.19 ## x4 5.19 9.70 14.12 16.00 14.12 9.70 ## x5 2.17 5.19 9.70 14.12 16.00 14.12 ## x6 0.70 2.17 5.19 9.70 14.12 16.00 Changez la valeur de l permet de bien saisir son influence sur la matrice de covariance. Avec un l de 1, la covariance entre \\(x_1\\) et \\(x_6\\) est pratiquement nulle: elle est un peut plus élevée avec l=2. Pour reprendre l’exemple du rythme cardiaque, on devrait en effet s’attendre à retrouver une plus grande corrélation entre celles mesurées aux temps 4 et 5 qu’entre les temps 1 et 6. De même que dans la situation où nous avions une distribution binormale, nous pouvons conditionner une distribution multinormale. Dans l’exemple suivant, je conditionne la distribution multinormale de 6 dimensions en spécifiant les valeurs prises par les deux premières dimensions. Le résultat du conditionnement est une distribution en 4 dimensions. Puisqu’il est difficile de présenter une distribution en 6D, le graphique en haut à gauche ne comprend que les dimensions 1 et 6. Remarquez que la corrélation entre les dimensions 1 et 6 est faible, en concordance avec la matrice de covariance générée par la fonction RBF_kernel. Lancez plusieurs fois le code et voyez ce qui advient des échantillonnages dans les dimensions 3 à 6 selon le conditionnement en 1 et 2. library(&quot;MASS&quot;) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select # 1. Distribution means_vec &lt;- rep(65, 6) covariance_mat &lt;- covariance_6 # 2. Condition conditions_x &lt;- c(61, 74) # changer ces chiffres pour visualiser l&#39;effet # 3. Densité conditionnée cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent.ind = 3:6, given.ind=1:2, X.given=conditions_x) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- sqrt(cond_parameters$condVar) x6_sequence &lt;- seq(50, 90, length=100) x6_dens &lt;- dnorm(x2_sequence, mean=cond_mean[4], sd=cond_sd[4, 4]) x_3.6_draw &lt;- mvrnorm(n = 1, mu = cond_mean, Sigma = cond_sd^2) # 4. Graphiques layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2)) par(mar=c(4, 4, 1, 1)) ## 4.1 Ellipse plot(ellipse(x=covariance_mat[c(1, 6), c(1, 6)], centre=means_vec[c(1, 6)], levels=0.95), type=&#39;l&#39;, xlab=&quot;BPM à 7:00&quot;, ylab=&quot;BPM à 8:00&quot;) abline(v=conditions_x[1], col=&#39;#f8ad00&#39;, lwd=1) lines(x=condition_x1 + x6_dens*40, y=x2_sequence, col=&quot;#f8ad00&quot;, lwd=1) lines(x = c(conditions_x[1], conditions_x[1]), y = c(cond_mean[4]-cond_sd[4, 4], cond_mean[4]+cond_sd[4, 4]), lwd=2, col=&#39;#46c19a&#39;) points(conditions_x[1], cond_mean[4], col=&#39;#46c19a&#39;, pch=16, cex=1) points(conditions_x[1], x_3.6_draw[4], pch=16, col=&quot;#b94a73&quot;) ## 4.2 Covariance plot_matrix(covariance_mat, cex=0.8) ## 4.3 Série plot(1:6, c(conditions_x, x_3.6_draw), xlim=c(0, 6), ylim=c(60, 85), type=&#39;l&#39;, xlab=&quot;Indice de la variable&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) points(c(1, 2), conditions_x, pch=16, col=&#39;#46c19a&#39;, cex=3) points(3:6, x_3.6_draw, pch=16, col=&#39;#b94a73&#39;, cex=3) La structure de la covariance assure que les dimensions proches prennent des valeurs similaires, assurant une courbe lisse et non en dents de scie. Pourquoi s’arrêter à 6 dimensions? Prenons-en plusieurs, puis générons plus d’un échantillon. Ensuite, utilisons ces simulations pour de calculer la moyenne et l’écart-type de chacune des dimensions. # 1. Distribution n &lt;- 20 means_vec &lt;- rep(65, n) covariance_mat &lt;- RBF_kernel(x = 1:n, sigma = 10, l = 2) # 2. Condition conditions_x &lt;- c(61, 74) # changer ces chiffres pour visualiser l&#39;effet # 3. Densité conditionnée cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent.ind = 3:n, given.ind=1:2, X.given=conditions_x) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- cond_parameters$condVar # 4. Graphiques par(mar=c(4, 4, 1, 1)) ## 4.3 Série plot(0, 0, xlim=c(0, n), ylim=c(40, 95), type=&#39;l&#39;, xlab=&quot;Indice de la variable&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) samples &lt;- 50 x_3.n_draw &lt;- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd) for (i in 1:samples) { lines(1:n, c(conditions_x, x_3.n_draw[i, ]), col = rgb(0, 0, 0, 0.15)) } x_3.n_draw_mean &lt;- apply(x_3.n_draw, 2, mean) x_3.n_draw_sd &lt;- apply(x_3.n_draw, 2, stats::sd) lines(1:n, c(conditions_x, x_3.n_draw_mean), lwd = 2) lines(1:n, c(conditions_x, x_3.n_draw_mean + x_3.n_draw_sd), col = &quot;#b94a73&quot;, lwd = 2) lines(1:n, c(conditions_x, x_3.n_draw_mean - x_3.n_draw_sd), col = &quot;#b94a73&quot;, lwd = 2) points(c(1, 2), conditions_x, pch=16, col=&#39;#46c19a&#39;, cex=2) Revenons au rythme cardiaque. On pourra utiliser le conditionnement aux temps observés, soit 7:00, 8:00, 10:00, 14:00 et 17:00 pour estimer la distribution à 12:00 et 16:00, où à des dimensions artificielles quelconques ici fixées aux demi-heures. # 1. Distribution n &lt;- 21 means_vec &lt;- rep(65, n) covariance_mat &lt;- RBF_kernel(x = 1:n, sigma = 5, l = 2) # 2. Condition conditions_x &lt;- c(61, 74, 69, 67, 78) conditions_indices &lt;- c(1, 3, 7, 15, 21) dependent_indices &lt;- (1:20)[! 1:20 %in% conditions_indices] # 3. Densité conditionnée cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent.ind = dependent_indices, given.ind=conditions_indices, X.given=conditions_x) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- cond_parameters$condVar samples &lt;- 100 x_draw &lt;- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd) means_draw &lt;- apply(x_draw, 2, mean) sd_draw &lt;- apply(x_draw, 2, stats::sd) # 4. Graphiques par(mar=c(4, 4, 1, 1)) ## 4.1 Combiner les prédictions bpm &lt;- rep(NA, n) bpm[conditions_indices] &lt;- conditions_x bpm[dependent_indices] &lt;- means_draw bpm_sd &lt;- rep(NA, n) bpm_sd[conditions_indices] &lt;- 0 bpm_sd[dependent_indices] &lt;- sd_draw ## 4.2 Combiner les tirages et les données x_draw_all &lt;- matrix(ncol = n, nrow = samples) for (i in 1:length(conditions_x)) x_draw_all[, conditions_indices[i]] &lt;- conditions_x[i] x_draw_all[, dependent_indices] &lt;- x_draw ## 4.3 Série plot(1:n, bpm, xlim=c(0, n), ylim=c(40, 90), type=&#39;l&#39;, lwd = 2, xlab=&quot;Indice de la variable&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) for (i in 1:samples) { lines(1:n, x_draw_all[i, ], col = rgb(0, 0, 0, 0.1)) } lines(1:n, bpm+bpm_sd, col = &quot;#b94a73&quot;, lwd = 2) lines(1:n, bpm-bpm_sd, col = &quot;#b94a73&quot;, lwd = 2) points(conditions_indices, bpm[conditions_indices], pch=16, col=&#39;#46c19a&#39;, cex=2) Comme on devrait s’y attendre, la régression résultant de la mise en indices de la distribution est précise aux mesures, et imprécise aux indices peu garnis en mesures. Nous avions utilisé 21 dimensions. Lorsque l’on généralise la procédure à une quantité infinie de dimensions, on obtient un processus gaussien. L’indice de la variable devient ainsi une valeur réelle. Un processus gaussien, \\(\\mathcal{GP}\\), est défini par une fonction de la moyenne, \\(m \\left( x \\right)\\), et une autre de la covariance que l’on nomme noyau (ou kernel), \\(K \\left( x, x&#39; \\right)\\). Un processus gaussien est noté de la manière suivante: \\[\\mathcal{GP} \\sim \\left( m \\left( x \\right), K \\left( x, x&#39; \\right) \\right)\\] La fonction définissant la moyenne peut être facilement écartée en s’assurant de centrer la variable réponse à zéro (\\(y_{centré} = y - \\hat{y}\\)). Ainsi, par convention, on spécifie une fonction de moyenne comme retournant toujours un zéro. Quant au noyau, il peut prendre différentes fonctions de covariance ou combinaisons de fonctions de covariance. Règle générale, on utilisera un noyau permettant de définir deux paramètres: la hauteur (\\(\\sigma\\)) et la longueur de l’ondulation (\\(l\\)). hyperparameters &lt;- expand.grid(l=c(1, 3, 9), sigma=1:3) # Graphique n &lt;- 100 samples_list &lt;- list() for (i in 1:nrow(hyperparameters)) { sample &lt;- mvrnorm(n = 1, mu = rep(0, n), Sigma = RBF_kernel(x=1:n, sigma = hyperparameters$sigma[i], l = hyperparameters$l[i])) samples_list[[i]] &lt;- data.frame(sigma = paste(&quot;sigma =&quot;, hyperparameters$sigma[i]), l = paste(&quot;l =&quot;, hyperparameters$l[i]), x = 1:n, sample = sample) } samples_df &lt;- bind_rows(samples_list) ## Warning in bind_rows_(x, .id): Unequal factor levels: coercing to character ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): Unequal factor levels: coercing to character ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector samples_df %&gt;% ggplot(mapping = aes(x = x, y = sample)) + geom_line() + facet_grid(l ~ sigma) On pourra ajouter à ce noyau un bruit blanc, c’est-à-dire une variation purement aléatoire, sans covariance (noyau générant une matrice diagonale). Le noyau devient ainsi un a priori, et le processus gaussien conditionné aux données devient un a posteriori probabiliste. Finalement, les processus gaussiens peuvent être extrapolés à plusieurs variables descriptives. 12.9.2 Les processus gaussiens en R Pas de souci, vous n’aurez pas à programmer vos propres fonctions pour lancer des processus gaussiens. Vous pourrez passer par caret. Vous pourriez, comme c’est le cas avec les réseaux neuronnaux, obtenir davantage de contrôle sur l’autoapprentissage en utilisant directement la fonction gausspr du package kernlab. library(kernlab) ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:permute&#39;: ## ## how ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha x &lt;- c(7, 8, 10, 14, 17) y &lt;- c(61, 74, 69, 67, 78) y_sc &lt;- (y - mean(y)) / sd(y) m &lt;- gausspr(x, y_sc, kernel = &#39;rbfdot&#39;, kpar = list(sigma = 4), variance.model = TRUE, scaled = TRUE, var = 0.01, cross = 2) xtest &lt;- seq(6, 18, by = 0.1) y_sc_pred_mean &lt;- predict(m, xtest, type=&quot;response&quot;) y_pred_mean &lt;- y_sc_pred_mean * sd(y) + mean(y) y_sc_pred_sd &lt;- predict(m, xtest, type=&quot;sdeviation&quot;) y_pred_sd &lt;- y_sc_pred_sd * sd(y) plot(x, y, xlim = c(6, 18), ylim = c(45, 90)) lines(xtest, y_pred_mean) lines(xtest, y_pred_mean + y_pred_sd, col=&quot;red&quot;) lines(xtest, y_pred_mean - y_pred_sd, col=&quot;red&quot;) abline(v=12, lty=3, col=&#39;gray50&#39;);text(12, 67, &#39;?&#39;, cex=2) abline(v=16, lty=3, col=&#39;gray50&#39;);text(16, 72, &#39;?&#39;, cex=2) 12.9.3 Application pratique Les processus gaussiens sont utiles pour effectuer des prédictions sur des phénomène sur lesquels on désire éviter de se commettre sur la structure. Les séries temporelles ou les signaux spectraux en sont des exemples. Aussi, j’ai utilisé les processus gaussiens pour modéliser des courbes de réponse aux fertilisants. EXEMPLE… Prédiction spatiale: - https://www.sciencedirect.com/science/article/pii/S2211675316300033 - https://stackoverflow.com/questions/43618633/multi-output-spatial-statistics-with-gaussian-processes "],
["chapitre-geo.html", "13 Les données spatiales", " 13 Les données spatiales Notions de base géomatique: systèmes géodésiques, projections, utilisation d’un GPS Utilisation de R comme GIS et comme outil d’analyse spatiale (donnée associées à des points, lignes, polygones) Visualisation de données géoréférencées avec ggplot et Leaflet Introduction aux projections spatiales Projections par KNN ou Procécus gaussiens Géostatistiques avec gstat (juste mentionner, préférer le ML) Modélisation de distribution des espèces (species distribution modelling) "],
["chapitre-ode.html", "14 Modélisation déterministe 14.1 Équations différentielles 14.2 Objectifs 14.3 Les équations différentielles ordinaires en modélisation écologique 14.4 Les équations différentielles partielles en modélisation écologique", " 14 Modélisation déterministe On se réfère à la modélisation mécanistique lorsque des principes théoriques guident une modélisation, à l’inverse de la modélisation phénoménologique, qui est guidée par les données. Il existe de nombreuses techniques de modélisation mécanistique, mais la plupart sont guidées par les équations différentielles. 14.1 Équations différentielles Les équations différentielles permettent la résolution de problèmes impliquant des gradients dans le temps et dans l’espace. On les utilise pour modéliser la dynamique des populations, la thermodynamique, l’écoulement de l’eau dans les sols, le transport des solutés, etc. On en distingue deux grandes catégories: les équations différentielles ordinaires et partielles. Équations différentielles ordinaires (EDO). Les équations différentielles ordinaires s’appliquent sur des fonctions s’appliquant à une seule variables, qui est souvent le temps. On pourra suivre, par exemple, l’évolution de la température en un point, en fonction du temps à partir d’une condition initiale. Parfois, plusieurs EDO sont utilisées conjointement pour créer un système d’EDO que l’on pourra nommé un système dynamique. Les solutions analytiques des EDO sont parfois relativement faciles à résoudre, mais les ordinateurs permettent des résolutions numériques en quelques lignes de code. Équations différentielles partielles (EDP). Dans ce cas, ce sont plusieurs variables qui sont différenciées dans la même fonction. Il peut s’agir des coordonnées dans l’espace \\([x, y, z]\\) (régime permanent), qui peuvent aussi être appliqués à différents pas de temps (régime transitoire). Le problème sera délimité non pas seulement par des conditions initiales, mais aussi par des conditions aux frontières du modèle. Puisque que les solutions analytiques des EDP peuvent rarement être développées, on utilisera pratiquement toujours des approches numériques que sont principalement les méthodes de résolution par différences finies ou par éléments finis. 14.2 Objectifs L’objectif de cette section est de vous initier à la modélisation par équations différentielles sur R. Au terme de chapitre, vous serez en mesure de: Détecter un problème impliquant le besoin d’utiliser des équations différentielles Différentier un problème d’EDO et d’EDP D’effectuer une modélisation impliquant un système d’EDO en contexte écologique D’effectuer une modélisation par différences finies impliquant une EDP simple en contexte écologique 14.3 Les équations différentielles ordinaires en modélisation écologique L’évolution des populations dans le temps peut être abordée à l’aide de systèmes d’équations différentielles. Une simple équation décrivant la croissance d’une population peut être couplée à des schémas d’exploitation de cette population, que ce soit une exploitation forestière, une terre fourragère ou un territoire de chasse. On pourra aussi faire interagir des populations dans des schémas de relations biologiques. Ces processus peuvent être implémentés avec des processus aléatoires pour générer des schémas probabilistes. De plus, les biostatistiques et l’autoapprentissage peuvent être mis à contribution afin de calibrer les modèles. 14.3.1 Évolution d’une seule population en fonction du temps La croissance d’une population (ou de sa densité) isolée en fonction du temps dépend des conditions qui lui offre son environnement. Dans le cas de la biomasse d’une culture à croissance constante, le taux de croissance est toujours le même. \\[ \\frac{d 🌿 }{dt} = c \\] \\[ \\int_0^t c dt = \\int_{🌿_0}^{🌿(t)} ~d🌿 \\] \\[ ct = 🌿(t) - 🌿_0\\] \\[ 🌿(t) = 🌿_0 + ct \\] par(mar=c(4, 4, 1, 1), ps=10) y0 &lt;- 2 c &lt;- 2 # exprimé en individu / pas de temps times &lt;- seq(0, 6, 0.1) y &lt;- y0 + c * times plot(times, y, &#39;l&#39;, xlab=&quot;An&quot;, ylab=&quot;Population&quot;, ylim=c(0, max(y))) text(max(times), max(y), round(max(y))) Dans le cas d’une population qui se reproduit, une formulation simple modélise une évolution linéaire associée à un taux de natalité \\(n\\) et un taux de mortalité \\(m\\), où \\(r = n-m\\) est le taux de croissance de la population d’une population de lapins 🐰 en fonction du temps \\(t\\). \\[ \\frac{d🐰}{dt} = n🐰 - m🐰 = r🐰 \\] \\[ \\int_0^t dt = \\int_{🐰_0}^{🐰(t)} \\frac{1}{r🐰} ~d🐰 \\] \\[ t = \\frac{1}{r} ln(🐰) \\bigg\\rvert_{🐰_0}^{🐰(t)} \\] \\[ rt = ln \\left( \\frac{🐰(t)}{🐰_0} \\right) \\] \\[ 🐰(t) = 🐰_0 exp(rt) \\] La vitesse de croissance est constante pour une population constante, mais la croissance de la population est exponentielle étant donnée que chaque nouvel individu se reproduit. par(mar=c(4, 4, 1, 1), ps=10) y0 &lt;- 10 r &lt;- 0.2 # exprimé en individu / pas de temps times &lt;- seq(0, 10, 0.1) y &lt;- y0 * exp(r*times) plot(times, y, &#39;l&#39;, xlab=&quot;An&quot;, ylab=&quot;Lapin&quot;, ylim=c(0, max(y))) text(max(times), max(y), round(max(y))) De 10 lapins au départ, nous en avons un peu plus de 75 après 10 ans… et près de 5 milliards après 100 ans! En fait, la capacité de support d’une population étant généralement limitée, on peut supposer que le taux de natalité décroit et que le taux de mortalité croit linéairement avec l’effectif. \\[ n(🐰) = \\alpha - \\beta 🐰 \\] \\[ m(🐰) = \\gamma + \\delta 🐰 \\] On aura donc \\[ \\frac{d🐰}{dt} = 🐰 \\left( \\alpha - \\beta 🐰 \\right) - 🐰 \\left( \\gamma + \\delta 🐰 \\right) = r🐰 \\left( 1 - \\frac{🐰}{K} \\right) \\] où \\(r = \\alpha - \\gamma\\) est l’ordonnée à l’origine du taux de croissance (théorique, lorsque la population est nulle) et \\(K = \\frac{\\alpha-\\gamma}{\\beta + \\delta}\\) est la capacité limite du milieu de subsistance. On pourra s’aider d’un logiciel de calcul symbolique comme sympy ou maxima pour en tirer une solution analytique. Mais à ce point, nous utiliserons une approximation numérique. Nous utiliserons le module deSolve. library(&quot;deSolve&quot;) deSolve demande de définir les paramètres de l’EDO ou du système d’EDO. Nous devons d’abord spécifier à quels pas de temps notre EDO doit être approximée. J’étends la plage de temps à 30 ans pour bien visualiser la courbe de croissance. times &lt;- seq(0, 30, by = 0.5) Les conditions initiales du système d’EDO sont aussi définies dans un vecteur. La seule condition initiale de notre EDO est le nombre initial de lapin. y0 &lt;- c(lapin = 10) On définira les paramètres dans un vecteur p. Dans notre cas, nous avons \\(r\\), le taux de croissance à l’origine et \\(K\\), la capacité de support de l’écosystème. Il est préférable de nommer les paramètres du vecteur pour éviter les erreurs. p &lt;- c(r = 0.2, K = 40) Enfin, une fonction définit l’EDO avec, comme entrées, les pas de temps, les conditions initiales et les paramètres. La sortie de la fonction est un vecteur des dérivées emboîtés dans une liste (lisez le fichier d’aide de la fonction ode pour les détails en lançant ?ode). model_logistic &lt;- function(t, y, p) { lapin &lt;- y[1] dlapin_dt &lt;- p[1] * lapin * (1 - lapin/p[2]) return(list(c(dlapin_dt))) } Une fois que les pas de temps, les conditions initiales, les paramètres et le modèle sont définis, on les spécifie comme arguments dans la fonction ode. La sortie de la fonction ode est une matrice dont la première colonne comprend les pas de temps imposés, et les autres colonnes sont les dérivées spécifiées à la sortie de la fonction ode. lapin_t &lt;- ode(y = y0, times = times, model_logistic, p) head(lapin_t) ## time lapin ## [1,] 0.0 10.00000 ## [2,] 0.5 10.76856 ## [3,] 1.0 11.57342 ## [4,] 1.5 12.41288 ## [5,] 2.0 13.28478 ## [6,] 2.5 14.18643 par(mar=c(4, 4, 1, 1), ps=10) plot(lapin_t[, 1], lapin_t[, 2], type=&#39;l&#39;, xlab=&quot;An&quot;, ylab=&quot;Lapin&quot;, ylim=c(0, max(lapin_t[, 2]))) Exercice. Que ce passerait-il si le taux de croissance était négatif? Profitez-en pour changer les paramètres r et K. Exercice. D’autres formulations existent pour exprimer des taux de croissance (Gompertz, Allee, etc.). En outre la formulation de Gompertz s’écrit comme suit. \\[ \\frac{d🐰}{dt} = r🐰 \\left( ln \\frac{K}{🐰} \\right) \\] Entrer cet EDO dans R avec deSolve. 14.3.2 Population exploitée L’exploitation d’une population peut être effectuée de différentes manières. D’abord, le prélèvement peut être effectué de manière constante, par exemple dans un élevage ou par la chasse ou la cueillette. Ajoutons un prélèvement constant dans une courbe de croissance logistique. \\[ \\frac{d🐰}{dt} = r🐰 \\left( 1 - \\frac{🐰}{K} \\right) - Q \\] où \\(Q\\) est le quota, ou le prélèvement constant. On pourra aussi effectuer un prélèvement proportionnel à la population. \\[ \\frac{d🐰}{dt} = r🐰 \\left( 1 - \\frac{🐰}{K} \\right) - E🐰 \\] où \\(E\\) est l’effort d’exploitation. Ou bien effectuer une série de prélèvement ponctuels, comme la récolte de plantes fourragères. \\[ \\frac{d🌿}{dt} = c - \\left[ 🌿 - \\gamma \\right] \\bigg\\rvert_{t=a, b, c, d, e, ...} \\] où \\(\\gamma\\) est le reste de la biomasse après la récolte et \\(t=a, b, c, d, e, ...\\) sont les pas de temps où le bloc entre les crochets est actif, c’est-à-dire la période de récolte. La solution analytique d’une culture à croissance constante est plutôt facile à déduire. Les fonctions de prélèvement peuvent être modulées à votre guise. Prenons pour l’exemple un prélèvement constant et une croissance logistique. p &lt;- c(r = 0.2, K = 40, Q = 1) model_logistic_expl &lt;- function(t, y, p) { lapin &lt;- y[1] dlapin_dt &lt;- p[1] * lapin * (1 - lapin/p[2]) - p[3] return(list(c(dlapin_dt))) } lapin_t &lt;- ode(y = y0, times = times, model_logistic_expl, p) par(mar=c(4, 4, 1, 1), ps=10) plot(lapin_t[, 1], lapin_t[, 2], type=&#39;l&#39;, xlab=&quot;An&quot;, ylab=&quot;Lapin&quot;, ylim=c(0, max(lapin_t[, 2]))) Exercice. Modéliser avec un prélèvement proportionnel. L’exploitation ponctuelle, comme la récolte ou l’administration d’une série de traitements, implique l’utilisation d’approches intermittentes. deSolve ignore les changements dans les variables d’état (y) tels que définis dans les dérivés. Pour ce faire, nous devons avoir recours à des évènements dans le jargon de deSolve. Ces évènements doivent être spécifiés dans un data.frame ou une liste. Il est difficile de trouver un exemple générique pour modéliser des évènements. Pour en savoir davantage, je vous invite donc à consulter la fiche d’aide ?events. Dans notre cas, nous allons modéliser une récolte de plantes fourragères. La récolte est déclenchée lorsque le rendement atteint 2 t/ha, et laisser 0.3 t/ha au sol pour assurer le renouvellement pour les coupes subséquentes. Définissons d’abord les entrées du modèles. times &lt;- seq(0, 120, 0.1) p &lt;- c(r = 0.1, K = 2.5) y0 &lt;- c(champ = 0.1) Nous devons définir une fonction root, dont la sortie est une valeur qui déclenchera un évènement lorsque la valeur sera nulle. Dans notre cas, la valeur correspond simplement au rendement moins 2, la quantité au champ y[1]. Notez que d’autres stratégies peuvent être utilisées pour déclencher une récolte, par exemple le pourcentage de floraison qui demanderait des simulations plus poussées. recolte_root &lt;- function(t, y, p) y[1]-2 Puis, lorsque la fonction root est déclenchée, l’évènement ramène la quantité au champs à 1 t/ha, une quantité qui permet de relancer la croissance. recolte_event &lt;- function(t, y, p) { y[1] &lt;- 0.3 return(y) } La fonction du modèle est telle qu’utilisée auparavant: une fonction logistique. recolte &lt;- function(t, y, p) { champ &lt;- y[1] dchamp_dt &lt;- p[1] * champ * (1 - champ/p[2]) return(list(c(dchamp_dt))) } La fonction ode est lancée en entrant les fonction root et events. out &lt;- ode(times = times, y = y0, func = recolte, parms = p, rootfun = recolte_root, events = list(func = recolte_event, root = TRUE), method=&quot;impAdams&quot;) plot(out) Nous pourrons organiser deux récoltes de 1.7 t/ha et une de 2 t/ha pour terminer la saison. Exercice. Qu’adviendrait-il si vous laissiez 0.15 t/ha au champ au lieu de 0.3? Ou si vous laissiez 1 t/ha? Ou si vous déclenchiez une récolte à 2.3 t/ha? Défi. Pouvez-vous modéliser l’ensilage? 14.3.3 Interactions biologiques Les interactions biologiques entre deux espèces à un stade de croissance défini peuvent prendre différentes formes, du mutualisme (les deux espèces bénéficient de la relation) à la compétition (les deux espèces se nuisent) en passant par la prédation ou le parasitisme (une espèce bénéficie de l’autre en lui nuisant) ou le neutralisme (aucun effet). Ces effets sont décrits dans Pringle (2016) en un tableau synthèse. Source: Pringle, E.G. 2016. Orienting the Interaction Compass: Resource Availability as a Major Driver of Context Dependence. Plos Biology. https://doi.org/10.1371/journal.pbio.2000891 Ces interactions peuvent être décrite mathématiquement dans des systèmes d’EDO, ou EDO couplées. Le cas d’étude le plus courant reprend le système d’équation prédateur-proie de Lotka-Volterra, deux auteurs ayant développé de manière indépendante des équations similaires respectivement en 1925 et 1926. Les équations de Lotka-Volterra supposent une croissance illimitée des deux espèces: les proies 🐰 se reproduisent par elles-mêmes (\\(\\alpha 🐰\\)), tandis que les prédateurs 🦊 croissent selon la disponibilité des proies (\\(\\delta 🐰🦊\\)). À l’inverse, la mortalité des proies dépend du nombre de prédateurs (\\(- \\beta 🐰🦊\\)), mais la mortalité des prédateurs est indépendante des proies (\\(- \\gamma 🦊\\)). On obtient ainsi un système d’équation. \\[\\frac{d🐰}{dt} = \\alpha 🐰 - \\beta 🐰🦊 = 🐰 \\left( \\alpha - \\beta 🦊 \\right)\\] \\[\\frac{d🦊}{dt} = \\delta 🐰🦊 - \\gamma 🦊 = 🦊 \\left( \\delta 🐰 - \\gamma \\right) \\] À l’équilibre de 🐰, c’est-à-dire où \\(\\frac{d🐰}{dt} = 0\\), on retrouve \\(🐰=0\\) ou \\(🦊 = \\frac{\\alpha}{\\beta}\\). De même, à l’équilibre de 🦊, on retrouve \\(🦊=0\\) ou \\(🐰 = \\frac{\\gamma}{\\delta}\\). En termes mathématiques, ces équilibre sont des isoclines, des points d’inflexion dans le système d’EDO. Nous allons résoudre les équations de Lotka-Volterra avec deSolve. Rappelons-nous que nous devons définir des pas de temps où approximer les populations (times), des conditions initiales (y0) et des paramètres (p). times &lt;- seq(0, 30, by = 0.1) y0 &lt;- c(lapin = 3, renard = 1) p &lt;- c(alpha = 2, # taux de croissance des lapins (naissance - mortalité, 1/an) beta = 0.8, # taux de prédation des lapins (renard / an) delta = 0.1, # taux de conversion lors de la prédation (lapin / renard) gamma = 0.2) # mortalité naturelle des renards (1/an) On peut calculer d’emblée les isoclines. lapin_iso &lt;- p[4]/p[3] renard_iso &lt;- p[1]/p[2] Nous devons ensuite créer notre modèle. modele_LV &lt;- function(t, y, p) { lapin = y[1] renard = y[2] dlapin_dt = p[1] * lapin - p[2] * lapin * renard drenard_dt = p[3] * lapin * renard - p[4] * renard return(list(c(dlapin_dt, drenard_dt))) } Lançons l’approximation. effectifs_t = ode(y = y0, times = times, modele_LV, p) head(effectifs_t) ## time lapin renard ## [1,] 0.0 3.000000 1.000000 ## [2,] 0.1 3.380961 1.011940 ## [3,] 0.2 3.806028 1.028156 ## [4,] 0.3 4.278154 1.049326 ## [5,] 0.4 4.799633 1.076263 ## [6,] 0.5 5.371673 1.109943 par(mar=c(4, 4, 1, 1), ps=10) plot(effectifs_t[, 1], effectifs_t[, 2], type = &#39;l&#39;, ylim = c(0, max(effectifs_t[, 2])), xlab = &#39;Temps&#39;, ylab = &quot;Nombre d&#39;individus&quot;) # lapins lines(effectifs_t[, 1], effectifs_t[, 3], col = &#39;red&#39;) legend(x=4, y=12, legend=c(&quot;Lapins&quot;, &quot;Renards&quot;), col=c(&quot;black&quot;, &quot;red&quot;), lty=c(1, 1), cex=1.2) Lorsque la population de lapins croit, celle des renards croit à retardement jusqu’à ce que la population de lapin diminue jusqu’à être presque éteinte. Dans ces conditions, la population de renard ne peut plus être soutenue, et décroit, ce qui en retour donne l’opportunité de la population de lapins de resurgir. par(mar=c(4, 4, 1, 1), ps=10) plot(effectifs_t[, 2], effectifs_t[, 3], type = &#39;l&#39;, xlab = &quot;Nombre lapins&quot;, ylab= &quot;Nombre de renards&quot;, xlim = c(0, max(effectifs_t[, 2])), ylim = c(0, max(effectifs_t[, 3]))) # isoclines abline(v=lapin_iso, lty=2, col=&quot;black&quot;) abline(h=renard_iso, lty=2, col=&quot;red&quot;) points(lapin_iso, renard_iso) # condition initiale points(y0[1], y0[2], pch = 16) Les conditions initiales sont responsables de l’amplitude des cycles. En faisant les faisant varier et en portant graphiquement les vecteurs de flux, on peut mieux apprécier l’importance des isoclines, qui séparent la direction que prend la relation entre deux espèces. effectifs_i &lt;- list() lapin_0 &lt;- 1:30 for (i in 1:length(lapin_0)) { y0[1] &lt;- lapin_0[i] effectifs_i[[i]] &lt;- ode(y = y0, times = times, modele_LV, p) offsets &lt;- effectifs_i[[i]][-1, -1] - effectifs_i[[i]][-nrow(effectifs_i[[i]]), -1] colnames(offsets) &lt;- c(&quot;d_lapin&quot;, &quot;d_renard&quot;) effectifs_i[[i]] &lt;- cbind(effectifs_i[[i]][-1, ], offsets) } effectifs_df &lt;- do.call(rbind.data.frame, effectifs_i) library(&quot;plotrix&quot;) ## ## Attaching package: &#39;plotrix&#39; ## The following object is masked from &#39;package:bayesplot&#39;: ## ## plot_bg plot(effectifs_df[, 2], effectifs_df[, 3], type = &#39;n&#39;, xlab = &quot;Nombre lapins&quot;, ylab= &quot;Nombre de renards&quot;, xlim = c(0, max(effectifs_df[, 2])), ylim = c(0, max(effectifs_df[, 3]))) # isoclines abline(v=lapin_iso, lty=2, col=&quot;black&quot;) abline(h=renard_iso, lty=2, col=&quot;red&quot;) points(lapin_iso, renard_iso) vectorField(u=effectifs_df[, 4], v=effectifs_df[, 5], xpos=effectifs_df[, 2], ypos=effectifs_df[, 3], scale=0.1, headspan=0.05, vecspec=&quot;lonlat&quot;) Nous avons modélisé une relation biologique de prédation. Il existe dans la littérature une panoplie de modèles d’EDO pour décrire les relations biologiques, qui peuvent être modélisés entre plusieurs espèces pour créer des réseaux trophiques complexes. Toutefois, la difficulté de collecter des données en quantité et en qualité suffisante rendent ces modèles difficiles à appréhender. Exercice. Qu’adviendrait-il des populations si l’on prenait plutôt un profil de croissance logistique chez les lapins? \\[\\frac{d🐰}{dt} = r🐰 \\left( 1-\\frac{x}{K} \\right) - \\beta 🐰🦊 \\] \\[\\frac{d🦊}{dt} = \\delta 🐰🦊 - \\gamma 🦊 \\] Exercice. Modéliser une compétition interspécifique où chaque population croit de manière logistique. \\[\\frac{d🐁}{dt} = r_1 🐁 \\left( 1-\\frac{🐁}{K_1} -\\alpha \\frac{🐀}{K_1} \\right) \\] \\[\\frac{d🐁}{dt} = r_2 🐀 \\left( 1-\\frac{🐀}{K_2} -\\beta \\frac{🐁}{K_2} \\right) \\] où \\(r_1\\) et \\(r_2\\) sont les taux de croissances respectifs des 🐁 et des 🐀, ainsi que \\(K_1\\) et que \\(K_2\\) sont les capacités de support des 🐁 et des 🐀. Le coefficient \\(\\alpha\\) décrit l’ampleur de la compétition de 🐀 sur 🐁 et le coefficient \\(\\beta\\) décrit l’ampleur de la compétition de 🐁 sur 🐀 (\\(\\alpha\\) et \\(\\beta\\) sont &gt;= 0). Exercice. Les interactions biologiques forment une bonne introduction aux systèmes d’équations différentielles ordinaires. On fait néanmoins souvent référence aux équations de Lorenz (1963), qui a développé un système d’EDO chaotique depuis trois équations, \\[ X&#39; = aX + YZ, \\] \\[ Y&#39; = b \\left(Y-Z\\right), \\] \\[ Z&#39; = -XY + cY - Z, \\] où \\(X\\) est la température horizontale, \\(Y\\) est la température verticale, \\(Z\\) est le flux de chaleur convectif, et où l’on retrouve les paramètres \\(a = -8/3\\), \\(b=-10\\) et \\(c=28\\). Résoudre les équations de Lorents avec deSolve. Porter graphiquement les relations entre X, Y et Z. 14.4 Les équations différentielles partielles en modélisation écologique Contrairement aux EDO, la solution des équations différentielle partielles (EDP) dépend de plus d’une variable indépendante. Typiquement, elles dépendent de coordonnées spatiales. Elles peuvent aussi dépendre du temps. Dans cette section, nous allons explorer les régimes permanents, c’est-à-dire indépendants du temps, en utilisant la méthode des différences finies. Nous allons aussi explorer les problèmes transitoires, qui eux dépendent du temps, en utilisant la méthode des lignes. À venir… "]
]
