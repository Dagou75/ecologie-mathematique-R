[
["chapitre-explorer.html", "7 Explorer R 7.1 R sur le web 7.2 R en chaire et en os 7.3 Quelques outils en écologie mathématique avec R", " 7 Explorer R L’apprentissage de R peut être étourdissant. Cette section est une petite pause fourre-tout qui vous introduira aux nombreuses possibilités de R. ️ Objectifs spécifiques: À la fin de ce chapitre, vous serez en mesure d’identifier les sources d’information principales sur le développement de R et de ses modules comprendrez l’importance du prétraitement des données, en particulier dans le cadre de l’analyse de données compositionnelles, et saurez effectuer un prétraitement adéquat saurez comment acquérir des données météo d’Environnement Canada avec le module weathercan saurez identifier les modules d’analyse de sols (soiltexture et aqp) saurez créer des cartes etc. Pour certains, le langage R est un labyrinthe. Pour d’autres, c’est une myriade de portes ouvertes. Si vous lisez ce manuel, vous vous êtes peut-être engagé dans un labyrinthe dans l’objectif d’y trouver la clé qui dévérouillera une porte bien précise qui mène à un trésor, un objet magique… ou un diplôme. Peut-être aussi prendrez-vous le goût d’errer dans ce labyrinthe, explorant ses débouchés, pour y dénicher au hasard des petits outils et des débouchés. Séquence du jeu vidéo The legend of Zelda. Cette section est un amalgame de plusieurs outils de R pertinents en analyse écologique. 7.1 R sur le web Dans un environnement de travail en évolution rapide et constante, il est difficile de considérer que ses compétences sont abouties. Rester informé sur le développement de R vous permettra de dénicher de résoudre des problèmes persistants de manière plus efficace ou par de nouvelles avenues, et vous offrira même l’occasion de dénicher des problèmes dont vous ne soupçonniez pas l’existance. Plusieurs sources d’information vous permettront de vous tenir à jour sur le développement de R, de ses environnement de travail (RStudio, Jupyter, Atom, etc.) et des nouveaux modules qui s’y greffent. Plus largement, vous gagnerez à vous informer sur les dernières tendances en calcul scientifique sur d’autres plate-forme que R (Python, Javascript, Julia, etc.). Évidemment, nos tâches quotidiennes ne nous permettent pas de tout suivre. Même si vous pouviez n’attrapper qu’1% du défilement, ce sera déjà 1% de plus que rien du tout. Évidemment, rester au courant aide parce que vous en apprenez davantage sur les outils et leurs applications. Mais ça aide aussi parce que ça vous permet de connaître des gens et des organisations! Il est très utile de savoir qui travaille sur quoi et où se déroulent les développements sur un sujet donné, car si vous cherchez consciemment quelque chose plus tard, ça vous aidera à trouver votre chemin plus facilement. - Maëlle Salmon, Keeping up to date with R news (ma traduction) Je vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez à l’aventure! The Hobbit: An Unexpected Journey, Peter Jackson (2012) 7.1.1 GitHub Nous verrons au chapitre 11 l’importance d’utilser des outils d’archivage et de suivi de version, comme git, dans le déploiement de la science ouverte. Pour l’instant, retenons que GitHub est une plate-forme git en ligne acquise par Microsoft qui est devenue un réseau social de développement informatique. De nombreux modules de R y sont développés. Au chapitre 11, vous serez invités à y ouvrir un compte et à y archiver du contenu. Vous pourrez alors suivre (dans le même sens que sur Facebook ou Twitter) le développement de projets et suivre les travaux des personnes qui vous semblent d’intérêt. 7.1.2 Nouvelles Le site d’aggrégation R-bloggers, mis à jour quotidiennement, republie des articles en anglais tirés d’un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux développement. Deux fois par mois, l’organisation rOpenSci offre un portrait de l’univ-R (💩), ce que R Weekely offre de manière hebdomadaire (l’information sera probablement redondante). Le tidyverse a quant à lui son propre blogue. 7.1.3 Twitter Le hashtag #rstats rassemble sur Twitter ce qui se tweete sur le sujet. On y retrouve les comptes de R-bloggers, RStudio et rOpenSci. Certaines communauté y sont aussi actives, comme R4DS online learning community, qui partage des nouvelles sur R, et R-Ladies Global, qui vise à amener davantage de diversité à la communauté de R. Des comptes thématiques comme Daily R Cheatsheets et One R Package a Day permettent de découvrir quotidiennement de nouvelles possibilités. Enfin, plusieurs personnes contribuent positivement à la communauté R. Hadley Wickham brille parmi les étoiles de R. Les comptes de Mara Averick, Claus Wilke et David Robinson sont aussi intéressants. 7.1.4 Des questions? Bien que davantage voués à la résolution de problème qu’ à l’exploration de nouvelles opportunités, Stackoverflow et Cross Validated sont des plate-forme prisées. De plus, la liste de courriels r-sig-ecology permet des échanges entre professionnels et novices en analyse de données écologiques avec R. 7.1.5 Participer R est un logiciel basé sur une communauté de développement, d’utilisation et de vulgarisation. Des personnes offrent généreusement du temps de support. Si vous vous sentez à l’aise, offrez aussi le vôtre! 7.1.6 Mise en garde Les modules de R sont développés par quiconque le veut bien: leur qualité n’est pas nécessairement auditée. Souvent, ils ne sont vérifiés que par une vigilance communautaire: dans ce cas, vous êtes les cobailles. Ce qui n’est pas nécessairement une mauvaise chose, mais cela nécessite de prendre ses précautions. Dans sa conférence How to be a resilient R user, Maëlle Salmon propose quelques guides pour juger de la qualité d’un module. 1. Le module est-il activement développé? Bien! Attention! 2. Le module est-il bien testé? Vérifiez si le module a fait l’objet d’une publication scientifique, s’il a été utilisé avec succès dans la litérature ou dans des documents crédibles. 3. Le module est-il bien documenté? Un site internet dédié est-il utilisé pour documenter l’utilisation du module? Les fichiers d’aide sont-ils complets, et sont-ils de bonne qualité? 4. Le module est-il largement utilisé? Un module peu populaire n’est pas nécessaissairement de mauvaise qualité: peut-être est-il seulement destiné à des applications de niche. S’il n’est pas un indicateur à lui seul de la solidité ou la validité d’un module, une masse critique indique que le module a passé sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peu être évalué par le nombre d’étoiles attribué au module (équivalent à un J’aime). 5. Le module est-il développé par une personne ou une organisation crédible? On peut affirmer sans trop se compromettre que l’équipe de RStudio développe des modules de confiance. Tout comme il faudrait se méfier d’un module développé par une personne anonyme. Le module packagemetrics permet d’évaluer ces critères. library(&quot;packagemetrics&quot;) pm &lt;- package_list_metrics(c(&quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;vegan&quot;, &quot;greta&quot;)) metrics_table(pm) package published dl_last_month stars tidyverse_happy has_tests vignette last_commit last_issue_closed contributors depends_count reverse_count dplyr 2018-11-10 694730 2807 0.0 167 1 1081 ggplot2 2018-10-25 744917 3607 0.0 173 1 1494 vegan 2019-02-04 45385 139 0.3 19 3 97 greta 2018-10-30 505 353 2.3 11 1 0 7.1.7 Prendre tout ça en note Un logiciel de prise de notes (comme Evernote, OneNote, Notion, Simplenote, Turtl, etc.) pourrait vous être utile pour retrouver l’information soutirée de vos flux d’information. Mais certaines personnes consignent simplement leurs informations dans un carnet ou un document de traitement de texte. 7.2 R en chaire et en os L’Université Laval (institution auprès de laquelle ce manuel est développé) sera haute en mai 2019 de la conférence R à Québec 2019. Des ateliers seront offerts pour les utilisateurs novices et avancés. 7.3 Quelques outils en écologie mathématique avec R 7.3.1 Prétraitement des données Il arrive souvent ques les données brutes ne soient pas exprimées de manière appropriée ou optimale pour l’analyse statistique ou la modélisation. Vous devrez alors effectuer un prétraitement sur ces données. Lors du chapitre 5, nous avons abordé la mise à l’échelle, où des variables numériques étaient transformées pour avoir une moyenne de zéro et un écart-type de 1. Cette opération permettait d’apprécier les coefficients et leur incertitude sur une même échelle. L’encodage catégorielle a quant à lui permi d’utiliser des méthodes quantitatives sur des données qualitatives. Dans les deux cas, nous n’avons pas utilisé le terme, mais il s’agissait d’un prétraitement, c’est-à-dire une transformation des données préalable à l’analyse ou la modélisation. Un prétraitement peut consister simplement en une transformation logarithmique ou exponentielle. En particulier, si vos données forment une partie d’un tout (exprimées en pourcentages ou fractions), vous devriez probablement utiliser un prétraitement grâce aux outils de l’analyse compositionnelle. Avant de les aborder, nous allons traiter des transformations de base. 7.3.1.1 Standardisation La standardisation consiste à centrer vos données à une moyenne de 0 et à les échelonner à une variance de 1, c’est-à-dire \\[x_{standard} = \\frac{x - \\bar{x}}{\\sigma}\\] où \\(\\bar{x}\\) est la moyenne du vecteur \\(x\\) et où \\(\\sigma\\) est son écart-type. Ce prétraitement des données peut s’avérér utile lorsque la modélisation tient compte de l’échelle de vos mesures (par exemple, les paramètres de régression vus au chapitre 5 ou les distances que nous verrons au chapitre 8). En effet, les pentes d’une régression linéaire multiple ne pourront être comparées entre elles que si elles sont une même échelle. Par exemple, on veut modéliser la consommation en miles au gallon (mpg) de voitures en fonction de leur puissance (hp), le temps en secondes pour parcourir un quart de mile (qsec) et le nombre de cylindre. data(mtcars) modl &lt;- lm(mpg ~ hp + qsec + cyl, mtcars) summary(modl) ## ## Call: ## lm(formula = mpg ~ hp + qsec + cyl, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3223 -1.9483 -0.5656 1.5452 7.7773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.30540 9.03697 6.120 1.33e-06 *** ## hp -0.03552 0.01622 -2.190 0.03700 * ## qsec -0.89424 0.42755 -2.092 0.04567 * ## cyl -2.26960 0.54505 -4.164 0.00027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.003 on 28 degrees of freedom ## Multiple R-squared: 0.7757, Adjusted R-squared: 0.7517 ## F-statistic: 32.29 on 3 and 28 DF, p-value: 3.135e-09 Les pentes signifient que la distance parcourue par gallon d’essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L’interprétation est conviviale à cette échelle. Mais lequel de ces effets est le plus important? L t value indique que ce seraient les cylindres. Mais pour juger l’importance en terme de pente, il vaudrait mieux standardiser. library(&quot;tidyverse&quot;) ## ── Attaching packages ──────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.3.0 ## ✔ tibble 2.0.1 ✔ dplyr 0.7.8 ## ✔ tidyr 0.8.2 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.3.0 ## ── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() standardise &lt;- function(x) (x-mean(x))/sd(x) mtcars_sc &lt;- mtcars %&gt;% mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE) modl_sc &lt;- lm(mpg ~ hp + qsec + cyl, mtcars_sc) summary(modl_sc) ## ## Call: ## lm(formula = mpg ~ hp + qsec + cyl, data = mtcars_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71716 -0.32326 -0.09384 0.25639 1.29042 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.061e-16 8.808e-02 0.000 1.00000 ## hp -4.041e-01 1.845e-01 -2.190 0.03700 * ## qsec -2.651e-01 1.268e-01 -2.092 0.04567 * ## cyl -6.725e-01 1.615e-01 -4.164 0.00027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4983 on 28 degrees of freedom ## Multiple R-squared: 0.7757, Adjusted R-squared: 0.7517 ## F-statistic: 32.29 on 3 and 28 DF, p-value: 3.135e-09 Les valeurs des pentes ne peuvent plus être interprétées directement, mais peuvent maintenant être comparées entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile. Les algorithmes basés sur des distances auront, de même, avantage à être standardisés. 7.3.1.2 À l’échelle de la plage Si vous désirez préserver le zéro dans le cas de données positives ou plus généralement vous voulez que vos données prétraitées soient positives, vous pouvez les transformer à l’échelle de la plage, c’est-à-dire les forcer à s’étaler de 0 à 1: \\[ x_{range01} = \\frac{x - x_{min}}{x_{max} - x_{min}} \\] Cette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transformé les valeurs aberrantes seront toutefois plus difficiles à détecter. range_01 &lt;- function(x) (x-min(x))/(max(x) - min(x)) mtcars %&gt;% mutate_if(is.numeric, range_01) %&gt;% # en fait, toutes les colonnes sont numériques, alors mutate_all aurait pu être utilisé au lieu de mutate_if sample_n(4) ## mpg cyl disp hp drat wt qsec vs am ## 11 0.3148936 0.5 0.2407084 0.2508834 0.5345622 0.4927129 0.5238095 1 0 ## 24 0.1234043 1.0 0.6956847 0.6819788 0.4470046 0.5949885 0.1083333 0 0 ## 23 0.2042553 1.0 0.5809429 0.3462898 0.1797235 0.4914344 0.3333333 0 0 ## 6 0.3276596 0.5 0.3838863 0.1872792 0.0000000 0.4978266 0.6809524 1 0 ## gear carb ## 11 0.5 0.4285714 ## 24 0.0 0.4285714 ## 23 0.0 0.1428571 ## 6 0.0 0.0000000 7.3.1.3 Normaliser Le terme normaliser est associer à des opérations différentes dans la littérature. Nous prendrons la nomenclature de scikit-learn, pour qui la normalisation consiste à faire en sorte que la longueur du vecteur (sa norme, d’où normaliser) soit unitaire. Cette opération est le plus souvent utilisée par observation (ligne), non pas par variable (colonne). Il existe plusieurs manières de mesures la distance d’un vecteur, mais la plus commune est la distance euclidienne. La seule fois que j’ai eu à utiliser ce prétraitement était en analyse spectrale (Chemometrics with R, Ron Wehrens, 2011, chapitre 3.5). En R, library(&quot;pls&quot;) ## ## Attaching package: &#39;pls&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## loadings data(&quot;gasoline&quot;) spectro &lt;- gasoline$NIR %&gt;% unclass() %&gt;% as_tibble() normalise &lt;- function(x) x/sqrt(sum(x^2)) spectro_norm &lt;- spectro %&gt;% rowwise() %&gt;% # différentes approches possibles pour les opérations sur les lignes normalise() spectro_norm[1:4, 1:4] ## 900 nm 902 nm 904 nm 906 nm ## 1 -0.0011224834 -0.0010265446 -0.0009434425 -0.0008314021 ## 2 -0.0009890637 -0.0008856332 -0.0007977676 -0.0006912734 ## 3 -0.0010481029 -0.0009227116 -0.0008269742 -0.0007035061 ## 4 -0.0010444801 -0.0009446277 -0.0008623530 -0.0007718261 7.3.1.4 Analyse compositionnelle en R En 1898, le statisticien Karl Pearson nota que des corrélations étaient induites lorsque l’on effectuait des ratios par rapport à une variable commune. Source Karl Pearson, 1897. Mathematical contributions to the theory of evolution.—on a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London Faisons l’exercice! Nous générons au hasard 1000 données (comme le proposait Pearson) pour trois dimensions: le fémur, le tibia et l’humérus. Ces dimensions ne sont pas générées par des distributions corrélées. set.seed(3570536) n &lt;- 1000 bones &lt;- tibble(femur = rnorm(n, 10, 3), tibia = rnorm(n, 8, 2), humerus = rnorm(n, 6, 2)) plot(bones) cor(bones) ## femur tibia humerus ## femur 1.000000000 -0.069006171 0.002652292 ## tibia -0.069006171 1.000000000 -0.008994704 ## humerus 0.002652292 -0.008994704 1.000000000 Pourtant, si j’utilise des ratios allométriques avec l’humérus comme base, bones_r &lt;- bones %&gt;% transmute(fh = femur/humerus, th = tibia/humerus) plot(bones_r) text(30, 20, paste(&quot;corrélation =&quot;, round(cor(bones_r$fh, bones_r$th), 2)), col = &quot;blue&quot;) Nous avons induit ce que Pearson appelait une fausse corrélation (spurious correlation). En 1960, Chayes proposa que de telles fausses corrélations sont induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios d’une somme totale. Par exemple, dans une composition simple de deux types d’utilisation du territoire, si une proportion augmente, l’autre doit nécessairement diminuer. n &lt;- 100 tibble(A = runif(n, 0, 1)) %&gt;% mutate(B = 1 - A) %&gt;% ggplot(aes(x=A, y=B)) + geom_point() Les variables exprimées relativement à une somme totale sont dites compositionnelles. Elles possèdent les caractéristiques suivantes. Redondance d’information. Un système de deux proportions ne contient qu’une seule variable du fait que l’on puisse déduire l’une en soutrayant l’autre de la somme totale. Un vecteur compositionnel contient de l’information redondante. Pourtant, effectuer des statistiques sur l’une plutôt que sur l’autre donnera des résultats différents. Dépendance d’échelle. Les statistiques devraient être indépendantes de la somme totale utilisée. Pourtant, elles différeront sur l’on utilise par exemple, une proportion des mâles d’une part et des femelles d’autre part, ou la proportion de la somme des deux, de même que les résultats d’un test sanguin différera si l’on utilise une base sèche ou une base humide. Distribution théorique des données. Étant donnée que les proportions sont confinées entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui s’étend de -∞ à +∞) n’est souvent pas appropriée. On pourra utiliser la distribution de Dirichlet, mais d’autres approches sont souvent plus pratiques. Les conséquences d’effectuer des statistiques linéaires sur des données compositionnelles brutes peuvent être majeures. En outre, Pawlowksy-Glahn et Egozcue (2006), s’appuyant en outre sur Rock (1988), note les problèmes suivants (exprimés en mes mots). les régressions, les regroupements et les analyses en composantes principales peuvent avoit peu ou pas de signification les propriétés des distributions peuvent être générées par l’opération de fermeture de la composition (s’assurer que le total des proportions donne 100%) les résultats d’analyses discriminantes linéaries sont propices à être illusoires tous les coefficients de corrélation seront affectés à des degrés inconnus les résultats des tests d’hypothèses seront intrinsèquement faussés Pour contourner ces problèmes, il faut d’abord aborder les données compositionnelles pour ce qu’elles sont: des données intrinsèquement multivariées. Elles sont un nuage de point, et non pas une collection de variables individuelles. Ceci qui n’empêche pas d’effectuer des analyses consciencieusement sous des angles particuliers. En R, on pourra aisément rapporter une composition en somme unitaire grâce à la fonction apply. Mais auparavant, chargeons le module compositions (n’oubliez pas de l’installer au préalable) pour accéder à des données fictives de proportions de sable, limon et argile dans des sédiments. library(&quot;compositions&quot;) ## Loading required package: tensorA ## ## Attaching package: &#39;tensorA&#39; ## The following object is masked from &#39;package:base&#39;: ## ## norm ## Loading required package: robustbase ## Loading required package: energy ## Loading required package: bayesm ## Welcome to compositions, a package for compositional data analysis. ## Find an intro with &quot;? compositions&quot; ## ## Attaching package: &#39;compositions&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## R2 ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, cov, dist, var ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, scale, scale.default data(&quot;ArcticLake&quot;) ArcticLake &lt;- ArcticLake %&gt;% as_tibble() head(ArcticLake) ## # A tibble: 6 x 4 ## sand silt clay depth ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 77.5 19.5 3 10.4 ## 2 71.9 24.9 3.2 11.7 ## 3 50.7 36.1 13.2 12.8 ## 4 52.2 40.9 6.6 13 ## 5 70 26.5 3.5 15.7 ## 6 66.5 32.2 1.3 16.3 comp &lt;- ArcticLake %&gt;% select(-depth) %&gt;% apply(., 1, function(x) x/sum(x)) %&gt;% t() comp[1:5, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 On pourra aussi utiliser la fonction acomp (pour Aitchison-composition) pour fermer la composition à une somme de 1. comp &lt;- ArcticLake %&gt;% select(-depth) %&gt;% acomp(.) comp[1:5, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 Cette stratégie a pour avantage d’attribuer à la variable comp la classe acomp, qui automatise les opérations dans l’espace compositionnel (que l’on nomme aussi le simplex). La représentation ternaire est souvent utilisée pour présenter des compositions. Toutefois, il est difficile d’interpréter les compositions de plus de trois parties. La classe acomp automatise aussi la représentation teranaire. plot(comp) Afin de transposer cet espace clôt en un espace ouvert, on pourra diviser chaque proportion par une proportion de référence choisie parmi n’importe quelle proportion. Du coup, on retire une dimension redondante! Dans ce ratio, on choisit d’utiliser la proportion de référence au dénominateur, ce qui est arbitraire. En utilisant le log du ratio, l’inverse du ratio ne sera qu’un changement de signe, ce qui est pratique en statistiques linéaries. Cette solution, proposée par Aitchison (1986), s’applique non seulement sur les compositions à deux composantes, mais sur toute composition. Il s’agit alors d’utiliser une composition de référence pour effecteur les ratios. Pour une composition de \\(A\\), \\(B\\), \\(C\\), \\(D\\) et \\(E\\): \\[alr_A = log \\left( \\frac{A}{E} \\right), alr_B = log \\left( \\frac{B}{E} \\right), alr_C = log \\left( \\frac{C}{E} \\right), alr_D = log \\left( \\frac{D}{E} \\right)\\] Dans R, la colonne de référence est par défaut la dernière colonne de la matrice des compositions. add_lr &lt;- alr(comp) Cette dernière stratégie se nomme les log-ratios aditifs (\\(alr\\) pour additive log-ratio). Bien que valide pour effectuer des tests statistiques, cette stratégie a le désavantage de dépendre de la décision arbitraire de la composante à utiliser au numérateur. Deuxième restriction des alr: les axes de l’espace des alr n’étant pas orthogonaux, ils ne peuvent pas être utilisés pour effectuer des statistiques basées sur les distances (que nous couvrirons au chapitre 8). L’autre stratégie proposée par Aitchison était d’effectuer un log-ratio entre chaque composante et la moyenne géométrique de toutes les composantes. Cette transformation se nomme le log-ratio centré (\\(clr\\), pour centered log-ratio) \\[clr_i = log \\left( \\frac{x_i}{g \\left( x \\right)} \\right)\\] En R, cen_lr &lt;- clr(comp) Avec des CLRs, les distances sont valides. Mais… nous restons avec le problème de la redondance d’information. En fait, la somme de chacunes des lignes d’une matrice de clr est de 0. Pas très pratique lorsque l’on effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, géostatistiques, etc.) #cen_lr %&gt;% # cov() %&gt;% # solve() # Error in solve.default(.) : le système est numériquement singulier : conditionnement de la réciproque = 4.44407e-17 Enfin, une autre méthode de transformation développée par Egoscue et al. (2003), les log-ratios isométriques (ou isometric log-ratios, ilr) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonormées. Ces dimensions doivent doivent être préalablement établie dans un dendrogramme de bifurcation, où chaque composante ou groupe de composante est successivement divisé en deux embranchement. La manière d’arranger ces balances importe peu, mais on aura avantage à créer des balances interprétables. Le diagramme de balances peut être encodé dans une partition binaire séquentielle (ou sequential bianry partition, sbp). Une sbp est une matrice de contraste ou chaque ligne représente une partition entre deux variables ou groupes de variables. Une composante étiquettée +1 correspondra au groupe du numérateur, une composante étiquettée -1 au dénominateur et une composante étiquettée 0 sera exclue de la partition (Parent et al., 2013). J’ai reformulé la fonction CoDaDendrogram pour que l’on puisse ajouter des informations intéressantes sur les balants horizontaux. Cette fonction est disponible sur github. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R&quot;) sbp &lt;- matrix(c(1, 1,-1, 1,-1, 0), byrow = TRUE, ncol = 3) CoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1), equal.height = TRUE) Si la SBP est plus imposante, il pourrait être plus aisé de monter dans un chiffrier, puis de l’importer dans R via un fichier csv. Le calcul des ILRs est effectué comme suit. \\[ ilr_i = \\sqrt{\\frac{n_i^+ n_i^-}{n_i^+ + n_i^-}} ln \\left( \\frac{g \\left( c_i^+ \\right)}{g \\left( c_i^+ \\right)} \\right) \\] ou, à la ligne \\(i\\) de la SBP, \\(n_i^+\\) et \\(n_i^-\\) sont respectivement le nombre de composantes au numérateur et au dénominateur, \\(g \\left( c_i^+ \\right)\\) est la moyenne géométrique des composantes au numérateur et \\(g \\left( c_i^- \\right)\\) est la moyenne géométrique des composantes au dénominateur. Les balances sont conventionnellement notées [A,B | C,D], ou les composantes A et B au dénominateur sont balancées avec les composantes C and D au numérateur. Une balance positive signifie que la moyenne géométrique des concentrations au numérateur est supérieur à celle au dénominateur, et inversement, alors qu’une balance nulle signifie que les moyennes géométriques sont égales (équilibre). Ainsi, en modélisation linéaire, un coefficient positif sur [A,B | C,D] signifie que l’augmentation de l’importance de C et D comparativement à A et B est associé à une augmentation de la variable réponse du modèle. En R, iso_lr &lt;- ilr(comp, V = gsi.buildilrBase(t(sbp))) Notez la forme gsi.buildilrBase(t(sbp)) est une opération pour obtenir la matrice d’orthonormalité à partir de la SBP. Les ILRs sont des balances multivariées sur lesquelles on pourra effectuer des statistiques linéaries. Bien que ni les statistiques linéaires multivariées, la distance entre les points ne seront affectés par la structure de la SBP, l’interprétation des résultats comme collection d’interprétation univariée pourra être affectée. Pour les transformations inverses, vous pourrez utiliser les fonctions alrInv, clrInv et ilrInv. Dans tous les cas, si vous tenez à garder la trace de vos données dans leur format original, vous aurez avantage à ajouter à votre vecteur compositionnel la valeur de remplissage, constitué d’un amalgame des composantes non mesurées. Par exemple, pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) acomp(pourc) # vous perdez la trace des proportions originales ## N P K ## 0.73170732 0.02439024 0.24390244 ## attr(,&quot;class&quot;) ## [1] acomp pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) Fv &lt;- 1 - sum(pourc) comp &lt;- acomp(c(pourc, Fv = Fv)) comp ## N P K Fv ## 0.030 0.001 0.010 0.959 ## attr(,&quot;class&quot;) ## [1] acomp iso_lr &lt;- ilr(comp) # avec une sbp par défaut ilrInv(iso_lr) ## 1 2 3 4 ## [1,] 0.03 0.001 0.01 0.959 ## attr(,&quot;class&quot;) ## [1] acomp Si vos données font partie d’un tout, je vous recommande chaudement d’utiliser des méthodes compositionnelles autant pour l’analyse que la modélisation. Pour en savoir davantage, le livre Compositional data analysis with R, de van den Boogart et Tolosana-Delgado, est disponible en format électronique à la bibliothèque de l’Université Laval. Pour aller plus loin, j’ai écri un billet à ce sujet (auquel à ce jour il manque toujours un cas d’étude): We should use balances and machine learning to diagnose ionomes. 7.3.2 Acquérir des données météo Une tâche commune en écologie est de lier des observations à la météo… qui sont rarement collectés lors d’expériences. Environnement Canada possède sont réseau de stations. Les données sont disponibles sur internet en libre accès. Vous pouvez chercher des stations, effectuer des requêtes et télécharger des fichiers csv. Pour un petit tableau, la tâche est plutôt triviale. Mais ça devient rapidement laborieux à mesure que l’on doit rechercher de nombreuses données. Le module weathercan, développé par Steffi LaZerte, permet d’effectuer des requêtes rapidement à partir des coordonnées de votre site expérimental. Par exemple, si je cherche une station météo sfournissant des données horaires situé à moins de 20 km du sommet du Mont-Bellevue, à Sherbrooke, aux coordonnées [latitude 45.35, longitude -71.90], library(&quot;weathercan&quot;) station_site &lt;- stations_search(coords = c(45.35, -71.90), dist = 20, interval = &quot;hour&quot;) station_site ## # A tibble: 4 x 14 ## prov station_name station_id climate_id WMO_id TC_id lat lon elev ## &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 QC LENNOXVILLE 5397 7024280 71611 WQH 45.4 -71.8 181 ## 2 QC SHERBROOKE 48371 7028123 71610 YSC 45.4 -71.7 241. ## 3 QC SHERBROOKE A 5530 7028124 71610 YSC 45.4 -71.7 241. ## 4 QC SHERBROOKE A 30171 7028126 &lt;NA&gt; GSC 45.4 -71.7 241. ## # … with 5 more variables: tz &lt;chr&gt;, interval &lt;chr&gt;, start &lt;int&gt;, ## # end &lt;int&gt;, distance &lt;dbl&gt; Je prends en note l’identifiant de la station désirée (ou des stations, disons 5397 et 48371), puis je lance une requête pour obtenir la météo horaire entre les dates désirées. mont_bellevue &lt;- weather_dl(station_ids = c(5397, 48371), start = &quot;2019-01-01&quot;, end = &quot;2019-01-07&quot;) mont_bellevue %&gt;% head(5) ## # A tibble: 5 x 35 ## station_name station_id station_operator prov lat lon elev ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## 2 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## 3 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## 4 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## 5 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## # … with 28 more variables: climate_id &lt;chr&gt;, WMO_id &lt;chr&gt;, TC_id &lt;chr&gt;, ## # date &lt;date&gt;, time &lt;dttm&gt;, year &lt;chr&gt;, month &lt;chr&gt;, day &lt;chr&gt;, ## # hour &lt;chr&gt;, weather &lt;chr&gt;, hmdx &lt;dbl&gt;, hmdx_flag &lt;chr&gt;, ## # pressure &lt;dbl&gt;, pressure_flag &lt;chr&gt;, rel_hum &lt;dbl&gt;, ## # rel_hum_flag &lt;chr&gt;, temp &lt;dbl&gt;, temp_dew &lt;dbl&gt;, temp_dew_flag &lt;chr&gt;, ## # temp_flag &lt;chr&gt;, visib &lt;dbl&gt;, visib_flag &lt;chr&gt;, wind_chill &lt;dbl&gt;, ## # wind_chill_flag &lt;chr&gt;, wind_dir &lt;dbl&gt;, wind_dir_flag &lt;chr&gt;, ## # wind_spd &lt;dbl&gt;, wind_spd_flag &lt;chr&gt; Et voilà. mont_bellevue %&gt;% ggplot(aes(x = time, y = temp)) + geom_line(aes(colour = station_name)) 7.3.3 Pédométrie avec R Cette section a été écrite par Michael Leblanc. Plusieurs fonctionnalités ont été développées sur R afin d’aider les pédométriciens à visualiser, explorer et traiter les données numériques en science des sols. Voici quelques exemples. 7.3.3.1 Texture du sol La texture du sol est définie par sa composition granulométrique, habituellement représentée par trois fractions (sable, limon, argile), laquelle peut être généralisée en classe texturale. La définition des classes texturales diffère d’un système ou d’un pays à l’autre comme en témoigne l’article Perdus dans le triangle des textures (Richer de Forges et al. 2008). La définition des fractions granulométriques peut également différer selon le domaine d’étude (ingénierie, pédologie) ou le pays. Par exemple, le diamètre du limon est de 0,002 mm à 0,05 mm dans le système canadien, américain et français alors qu’il est de 0,002 mm à 0,02 mm dans le système australien et de 0,002 mm à 0,063 mm dans le système allemand. Il est donc important de vérifier la méthodologie et le système de classification utilisés pour interpréter les données de texture du sol. Le module soilTexture propose des fonctions permettant d’aborder ces multiples définitions. library(&quot;soiltexture&quot;) Les triangles texturaux Avec la fonction TT.plot, vous pouvez présenter vos données granulométriques dans un triangle textural tel que défini par les différents systèmes nationaux. Auparavant, créons un objet comprenant des textures aléatoires. set.seed(848341) # random.org rand_text &lt;- TT.dataset(n=100, seed.val=29) head(rand_text) ## CLAY SILT SAND Z ## 1 54.650857 40.37101 4.978129 13.2477582 ## 2 44.745954 40.81782 14.436221 20.8433109 ## 3 18.192509 48.26752 33.539970 7.1814626 ## 4 17.750492 40.14405 42.105458 -0.2077358 ## 5 65.518360 23.36110 11.120538 10.8656027 ## 6 6.610293 22.45353 70.936173 3.7108567 Avec le module soiltexture, les tableaux de texture doivent inclure les intitullés exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures générées peuvent être portés dans des diagrammes ternaires texturaux de différents systèmes de classification, par exemple le système canadioen et le système USDA. par(mfrow=c(1, 2)) TT.plot(class.sys = &quot;CA.FR.TT&quot;, tri.data = rand_text, col = &quot;blue&quot;) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = rand_text, col = &quot;blue&quot;) Les paramètres de la figure (titres, polices, style de la grille, etc.) peuvent être personnalisés avec les arguments TT.plot. Les classes texturales La fonction TT.points.in.classes est utile pour désigner la classe texturale à partir des données granulométriques, en spécifiant bien le système de classification désiré. TT.points.in.classes( tri.data = rand_text[1:10, ], # class.sys = &quot;CA.FR.TT&quot;, PiC.type = &quot;t&quot; ) ## [1] &quot;ALi&quot; &quot;ALi&quot; &quot;L&quot; &quot;L&quot; &quot;ALo&quot; &quot;LS&quot; &quot;ALo&quot; &quot;A&quot; &quot;LLi&quot; &quot;LSA&quot; Plusieurs autres fonctions sont proposées par soiltexture afin de visualiser, classifier et transformer les données de texture du sol : Functions in soiltexture. Julien Moeys (2018) propose également le tutoriel The soil texture wizard: a tutorial. 7.3.3.2 Profils de sols Le profil de sols est une entité décrite par une séquence de couches ou d’horizons avec différentes caractéristiques morphologiques. Le module AQP, pour Algorithms for Quantitative Pedology, propose des fonctions de visualisation, d’agrégation et de classification permettant d’aborder la complexité inhérente aux informations pédologiques. 7.3.3.2.1 La visualisation de profils Vous devez d’abord structurer vos données dans un tableau (data.frame) incluant minimalement ces trois colonnes : Identifiant unique du profil (groupes d’horizons) (id) Limites supérieures de l’horizon (top) Limites inférieures de l’horizon (down) Vos données morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier pédologique à titre d’exemple. profils &lt;- read_csv(&quot;data/06_pedometric-profile.csv&quot;) ## Parsed with column specification: ## cols( ## id = col_double(), ## horizon = col_character(), ## top = col_double(), ## bottom = col_double(), ## hue = col_character(), ## value = col_double(), ## chroma = col_double(), ## pH.CaCl2 = col_double(), ## C.CNS.pc = col_double() ## ) head(profils) ## # A tibble: 6 x 9 ## id horizon top bottom hue value chroma pH.CaCl2 C.CNS.pc ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Ap1 0 23 10YR 2 3 4.78 2.71 ## 2 1 Ap2 23 34 10YR 2 2 4.74 2.2 ## 3 1 Bfcj 34 46 7.5YR 4 5 4.79 2.4 ## 4 1 BC 46 83 2.5Y 4 5 4.93 0.22 ## 5 1 C 83 100 2.5Y 5 4 4.82 0.18 ## 6 2 Ap 0 29 10YR 2 2 4.6 4.22 La fonction munsell2rgb permet de convertir le code de couleur Munsell en format RGB. library(&quot;aqp&quot;) ## This is aqp 1.17 ## ## Attaching package: &#39;aqp&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## slice, union ## The following object is masked from &#39;package:base&#39;: ## ## union profils$soil_color &lt;- with(profils, munsell2rgb(hue, value, chroma)) Préalablement à la visualisation, le tableau est transformé en objet SoilProfileCollection par la fonction depths. Pour ce faire, le tableau doit être un pur data.frame, non pas un tibble. profils &lt;- profils %&gt;% as.data.frame() depths(profils) &lt;- id ~ top + bottom La fonction plot détectera le type d’objet et appellera la fonction de visualisation en conséquence. par(mfrow = c(1, 3)) plot(profils, name=&quot;horizon&quot;) title(&#39;Couleur des horizons&#39;, cex.main=1) plot(profils, name=&quot;horizon&quot;, color=&#39;C.CNS.pc&#39;, col.label=&#39;C total (%)&#39;) plot(profils, name=&quot;horizon&quot;, color=&#39;pH.CaCl2&#39;, col.label=&#39;pH CaCl2&#39;) De multiples figures thématiques peuvent être générées afin de représenter les particuliarités des profils. Pour aller plus loin, consultez les guides Introduction to SoilProfileCollection Objects et Generating Sketches from SPC Objects. 7.3.3.2.2 Les plans verticaux (depth functions) Les plans verticaux sont des diagrammes qui permettent d’interpréter les données en fonction de la profondeur. La fonction slab permet le calcul de statisques descriptives par interval de profondeur régulier lesquelles permettent de figurer la variabilité verticale des propriétés de sols. agg &lt;- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2) La visualisation est générée par le module graphique ggplot2 agg %&gt;% ggplot(mapping = aes(x = -top, y = p.q50)) + facet_grid(. ~ variable, scale = &quot;free&quot;) + geom_ribbon(aes(ymin = p.q25, ymax = p.q75), fill = &quot;grey75&quot;, alpha = 0.5) + geom_path() + labs(x = &quot;Profondeur (cm)&quot;, y = &quot;Médiane bordée des 25e and 75e percentiles&quot;) + coord_flip() 7.3.3.2.3 Le regroupement de profils Le calcul des distances de dissimilarité entre les profils avec profile_compare permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitre 8 les concepts de dissimilarité et de partitionnement. library(&quot;cluster&quot;) library(&quot;sharpshootR&quot;) d &lt;- profile_compare(profils, vars=c(&#39;C.CNS.pc&#39;, &#39;pH.CaCl2&#39;), k=0, max_d=40) ## Computing dissimilarity matrices from 10 profiles [0.08 Mb] d_diana &lt;- diana(d) plotProfileDendrogram(profils, name=&quot;horizon&quot;, d_diana, scaling.factor = 0.3, y.offset = 5, color=&#39;pH.CaCl2&#39;, col.label=&#39;pH CaCl2&#39;) 7.3.3.2.4 Diagramme de relations entre les horizons Il est possible de visualiser les transitions d’horizon les plus probables dans un groupe de profils de sols. tp &lt;- hzTransitionProbabilities(profils, name=&quot;horizon&quot;) ## Warning: ties in transition probability matrix par(mar = c(0, 0, 0, 0), mfcol = c(1, 2)) plot(profils, name=&quot;horizon&quot;) plotSoilRelationGraph(tp, graph.mode = &quot;directed&quot;, edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, vertex.label.family = &quot;sans&quot;) Consultez AQP project pour des présentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilités du package AQP. 7.3.4 Créer des applications avec R RStudio vous permet de déployer vos résultats sous forme d’applications web grâce à son module shiny. Pour ce faire, le seul préalable est de savoir programmer en R. En agençant une interface avec des inputs (listes de sélection, des boîtes de dialogue, des sélecteurs, des boutons, etc.) avec des modèles que vous développez, vous pourrez créer des interfaces intéractives. Pour créer une application shiny, vous devez créer une partie pour l’interface (ui) et une autre pour le calcul (server). Je n’irai pas dans les détails, étant donnée qu’il s’agit d’un sujet à part entière. Pour aller plus loin, visitez le site du projet shiny. library(&quot;shiny&quot;) ui &lt;- basicPage( sliderInput(&quot;A&quot;, &quot;Asymptote:&quot;, min = 0, max = 100, value = 50), sliderInput(&quot;E&quot;, &quot;Environnement:&quot;, min = -10, max = 100, value = 20), sliderInput(&quot;R&quot;, &quot;Taux:&quot;, min = 0, max = 0.1, value = 0.035), sliderInput(&quot;prix_dose&quot;, &quot;Prix dose:&quot;, min = 0, max = 5, value = 1), sliderInput(&quot;prix_vente&quot;, &quot;Prix vente:&quot;, min = 0, max = 200, value = 100), sliderInput(&quot;dose&quot;, &quot;Dose:&quot;, min = 0, max = 300, value = c(0, 200)), plotOutput(&quot;distPlot&quot;) ) server &lt;- function(input, output) { mitsch_f &lt;- reactive({ input$A * (1 - exp(-input$R * (seq(input$dose[1], input$dose[2], length = 100) + input$E))) }) mitsch_opt &lt;- reactive({ (log((input$A * input$R * input$prix_vente) / input$prix_dose - input$E * input$R) / input$R ) }) output$distPlot &lt;- renderPlot({ plot(seq(input$dose[1], input$dose[2], length = 100), mitsch_f(), type = &quot;l&quot;, ylim = c(0, max(mitsch_f()) * 1.1)) abline(v = mitsch_opt() ) text(mitsch_opt(), 2, paste(&quot;Dose optimale:&quot;, round(mitsch_opt(), 0))) }) } shinyApp(ui, server) Une fois l’application créée, il est possible de la déployer sur le site shninyapps.io. D’abord créer une application shiny dans RStudio: File &gt; New File &gt; Shiny Web App. Écrivez votre code dans le fichier app.R (dans ce cas, ce peut être un copier-coller), puis cliquez sur Run App en haut à droite de la fenêtre d’édition du code. Lorsque l’application fonctionne, vous pourrez la publier via RStudio en cliquant sur le bouton Publish dans la fenêtre Viewer (vous devez au préalable avoir un comte sur shinyapp.io). Une application sera publique et sera ouverte. https://essicolo.shinyapps.io/Mitscherlich/ Pour déployer en mode privé, vous devrez débourser pour un forfait ou installer votre propre serveur. "]
]
